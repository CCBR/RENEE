{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"RENEE \u00b6 R na s E quencing a N alysis pip E lin E An open-source, reproducible, and scalable solution for analyzing RNA-seq data. See the website for detailed information, documentation, and examples: https://ccbr.github.io/RENEE/latest/ Table of Contents \u00b6 RENEE - R na s E quencing a N alysis pip E lin E Table of Contents 1. Introduction 2. Overview 2.1 RENEE Pipeline 2.2 Reference Genomes 2.3 Dependencies 3. Run RENEE pipeline 3.1 Biowulf 3.2 FRCE 4. References 5. Version Notes 1. Introduction \u00b6 RNA-sequencing ( RNA-seq ) has a wide variety of applications. This popular transcriptome profiling technique can be used to quantify gene and isoform expression, detect alternative splicing events, predict gene-fusions, call variants and much more. RENEE is a comprehensive, open-source RNA-seq pipeline that relies on technologies like Docker 20 and Singularity 21 ... now called Apptainer to maintain the highest-level of reproducibility. The pipeline consists of a series of data processing and quality-control steps orchestrated by Snakemake 19 , a flexible and scalable workflow management system, to submit jobs to a cluster or cloud provider. Fig 1. Run locally on a compute instance, on-premise using a cluster, or on the cloud using AWS. A user can define the method or mode of execution. The pipeline can submit jobs to a cluster using a job scheduler like SLURM, or run on AWS using Tibanna (feature coming soon!). A hybrid approach ensures the pipeline is accessible to all users. As an optional step, relevelant output files and metadata can be stored in object storage using HPC DME (NIH users) or Amazon S3 for archival purposes (coming soon!). 2. Overview \u00b6 2.1 RENEE Pipeline \u00b6 A bioinformatics pipeline is more than the sum of its data processing steps. A pipeline without quality-control steps provides a myopic view of the potential sources of variation within your data (i.e., biological verses technical sources of variation). RENEE pipeline is composed of a series of quality-control and data processing steps. The accuracy of the downstream interpretations made from transcriptomic data are highly dependent on initial sample library. Unwanted sources of technical variation, which if not accounted for properly, can influence the results. RENEE's comprehensive quality-control helps ensure your results are reliable and reproducible across experiments . In the data processing steps, RENEE quantifies gene and isoform expression and predicts gene fusions. Please note that the detection of alternative splicing events and variant calling will be incorporated in a later release. Fig 2. An Overview of RENEE Pipeline. Gene and isoform counts are quantified and a series of QC-checks are performed to assess the quality of the data. This pipeline stops at the generation of a raw counts matrix and gene-fusion calling. To run the pipeline, a user must select their raw data, a reference genome, and output directory (i.e., the location where the pipeline performs the analysis). Quality-control information is summarized across all samples in a MultiQC report. Quality Control FastQC 2 is used to assess the sequencing quality. FastQC is run twice, before and after adapter trimming. It generates a set of basic statistics to identify problems that can arise during sequencing or library preparation. FastQC will summarize per base and per read QC metrics such as quality scores and GC content. It will also summarize the distribution of sequence lengths and will report the presence of adapter sequences. Kraken2 14 and FastQ Screen 17 are used to screen for various sources of contamination. During the process of sample collection to library preparation, there is a risk for introducing wanted sources of DNA. FastQ Screen compares your sequencing data to a set of different reference genomes to determine if there is contamination. It allows a user to see if the composition of your library matches what you expect. Also, if there are high levels of microbial contamination, Kraken can provide an estimation of the taxonomic composition. Kraken can be used in conjunction with Krona 15 to produce interactive reports. Preseq 1 is used to estimate the complexity of a library for each samples. If the duplication rate is very high, the overall library complexity will be low. Low library complexity could signal an issue with library preparation where very little input RNA was over-amplified or the sample may be degraded. Picard 10 can be used to estimate the duplication rate, and it has another particularly useful sub-command called CollectRNAseqMetrics which reports the number and percentage of reads that align to various regions: such as coding, intronic, UTR, intergenic and ribosomal regions. This is particularly useful as you would expect a library constructed with ploy(A)-selection to have a high percentage of reads that map to coding regions. Picard CollectRNAseqMetrics will also report the uniformity of coverage across all genes, which is useful for determining whether a sample has a 3' bias (observed in ploy(A)-selection libraries containing degraded RNA). RSeQC 9 is another particularity useful package that is tailored for RNA-seq data. It is used to calculate the inner distance between paired-end reads and calculate TIN values for a set of canonical protein-coding transcripts. A median TIN value is calucated for each sample, which analogous to a computationally derived RIN. MultiQC 11 is used to aggregate the results of each tool into a single interactive report. Quantification Cutadapt 3 is used to remove adapter sequences, perform quality trimming, and remove very short sequences that would otherwise multi-map all over the genome prior to alignment. STAR 4 is used to align reads to the reference genome. The RENEE pipeline runs STAR in a two-passes where splice-junctions are collected and aggregated across all samples and provided to the second-pass of STAR. In the second pass of STAR, the splice-junctions detected in the first pass are inserted into the genome indices prior to alignment. RSEM 5 is used to quantify gene and isoform expression. The expected counts from RSEM are merged across samples to create a two counts matrices for gene counts and isoform counts. Arriba 22 is used to predict gene-fusion events. The pre-built human and mouse reference genomes use Arriba blacklists to reduce the false-positive rate. 2.2 Reference Genomes \u00b6 Pre-built reference genomes are provided on Biowulf and FRCE for a number of different annotation versions, view the list here: https://ccbr.github.io/RENEE/latest/RNA-seq/Resources/#1-reference-genomes If you would like to use a custom reference that is not already listed above, you can prepare it with the renee build command. See docs here: https://ccbr.github.io/RENEE/latest/RNA-seq/build/ 2.3 Dependencies \u00b6 Requires: singularity>=3.5 snakemake>=6.0 NOTE: Biowulf users : Both, singularity and snakemake, modules are already installed and available for all Biowulf users. Please skip this step as module load ccbrpipeliner will preload singularity and snakemake. Snakemake and singularity must be installed on the target system. Snakemake orchestrates the execution of each step in the pipeline. To guarantee reproducibility, each step relies on pre-built images from DockerHub . Snakemake pulls these docker images while converting them to singularity on the fly and saves them onto the local filesystem prior to job execution, and as so, snakemake and singularity are the only two dependencies. Back to Top 3. Run RENEE pipeline \u00b6 3.1 Biowulf \u00b6 # RENEE is configured to use different execution backends: local or slurm # view the help page for more information module load ccbrpipeliner renee run --help # @local: uses local singularity execution method # The local MODE will run serially on compute # instance. This is useful for testing, debugging, # or when a users does not have access to a high # performance computing environment. # Please note that you can dry-run the command below # by providing the --dry-run flag # Do not run this on the head node! # Grab an interactive node sinteractive --mem = 110g --cpus-per-task = 12 --gres = lscratch:200 module load ccbrpipeliner renee run --input .tests/*.R?.fastq.gz --output /data/ $USER /RNA_hg38 --genome hg38_36 --mode local # @slurm: uses slurm and singularity execution method # The slurm MODE will submit jobs to the cluster. # The --sif-cache flag will re-use singularity containers from a shared location. # It is recommended running RENEE in this mode. module load ccbrpipeliner renee run \\ --input .tests/*.R?.fastq.gz \\ --output /data/ $USER /RNA_hg38 \\ --genome hg38_36 \\ --mode slurm \\ --sif-cache /data/CCBR_Pipeliner/SIFs 3.2 FRCE \u00b6 # grab an interactive node srun --export all --pty --x11 bash # add renee to path correctly . /mnt/projects/CCBR-Pipelines/pipelines/guis/latest/bin/setup # run renee renee --help When running renee on FRCE, we recommend setting --tmp-dir and --sif-cache with the following values: renee run \\ --input .tests/*.R?.fastq.gz \\ --output /scratch/cluster_scratch/ $USER /RNA_hg38 \\ --genome hg38_36 \\ --mode slurm \\ --tmp-dir /scratch/cluster_scratch/ $USER \\ --sif-cache /mnt/projects/CCBR-Pipelines/SIFs Back to Top 4. References \u00b6 1. Daley, T. and A.D. Smith, Predicting the molecular complexity of sequencing libraries. Nat Methods, 2013. 10(4): p. 325-7. 2. Andrews, S. (2010). FastQC: a quality control tool for high throughput sequence data. 3. Martin, M. (2011). \"Cutadapt removes adapter sequences from high-throughput sequencing reads.\" EMBnet 17(1): 10-12. 4. Dobin, A., et al., STAR: ultrafast universal RNA-seq aligner. Bioinformatics, 2013. 29(1): p. 15-21. 5. Li, B. and C.N. Dewey, RSEM: accurate transcript quantification from RNA-Seq data with or without a reference genome. BMC Bioinformatics, 2011. 12: p. 323. 6. Harrow, J., et al., GENCODE: the reference human genome annotation for The ENCODE Project. Genome Res, 2012. 22(9): p. 1760-74. 7. Law, C.W., et al., voom: Precision weights unlock linear model analysis tools for RNA-seq read counts. Genome Biol, 2014. 15(2): p. R29. 8. Smyth, G.K., Linear models and empirical bayes methods for assessing differential expression in microarray experiments. Stat Appl Genet Mol Biol, 2004. 3: p. Article3. 9. Wang, L., et al. (2012). \"RSeQC: quality control of RNA-seq experiments.\" Bioinformatics 28(16): 2184-2185. 10. The Picard toolkit. https://broadinstitute.github.io/picard/. 11. Ewels, P., et al. (2016). \"MultiQC: summarize analysis results for multiple tools and samples in a single report.\" Bioinformatics 32(19): 3047-3048. 12. R Core Team (2018). R: A Language and Environment for Statistical Computing. Vienna, Austria, R Foundation for Statistical Computing. 13. Li, H., et al. (2009). \"The Sequence Alignment/Map format and SAMtools.\" Bioinformatics 25(16): 2078-2079. 14. Wood, D. E. and S. L. Salzberg (2014). \"Kraken: ultrafast metagenomic sequence classification using exact alignments.\" Genome Biol 15(3): R46. 15. Ondov, B. D., et al. (2011). \"Interactive metagenomic visualization in a Web browser.\" BMC Bioinformatics 12(1): 385. 16. Okonechnikov, K., et al. (2015). \"Qualimap 2: advanced multi-sample quality control for high-throughput sequencing data.\" Bioinformatics 32(2): 292-294. 17. Wingett, S. and S. Andrews (2018). \"FastQ Screen: A tool for multi-genome mapping and quality control.\" F1000Research 7(2): 1338. 18. Robinson, M. D., et al. (2009). \"edgeR: a Bioconductor package for differential expression analysis of digital gene expression data.\" Bioinformatics 26(1): 139-140. 19. Koster, J. and S. Rahmann (2018). \"Snakemake-a scalable bioinformatics workflow engine.\" Bioinformatics 34(20): 3600. 20. Merkel, D. (2014). Docker: lightweight linux containers for consistent development and deployment. Linux Journal, 2014(239), 2. 21. Kurtzer GM, Sochat V, Bauer MW (2017). Singularity: Scientific containers for mobility of compute. PLoS ONE 12(5): e0177459. 22. Haas, B. J., et al. (2019). \"Accuracy assessment of fusion transcript detection via read-mapping and de novo fusion transcript assembly-based methods.\" Genome Biology 20(1): 213. Back to Top","title":"About"},{"location":"#renee","text":"R na s E quencing a N alysis pip E lin E An open-source, reproducible, and scalable solution for analyzing RNA-seq data. See the website for detailed information, documentation, and examples: https://ccbr.github.io/RENEE/latest/","title":"RENEE"},{"location":"#table-of-contents","text":"RENEE - R na s E quencing a N alysis pip E lin E Table of Contents 1. Introduction 2. Overview 2.1 RENEE Pipeline 2.2 Reference Genomes 2.3 Dependencies 3. Run RENEE pipeline 3.1 Biowulf 3.2 FRCE 4. References 5. Version Notes","title":"Table of Contents"},{"location":"#1-introduction","text":"RNA-sequencing ( RNA-seq ) has a wide variety of applications. This popular transcriptome profiling technique can be used to quantify gene and isoform expression, detect alternative splicing events, predict gene-fusions, call variants and much more. RENEE is a comprehensive, open-source RNA-seq pipeline that relies on technologies like Docker 20 and Singularity 21 ... now called Apptainer to maintain the highest-level of reproducibility. The pipeline consists of a series of data processing and quality-control steps orchestrated by Snakemake 19 , a flexible and scalable workflow management system, to submit jobs to a cluster or cloud provider. Fig 1. Run locally on a compute instance, on-premise using a cluster, or on the cloud using AWS. A user can define the method or mode of execution. The pipeline can submit jobs to a cluster using a job scheduler like SLURM, or run on AWS using Tibanna (feature coming soon!). A hybrid approach ensures the pipeline is accessible to all users. As an optional step, relevelant output files and metadata can be stored in object storage using HPC DME (NIH users) or Amazon S3 for archival purposes (coming soon!).","title":"1. Introduction"},{"location":"#2-overview","text":"","title":"2. Overview"},{"location":"#21-renee-pipeline","text":"A bioinformatics pipeline is more than the sum of its data processing steps. A pipeline without quality-control steps provides a myopic view of the potential sources of variation within your data (i.e., biological verses technical sources of variation). RENEE pipeline is composed of a series of quality-control and data processing steps. The accuracy of the downstream interpretations made from transcriptomic data are highly dependent on initial sample library. Unwanted sources of technical variation, which if not accounted for properly, can influence the results. RENEE's comprehensive quality-control helps ensure your results are reliable and reproducible across experiments . In the data processing steps, RENEE quantifies gene and isoform expression and predicts gene fusions. Please note that the detection of alternative splicing events and variant calling will be incorporated in a later release. Fig 2. An Overview of RENEE Pipeline. Gene and isoform counts are quantified and a series of QC-checks are performed to assess the quality of the data. This pipeline stops at the generation of a raw counts matrix and gene-fusion calling. To run the pipeline, a user must select their raw data, a reference genome, and output directory (i.e., the location where the pipeline performs the analysis). Quality-control information is summarized across all samples in a MultiQC report. Quality Control FastQC 2 is used to assess the sequencing quality. FastQC is run twice, before and after adapter trimming. It generates a set of basic statistics to identify problems that can arise during sequencing or library preparation. FastQC will summarize per base and per read QC metrics such as quality scores and GC content. It will also summarize the distribution of sequence lengths and will report the presence of adapter sequences. Kraken2 14 and FastQ Screen 17 are used to screen for various sources of contamination. During the process of sample collection to library preparation, there is a risk for introducing wanted sources of DNA. FastQ Screen compares your sequencing data to a set of different reference genomes to determine if there is contamination. It allows a user to see if the composition of your library matches what you expect. Also, if there are high levels of microbial contamination, Kraken can provide an estimation of the taxonomic composition. Kraken can be used in conjunction with Krona 15 to produce interactive reports. Preseq 1 is used to estimate the complexity of a library for each samples. If the duplication rate is very high, the overall library complexity will be low. Low library complexity could signal an issue with library preparation where very little input RNA was over-amplified or the sample may be degraded. Picard 10 can be used to estimate the duplication rate, and it has another particularly useful sub-command called CollectRNAseqMetrics which reports the number and percentage of reads that align to various regions: such as coding, intronic, UTR, intergenic and ribosomal regions. This is particularly useful as you would expect a library constructed with ploy(A)-selection to have a high percentage of reads that map to coding regions. Picard CollectRNAseqMetrics will also report the uniformity of coverage across all genes, which is useful for determining whether a sample has a 3' bias (observed in ploy(A)-selection libraries containing degraded RNA). RSeQC 9 is another particularity useful package that is tailored for RNA-seq data. It is used to calculate the inner distance between paired-end reads and calculate TIN values for a set of canonical protein-coding transcripts. A median TIN value is calucated for each sample, which analogous to a computationally derived RIN. MultiQC 11 is used to aggregate the results of each tool into a single interactive report. Quantification Cutadapt 3 is used to remove adapter sequences, perform quality trimming, and remove very short sequences that would otherwise multi-map all over the genome prior to alignment. STAR 4 is used to align reads to the reference genome. The RENEE pipeline runs STAR in a two-passes where splice-junctions are collected and aggregated across all samples and provided to the second-pass of STAR. In the second pass of STAR, the splice-junctions detected in the first pass are inserted into the genome indices prior to alignment. RSEM 5 is used to quantify gene and isoform expression. The expected counts from RSEM are merged across samples to create a two counts matrices for gene counts and isoform counts. Arriba 22 is used to predict gene-fusion events. The pre-built human and mouse reference genomes use Arriba blacklists to reduce the false-positive rate.","title":"2.1 RENEE Pipeline"},{"location":"#22-reference-genomes","text":"Pre-built reference genomes are provided on Biowulf and FRCE for a number of different annotation versions, view the list here: https://ccbr.github.io/RENEE/latest/RNA-seq/Resources/#1-reference-genomes If you would like to use a custom reference that is not already listed above, you can prepare it with the renee build command. See docs here: https://ccbr.github.io/RENEE/latest/RNA-seq/build/","title":"2.2 Reference Genomes"},{"location":"#23-dependencies","text":"Requires: singularity>=3.5 snakemake>=6.0 NOTE: Biowulf users : Both, singularity and snakemake, modules are already installed and available for all Biowulf users. Please skip this step as module load ccbrpipeliner will preload singularity and snakemake. Snakemake and singularity must be installed on the target system. Snakemake orchestrates the execution of each step in the pipeline. To guarantee reproducibility, each step relies on pre-built images from DockerHub . Snakemake pulls these docker images while converting them to singularity on the fly and saves them onto the local filesystem prior to job execution, and as so, snakemake and singularity are the only two dependencies. Back to Top","title":"2.3 Dependencies"},{"location":"#3-run-renee-pipeline","text":"","title":"3. Run RENEE pipeline"},{"location":"#31-biowulf","text":"# RENEE is configured to use different execution backends: local or slurm # view the help page for more information module load ccbrpipeliner renee run --help # @local: uses local singularity execution method # The local MODE will run serially on compute # instance. This is useful for testing, debugging, # or when a users does not have access to a high # performance computing environment. # Please note that you can dry-run the command below # by providing the --dry-run flag # Do not run this on the head node! # Grab an interactive node sinteractive --mem = 110g --cpus-per-task = 12 --gres = lscratch:200 module load ccbrpipeliner renee run --input .tests/*.R?.fastq.gz --output /data/ $USER /RNA_hg38 --genome hg38_36 --mode local # @slurm: uses slurm and singularity execution method # The slurm MODE will submit jobs to the cluster. # The --sif-cache flag will re-use singularity containers from a shared location. # It is recommended running RENEE in this mode. module load ccbrpipeliner renee run \\ --input .tests/*.R?.fastq.gz \\ --output /data/ $USER /RNA_hg38 \\ --genome hg38_36 \\ --mode slurm \\ --sif-cache /data/CCBR_Pipeliner/SIFs","title":"3.1 Biowulf"},{"location":"#32-frce","text":"# grab an interactive node srun --export all --pty --x11 bash # add renee to path correctly . /mnt/projects/CCBR-Pipelines/pipelines/guis/latest/bin/setup # run renee renee --help When running renee on FRCE, we recommend setting --tmp-dir and --sif-cache with the following values: renee run \\ --input .tests/*.R?.fastq.gz \\ --output /scratch/cluster_scratch/ $USER /RNA_hg38 \\ --genome hg38_36 \\ --mode slurm \\ --tmp-dir /scratch/cluster_scratch/ $USER \\ --sif-cache /mnt/projects/CCBR-Pipelines/SIFs Back to Top","title":"3.2 FRCE"},{"location":"#4-references","text":"1. Daley, T. and A.D. Smith, Predicting the molecular complexity of sequencing libraries. Nat Methods, 2013. 10(4): p. 325-7. 2. Andrews, S. (2010). FastQC: a quality control tool for high throughput sequence data. 3. Martin, M. (2011). \"Cutadapt removes adapter sequences from high-throughput sequencing reads.\" EMBnet 17(1): 10-12. 4. Dobin, A., et al., STAR: ultrafast universal RNA-seq aligner. Bioinformatics, 2013. 29(1): p. 15-21. 5. Li, B. and C.N. Dewey, RSEM: accurate transcript quantification from RNA-Seq data with or without a reference genome. BMC Bioinformatics, 2011. 12: p. 323. 6. Harrow, J., et al., GENCODE: the reference human genome annotation for The ENCODE Project. Genome Res, 2012. 22(9): p. 1760-74. 7. Law, C.W., et al., voom: Precision weights unlock linear model analysis tools for RNA-seq read counts. Genome Biol, 2014. 15(2): p. R29. 8. Smyth, G.K., Linear models and empirical bayes methods for assessing differential expression in microarray experiments. Stat Appl Genet Mol Biol, 2004. 3: p. Article3. 9. Wang, L., et al. (2012). \"RSeQC: quality control of RNA-seq experiments.\" Bioinformatics 28(16): 2184-2185. 10. The Picard toolkit. https://broadinstitute.github.io/picard/. 11. Ewels, P., et al. (2016). \"MultiQC: summarize analysis results for multiple tools and samples in a single report.\" Bioinformatics 32(19): 3047-3048. 12. R Core Team (2018). R: A Language and Environment for Statistical Computing. Vienna, Austria, R Foundation for Statistical Computing. 13. Li, H., et al. (2009). \"The Sequence Alignment/Map format and SAMtools.\" Bioinformatics 25(16): 2078-2079. 14. Wood, D. E. and S. L. Salzberg (2014). \"Kraken: ultrafast metagenomic sequence classification using exact alignments.\" Genome Biol 15(3): R46. 15. Ondov, B. D., et al. (2011). \"Interactive metagenomic visualization in a Web browser.\" BMC Bioinformatics 12(1): 385. 16. Okonechnikov, K., et al. (2015). \"Qualimap 2: advanced multi-sample quality control for high-throughput sequencing data.\" Bioinformatics 32(2): 292-294. 17. Wingett, S. and S. Andrews (2018). \"FastQ Screen: A tool for multi-genome mapping and quality control.\" F1000Research 7(2): 1338. 18. Robinson, M. D., et al. (2009). \"edgeR: a Bioconductor package for differential expression analysis of digital gene expression data.\" Bioinformatics 26(1): 139-140. 19. Koster, J. and S. Rahmann (2018). \"Snakemake-a scalable bioinformatics workflow engine.\" Bioinformatics 34(20): 3600. 20. Merkel, D. (2014). Docker: lightweight linux containers for consistent development and deployment. Linux Journal, 2014(239), 2. 21. Kurtzer GM, Sochat V, Bauer MW (2017). Singularity: Scientific containers for mobility of compute. PLoS ONE 12(5): e0177459. 22. Haas, B. J., et al. (2019). \"Accuracy assessment of fusion transcript detection via read-mapping and de novo fusion transcript assembly-based methods.\" Genome Biology 20(1): 213. Back to Top","title":"4. References"},{"location":"contributing/","text":"Contributing to RENEE \u00b6 Proposing changes with issues \u00b6 If you want to make a change, it's a good idea to first open an issue and make sure someone from the team agrees that it\u2019s needed. If you've decided to work on an issue, assign yourself to the issue so others will know you're working on it. Pull request process \u00b6 We use GitHub Flow as our collaboration process. Follow the steps below for detailed instructions on contributing changes to RENEE. Clone the repo \u00b6 If you are a member of CCBR , you can clone this repository to your computer or development environment. Otherwise, you will first need to fork the repo and clone your fork. You only need to do this step once. git clone https://github.com/CCBR/RENEE Cloning into 'RENEE'... remote: Enumerating objects: 1136, done. remote: Counting objects: 100% (463/463), done. remote: Compressing objects: 100% (357/357), done. remote: Total 1136 (delta 149), reused 332 (delta 103), pack-reused 673 Receiving objects: 100% (1136/1136), 11.01 MiB | 9.76 MiB/s, done. Resolving deltas: 100% (530/530), done. cd RENEE If this is your first time cloning the repo, you may need to install dependencies \u00b6 Install snakemake and singularity or docker if needed (biowulf already has these available as modules). Install the python dependencies with pip pip install . If you're developing on biowulf, you can use our shared conda environment which already has these dependencies installed . \"/data/CCBR_Pipeliner/db/PipeDB/Conda/etc/profile.d/conda.sh\" conda activate py311 Install pre-commit if you don't already have it. Then from the repo's root directory, run pre-commit install This will install the repo's pre-commit hooks. You'll only need to do this step the first time you clone the repo. Create a branch \u00b6 Create a Git branch for your pull request (PR). Give the branch a descriptive name for the changes you will make, such as iss-10 if it is for a specific issue. # create a new branch and switch to it git branch iss-10 git switch iss-10 Switched to a new branch 'iss-10' Make your changes \u00b6 Edit the code, write and run tests, and update the documentation as needed. test \u00b6 Changes to the python package code will also need unit tests to demonstrate that the changes work as intended. We write unit tests with pytest and store them in the tests/ subdirectory. Run the tests with python -m pytest . If you change the workflow , please run the workflow with the test profile and make sure your new feature or bug fix works as intended. document \u00b6 If you have added a new feature or changed the API of an existing feature, you will likely need to update the documentation in docs/ . Commit and push your changes \u00b6 If you're not sure how often you should commit or what your commits should consist of, we recommend following the \"atomic commits\" principle where each commit contains one new feature, fix, or task. Learn more about atomic commits here: https://www.freshconsulting.com/insights/blog/atomic-commits/ First, add the files that you changed to the staging area: git add path/to/changed/files/ Then make the commit. Your commit message should follow the Conventional Commits specification. Briefly, each commit should start with one of the approved types such as feat , fix , docs , etc. followed by a description of the commit. Take a look at the Conventional Commits specification for more detailed information about how to write commit messages. git commit -m 'feat: create function for awesome feature' pre-commit will enforce that your commit message and the code changes are styled correctly and will attempt to make corrections if needed. Check for added large files..............................................Passed Fix End of Files.........................................................Passed Trim Trailing Whitespace.................................................Failed hook id: trailing-whitespace exit code: 1 files were modified by this hook > Fixing path/to/changed/files/file.txt > codespell................................................................Passed style-files..........................................(no files to check)Skipped readme-rmd-rendered..................................(no files to check)Skipped use-tidy-description.................................(no files to check)Skipped In the example above, one of the hooks modified a file in the proposed commit, so the pre-commit check failed. You can run git diff to see the changes that pre-commit made and git status to see which files were modified. To proceed with the commit, re-add the modified file(s) and re-run the commit command: git add path/to/changed/files/file.txt git commit -m 'feat: create function for awesome feature' This time, all the hooks either passed or were skipped (e.g. hooks that only run on R code will not run if no R files were committed). When the pre-commit check is successful, the usual commit success message will appear after the pre-commit messages showing that the commit was created. Check for added large files..............................................Passed Fix End of Files.........................................................Passed Trim Trailing Whitespace.................................................Passed codespell................................................................Passed style-files..........................................(no files to check)Skipped readme-rmd-rendered..................................(no files to check)Skipped use-tidy-description.................................(no files to check)Skipped Conventional Commit......................................................Passed > [iss-10 9ff256e] feat: create function for awesome feature 1 file changed, 22 insertions(+), 3 deletions(-) Finally, push your changes to GitHub: git push If this is the first time you are pushing this branch, you may have to explicitly set the upstream branch: git push --set-upstream origin iss-10 Enumerating objects: 7, done. Counting objects: 100% (7/7), done. Delta compression using up to 10 threads Compressing objects: 100% (4/4), done. Writing objects: 100% (4/4), 648 bytes | 648.00 KiB/s, done. Total 4 (delta 3), reused 0 (delta 0), pack-reused 0 remote: Resolving deltas: 100% (3/3), completed with 3 local objects. remote: remote: Create a pull request for 'iss-10' on GitHub by visiting: remote: https://github.com/CCBR/RENEE/pull/new/iss-10 remote: To https://github.com/CCBR/RENEE > > [new branch] iss-10 -> iss-10 branch 'iss-10' set up to track 'origin/iss-10'. We recommend pushing your commits often so they will be backed up on GitHub. You can view the files in your branch on GitHub at https://github.com/CCBR/RENEE/tree/<your-branch-name> (replace <your-branch-name> with the actual name of your branch). Create the PR \u00b6 Once your branch is ready, create a PR on GitHub: https://github.com/CCBR/RENEE/pull/new/ Select the branch you just pushed: Edit the PR title and description. The title should briefly describe the change. Follow the comments in the template to fill out the body of the PR, and you can delete the comments (everything between <!-- and --> ) as you go. Be sure to fill out the checklist, checking off items as you complete them or striking through any irrelevant items. When you're ready, click 'Create pull request' to open it. Optionally, you can mark the PR as a draft if you're not yet ready for it to be reviewed, then change it later when you're ready. Wait for a maintainer to review your PR \u00b6 We will do our best to follow the tidyverse code review principles: https://code-review.tidyverse.org/ . The reviewer may suggest that you make changes before accepting your PR in order to improve the code quality or style. If that's the case, continue to make changes in your branch and push them to GitHub, and they will appear in the PR. Once the PR is approved, the maintainer will merge it and the issue(s) the PR links will close automatically. Congratulations and thank you for your contribution! After your PR has been merged \u00b6 After your PR has been merged, update your local clone of the repo by switching to the main branch and pulling the latest changes: git checkout main git pull It's a good idea to run git pull before creating a new branch so it will start from the most recent commits in main. Helpful links for more information \u00b6 GitHub Flow semantic versioning guidelines changelog guidelines tidyverse code review principles reproducible examples nf-core extensions for VS Code","title":"How to contribute"},{"location":"contributing/#contributing-to-renee","text":"","title":"Contributing to RENEE"},{"location":"contributing/#proposing-changes-with-issues","text":"If you want to make a change, it's a good idea to first open an issue and make sure someone from the team agrees that it\u2019s needed. If you've decided to work on an issue, assign yourself to the issue so others will know you're working on it.","title":"Proposing changes with issues"},{"location":"contributing/#pull-request-process","text":"We use GitHub Flow as our collaboration process. Follow the steps below for detailed instructions on contributing changes to RENEE.","title":"Pull request process"},{"location":"contributing/#clone-the-repo","text":"If you are a member of CCBR , you can clone this repository to your computer or development environment. Otherwise, you will first need to fork the repo and clone your fork. You only need to do this step once. git clone https://github.com/CCBR/RENEE Cloning into 'RENEE'... remote: Enumerating objects: 1136, done. remote: Counting objects: 100% (463/463), done. remote: Compressing objects: 100% (357/357), done. remote: Total 1136 (delta 149), reused 332 (delta 103), pack-reused 673 Receiving objects: 100% (1136/1136), 11.01 MiB | 9.76 MiB/s, done. Resolving deltas: 100% (530/530), done. cd RENEE","title":"Clone the repo"},{"location":"contributing/#if-this-is-your-first-time-cloning-the-repo-you-may-need-to-install-dependencies","text":"Install snakemake and singularity or docker if needed (biowulf already has these available as modules). Install the python dependencies with pip pip install . If you're developing on biowulf, you can use our shared conda environment which already has these dependencies installed . \"/data/CCBR_Pipeliner/db/PipeDB/Conda/etc/profile.d/conda.sh\" conda activate py311 Install pre-commit if you don't already have it. Then from the repo's root directory, run pre-commit install This will install the repo's pre-commit hooks. You'll only need to do this step the first time you clone the repo.","title":"If this is your first time cloning the repo, you may need to install dependencies"},{"location":"contributing/#create-a-branch","text":"Create a Git branch for your pull request (PR). Give the branch a descriptive name for the changes you will make, such as iss-10 if it is for a specific issue. # create a new branch and switch to it git branch iss-10 git switch iss-10 Switched to a new branch 'iss-10'","title":"Create a branch"},{"location":"contributing/#make-your-changes","text":"Edit the code, write and run tests, and update the documentation as needed.","title":"Make your changes"},{"location":"contributing/#test","text":"Changes to the python package code will also need unit tests to demonstrate that the changes work as intended. We write unit tests with pytest and store them in the tests/ subdirectory. Run the tests with python -m pytest . If you change the workflow , please run the workflow with the test profile and make sure your new feature or bug fix works as intended.","title":"test"},{"location":"contributing/#document","text":"If you have added a new feature or changed the API of an existing feature, you will likely need to update the documentation in docs/ .","title":"document"},{"location":"contributing/#commit-and-push-your-changes","text":"If you're not sure how often you should commit or what your commits should consist of, we recommend following the \"atomic commits\" principle where each commit contains one new feature, fix, or task. Learn more about atomic commits here: https://www.freshconsulting.com/insights/blog/atomic-commits/ First, add the files that you changed to the staging area: git add path/to/changed/files/ Then make the commit. Your commit message should follow the Conventional Commits specification. Briefly, each commit should start with one of the approved types such as feat , fix , docs , etc. followed by a description of the commit. Take a look at the Conventional Commits specification for more detailed information about how to write commit messages. git commit -m 'feat: create function for awesome feature' pre-commit will enforce that your commit message and the code changes are styled correctly and will attempt to make corrections if needed. Check for added large files..............................................Passed Fix End of Files.........................................................Passed Trim Trailing Whitespace.................................................Failed hook id: trailing-whitespace exit code: 1 files were modified by this hook > Fixing path/to/changed/files/file.txt > codespell................................................................Passed style-files..........................................(no files to check)Skipped readme-rmd-rendered..................................(no files to check)Skipped use-tidy-description.................................(no files to check)Skipped In the example above, one of the hooks modified a file in the proposed commit, so the pre-commit check failed. You can run git diff to see the changes that pre-commit made and git status to see which files were modified. To proceed with the commit, re-add the modified file(s) and re-run the commit command: git add path/to/changed/files/file.txt git commit -m 'feat: create function for awesome feature' This time, all the hooks either passed or were skipped (e.g. hooks that only run on R code will not run if no R files were committed). When the pre-commit check is successful, the usual commit success message will appear after the pre-commit messages showing that the commit was created. Check for added large files..............................................Passed Fix End of Files.........................................................Passed Trim Trailing Whitespace.................................................Passed codespell................................................................Passed style-files..........................................(no files to check)Skipped readme-rmd-rendered..................................(no files to check)Skipped use-tidy-description.................................(no files to check)Skipped Conventional Commit......................................................Passed > [iss-10 9ff256e] feat: create function for awesome feature 1 file changed, 22 insertions(+), 3 deletions(-) Finally, push your changes to GitHub: git push If this is the first time you are pushing this branch, you may have to explicitly set the upstream branch: git push --set-upstream origin iss-10 Enumerating objects: 7, done. Counting objects: 100% (7/7), done. Delta compression using up to 10 threads Compressing objects: 100% (4/4), done. Writing objects: 100% (4/4), 648 bytes | 648.00 KiB/s, done. Total 4 (delta 3), reused 0 (delta 0), pack-reused 0 remote: Resolving deltas: 100% (3/3), completed with 3 local objects. remote: remote: Create a pull request for 'iss-10' on GitHub by visiting: remote: https://github.com/CCBR/RENEE/pull/new/iss-10 remote: To https://github.com/CCBR/RENEE > > [new branch] iss-10 -> iss-10 branch 'iss-10' set up to track 'origin/iss-10'. We recommend pushing your commits often so they will be backed up on GitHub. You can view the files in your branch on GitHub at https://github.com/CCBR/RENEE/tree/<your-branch-name> (replace <your-branch-name> with the actual name of your branch).","title":"Commit and push your changes"},{"location":"contributing/#create-the-pr","text":"Once your branch is ready, create a PR on GitHub: https://github.com/CCBR/RENEE/pull/new/ Select the branch you just pushed: Edit the PR title and description. The title should briefly describe the change. Follow the comments in the template to fill out the body of the PR, and you can delete the comments (everything between <!-- and --> ) as you go. Be sure to fill out the checklist, checking off items as you complete them or striking through any irrelevant items. When you're ready, click 'Create pull request' to open it. Optionally, you can mark the PR as a draft if you're not yet ready for it to be reviewed, then change it later when you're ready.","title":"Create the PR"},{"location":"contributing/#wait-for-a-maintainer-to-review-your-pr","text":"We will do our best to follow the tidyverse code review principles: https://code-review.tidyverse.org/ . The reviewer may suggest that you make changes before accepting your PR in order to improve the code quality or style. If that's the case, continue to make changes in your branch and push them to GitHub, and they will appear in the PR. Once the PR is approved, the maintainer will merge it and the issue(s) the PR links will close automatically. Congratulations and thank you for your contribution!","title":"Wait for a maintainer to review your PR"},{"location":"contributing/#after-your-pr-has-been-merged","text":"After your PR has been merged, update your local clone of the repo by switching to the main branch and pulling the latest changes: git checkout main git pull It's a good idea to run git pull before creating a new branch so it will start from the most recent commits in main.","title":"After your PR has been merged"},{"location":"contributing/#helpful-links-for-more-information","text":"GitHub Flow semantic versioning guidelines changelog guidelines tidyverse code review principles reproducible examples nf-core extensions for VS Code","title":"Helpful links for more information"},{"location":"general-questions/","text":"If you are experiencing an issue, please read through this list first before contacting our team. We have compiled this FAQ from the most common questions. If you have a question that is not on this page, please feel free to reach out to our team . Contribute \u00b6 Q. I would like to contribute to RNA-seek. How do I get involved? A. There are several ways you can get involved with the project. If you have added new features or adding new changes, please consider contributing them back to the original repository: Fork the original repo to a personal or org account. Clone the fork to your local filesystem. Copy the modified files to the cloned fork. Commit and push your changes to your fork. Create a pull request to this repository. If you would like to create or tackle an issue, please reference our issue tracker on Github. Also, feel free to like or :star: the project on Github! Additional Support \u00b6 Q. I have a few questions about the pipeline. How can I reach you? A. For general questions and/or support, please free to open an issue on Github or send an email to CCBR","title":"General Questions"},{"location":"general-questions/#contribute","text":"Q. I would like to contribute to RNA-seek. How do I get involved? A. There are several ways you can get involved with the project. If you have added new features or adding new changes, please consider contributing them back to the original repository: Fork the original repo to a personal or org account. Clone the fork to your local filesystem. Copy the modified files to the cloned fork. Commit and push your changes to your fork. Create a pull request to this repository. If you would like to create or tackle an issue, please reference our issue tracker on Github. Also, feel free to like or :star: the project on Github!","title":"Contribute"},{"location":"general-questions/#additional-support","text":"Q. I have a few questions about the pipeline. How can I reach you? A. For general questions and/or support, please free to open an issue on Github or send an email to CCBR","title":"Additional Support"},{"location":"license/","text":"MIT License \u00b6 Copyright \u00a9 2023 CCBR Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"license/#mit-license","text":"Copyright \u00a9 2023 CCBR Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"MIT License"},{"location":"troubleshooting/","text":"If you are experiencing an issue, please read through this list first before contacting our team. We have compiled this FAQ from the most common problems. If you are running into an issue that is not on this page, please feel free to reach out to our team . Job Status \u00b6 Q: How do I know if RENEE pipeline finished running? How to check status of each job? A. Once the pipeline is done running to completion, you will receive an email with header like Slurm Job_id=xxxx Name=pl:renee Ended, Run time xx:xx:xx, COMPLETED, ExitCode 0 To check the status of each individual job submitted to the cluster, there are several different ways. Here are a few suggestions: Check Job Status Biowulf Dashboard You can check the status of Biowulf jobs through the your user dashboard . Each job that RENEE submits to the cluster starts with the pl: prefix. Query Job Scheduler SLURM has built-in commands that allow a user to view the status of jobs submitted to the cluster. Method 1: To see what jobs you have running, run the following command: squeue -u $USER Method 2 You can also run this alternative command to check the status of your running jobs: sjobs Each job that RENEE submits to the cluster starts with the pl: prefix. Q: What if the pipeline is finished running but I received a \"FAILED\" status? How do I identify failed jobs? A. In case there was some error during the run, the easiest way to diagnose the problem is to go to logfiles folder within the RENEE output folder and look at the snakemake.log.jobby.short file. It contains three columns: jobname, state, and std_err. The jobs that completed successfully would have \"COMPLETED\" state and jobs that failed would have the FAILED state. Find Failed Jobs SLURM output files All the failed jobs would be listed with absolute paths to the error file (with extension .err ). Go through the error files corresponding to the FAILED jobs (std_err) to explore why the job failed. # Go to the logfiles folder within the renee output folder cd renee_output/logfiles # List the files that failed grep \"FAILED\" snakemake.log.jobby.short | less Many failures are caused by filesystem or network issues on Biowulf, and in such cases, simply re-starting the Pipeline should resolve the issue. Snakemake will dynamically determine which steps have been completed, and which steps still need to be run. If you are still running into problems after re-running the pipeline, there may be another issue. If that is the case, please feel free to contact us . Q. How do I cancel ongoing RENEE jobs? A. Sometimes, you might need to manually stop a RENEE run prematurely, perhaps because the run was configured incorrectly or if a job is stalled. Although the walltime limits will eventually stop the workflow, this can take up to 5 or 10 days depending on the pipeline. To stop RENEE jobs that are currently running, you can follow these options. Cancel running jobs Master Job You can use the sjobs tool provided by Biowulf to monitor ongoing jobs. Examine the NAME column of the sjobs output, one of them should match pl:renee . This is the \"primary\" job that orchestrates the submission of child jobs as the pipeline completes. Terminating this job will ensure that the pipeline is cancelled; however, you will likely need to unlock the working directory before re-running renee again. Please see our instructions below in Error: Directory cannot be locked for how to unlock a working directory. You can manually cancel the primary job using scancel . However, secondary jobs that are already running will continue to completion (or failure). To stop them immediately, you will need to run scancel individually for each secondary job. See the next tab for a bash script that tries to automate this process. Child Jobs When there are lots of secondary jobs running, or if you have multiple RENEE runs ongoing simultaneously, it's not feasible to manually cancel jobs based on the sjobs output (see previous tab). We provide a script that will parse the snakemake log file and cancel all jobs listed within. ## Download the script (to the current directory) wget https://raw.githubusercontent.com/CCBR/Tools/c3324fc0ad2f9858438c84bbb2f24927a8f3a220/scripts/cancel_snakemake_jobs.sh ## Run the script bash cancel_snakemake_jobs.sh /path/to/output/logfiles/snakemake.log The script accepts one argument, which should be the path to the snakemake log file. This will work for any log output from Snakemake. This script will NOT cancel the primary job, which you will still have to identify and cancel manually, as described in the previous tab. Once you've ensured that all running jobs have been stopped, you need to unlock the working directory (see below), and re-run RENEE to resume the pipeline. Job Errors \u00b6 Q. Why am I getting sbatch: command not found error ? A. Are you running the renee on helix.nih.gov by mistake? Helix does not have a job scheduler. One may be able to fire up the singularity module, initial working directory and perform dry-run on helix . But to submit jobs, you need to log into biowulf using ssh -Y username@biowulf.nih.gov . Q. Why am I getting a message saying Error: Directory cannot be locked. ... when I do the dry-run? A. This is caused when a run is stopped prematurely, either accidentally or on purpose, or the pipeline is still running in your working directory. Snakemake will lock a working directory to prevent two concurrent pipelines from writing to the same location. This can be remedied easily by running renee unlock sub command. Please check to see if the pipeline is still running prior to running the commands below. If you would like to cancel a submitted or running pipeline, please reference the instructions above. # Load Dependencies module load ccbrpipeliner # Unlock the working directory renee unlock --output /path/to/working/dir Q. Why am I getting a message saying MissingInputException in line ... when I do the dry-run? A. This error usually occurs when snakemake is terminated ungracefully. Did you forcefully cancel a running pipeline? Or did one of your running pipelines abruptly end? Either way, the solution is straight-forward. Please go to your pipeline's output directory, and rename or delete the following hidden directory: .snakemake/ . This directory contains metadata pertaining any snakemake runs inside that working directory. Sometimes when a pipeline is pre-maturely or forcefully terminated, a few files related to tracking temp() files are not deleted and snakemake raises a MissingInputException. # Navigate to working directory cd /path/to/working/dir # Rename .snakemake directory to something else # And try re-dry running the pipeline mv .snakemake .old_snakemake","title":"Troubleshooting"},{"location":"troubleshooting/#job-status","text":"Q: How do I know if RENEE pipeline finished running? How to check status of each job? A. Once the pipeline is done running to completion, you will receive an email with header like Slurm Job_id=xxxx Name=pl:renee Ended, Run time xx:xx:xx, COMPLETED, ExitCode 0 To check the status of each individual job submitted to the cluster, there are several different ways. Here are a few suggestions: Check Job Status Biowulf Dashboard You can check the status of Biowulf jobs through the your user dashboard . Each job that RENEE submits to the cluster starts with the pl: prefix. Query Job Scheduler SLURM has built-in commands that allow a user to view the status of jobs submitted to the cluster. Method 1: To see what jobs you have running, run the following command: squeue -u $USER Method 2 You can also run this alternative command to check the status of your running jobs: sjobs Each job that RENEE submits to the cluster starts with the pl: prefix. Q: What if the pipeline is finished running but I received a \"FAILED\" status? How do I identify failed jobs? A. In case there was some error during the run, the easiest way to diagnose the problem is to go to logfiles folder within the RENEE output folder and look at the snakemake.log.jobby.short file. It contains three columns: jobname, state, and std_err. The jobs that completed successfully would have \"COMPLETED\" state and jobs that failed would have the FAILED state. Find Failed Jobs SLURM output files All the failed jobs would be listed with absolute paths to the error file (with extension .err ). Go through the error files corresponding to the FAILED jobs (std_err) to explore why the job failed. # Go to the logfiles folder within the renee output folder cd renee_output/logfiles # List the files that failed grep \"FAILED\" snakemake.log.jobby.short | less Many failures are caused by filesystem or network issues on Biowulf, and in such cases, simply re-starting the Pipeline should resolve the issue. Snakemake will dynamically determine which steps have been completed, and which steps still need to be run. If you are still running into problems after re-running the pipeline, there may be another issue. If that is the case, please feel free to contact us . Q. How do I cancel ongoing RENEE jobs? A. Sometimes, you might need to manually stop a RENEE run prematurely, perhaps because the run was configured incorrectly or if a job is stalled. Although the walltime limits will eventually stop the workflow, this can take up to 5 or 10 days depending on the pipeline. To stop RENEE jobs that are currently running, you can follow these options. Cancel running jobs Master Job You can use the sjobs tool provided by Biowulf to monitor ongoing jobs. Examine the NAME column of the sjobs output, one of them should match pl:renee . This is the \"primary\" job that orchestrates the submission of child jobs as the pipeline completes. Terminating this job will ensure that the pipeline is cancelled; however, you will likely need to unlock the working directory before re-running renee again. Please see our instructions below in Error: Directory cannot be locked for how to unlock a working directory. You can manually cancel the primary job using scancel . However, secondary jobs that are already running will continue to completion (or failure). To stop them immediately, you will need to run scancel individually for each secondary job. See the next tab for a bash script that tries to automate this process. Child Jobs When there are lots of secondary jobs running, or if you have multiple RENEE runs ongoing simultaneously, it's not feasible to manually cancel jobs based on the sjobs output (see previous tab). We provide a script that will parse the snakemake log file and cancel all jobs listed within. ## Download the script (to the current directory) wget https://raw.githubusercontent.com/CCBR/Tools/c3324fc0ad2f9858438c84bbb2f24927a8f3a220/scripts/cancel_snakemake_jobs.sh ## Run the script bash cancel_snakemake_jobs.sh /path/to/output/logfiles/snakemake.log The script accepts one argument, which should be the path to the snakemake log file. This will work for any log output from Snakemake. This script will NOT cancel the primary job, which you will still have to identify and cancel manually, as described in the previous tab. Once you've ensured that all running jobs have been stopped, you need to unlock the working directory (see below), and re-run RENEE to resume the pipeline.","title":"Job Status"},{"location":"troubleshooting/#job-errors","text":"Q. Why am I getting sbatch: command not found error ? A. Are you running the renee on helix.nih.gov by mistake? Helix does not have a job scheduler. One may be able to fire up the singularity module, initial working directory and perform dry-run on helix . But to submit jobs, you need to log into biowulf using ssh -Y username@biowulf.nih.gov . Q. Why am I getting a message saying Error: Directory cannot be locked. ... when I do the dry-run? A. This is caused when a run is stopped prematurely, either accidentally or on purpose, or the pipeline is still running in your working directory. Snakemake will lock a working directory to prevent two concurrent pipelines from writing to the same location. This can be remedied easily by running renee unlock sub command. Please check to see if the pipeline is still running prior to running the commands below. If you would like to cancel a submitted or running pipeline, please reference the instructions above. # Load Dependencies module load ccbrpipeliner # Unlock the working directory renee unlock --output /path/to/working/dir Q. Why am I getting a message saying MissingInputException in line ... when I do the dry-run? A. This error usually occurs when snakemake is terminated ungracefully. Did you forcefully cancel a running pipeline? Or did one of your running pipelines abruptly end? Either way, the solution is straight-forward. Please go to your pipeline's output directory, and rename or delete the following hidden directory: .snakemake/ . This directory contains metadata pertaining any snakemake runs inside that working directory. Sometimes when a pipeline is pre-maturely or forcefully terminated, a few files related to tracking temp() files are not deleted and snakemake raises a MissingInputException. # Navigate to working directory cd /path/to/working/dir # Rename .snakemake directory to something else # And try re-dry running the pipeline mv .snakemake .old_snakemake","title":"Job Errors"},{"location":"RNA-seq/Resources/","text":"1. Reference genomes \u00b6 On Biowulf , RENEE comes bundled with the following pre-built GENCODE 1 reference genomes: As of RENEE v2.6.0, all hg19 and hg38 indices were built using the NCI Genomic Data Commons reference fasta , which contains the primary genome from Encode plus virus and decoy sequences. The hg38 fasta files were downloaded from the GDC with virus and decoy sequences already added, while these sequences were manually added to the hg19 fasta from Encode. See details here: https://github.com/CCBR/build-renee-refs Genome Species Annotation Version Notes hg19_19 Homo sapiens (human) Gencode Release 19 GRCh37 , Annotation Release date: 07/2013 hg19_36 Homo sapiens (human) Gencode Release 36-lift-37 GRCh37 hg38_30 Homo sapiens (human) Gencode Release 30 GRCh38 , Annotation Release date: 11/2018 hg38_34 Homo sapiens (human) Gencode Release 34 GRCh38 , Annotation Release date: 04/2020 hg38_36 Homo sapiens (human) Gencode Release 36 GRCh38 , Annotation Release date: 05/2020 hg38_38 Homo sapiens (human) Gencode Release 38 GRCh38 , Annotation Release date: 05/2021 hg38_41 Homo sapiens (human) Gencode Release 41 GRCh38 , Annotation Release date: 07/2022 hg38_45 Homo sapiens (human) Gencode Release 45 GRCh38 , Annotation Release date: 03/2023 mm10_M21 Mus musculus (mouse) Gencode Release M21 GRCm38 , Annotation Release date: 04/2019 mm10_M23 Mus musculus (mouse) Gencode Release M23 GRCm38 , Annotation Release date: 09/2019 mm10_M25 Mus musculus (mouse) Gencode Release M25 GRCm38 , Annotation Release date: 04/2020 mCalJac1_2021 Callithrix jacchus (white-tufted-ear marmoset) Genome assembly mCalJa1.2.pat.X Annotation release date: 04/2021 You can run renee run --help to view the most up-to-date list of genome annotations available in your installation of RENEE. Note: Newer annotations versions may be added upon request and may be already available. Please contact Vishal Koparde for details. However, building new reference genomes is easy! If you do not have access to Biowulf or you are looking for a reference genome and/or annotation that is currently not available , it can be built with RENEE's build sub-command. Given a genomic FASTA file (ref.fa) and a GTF file (genes.gtf), renee build will create all of the required reference files to run the RENEE pipeline. Once the build pipeline completes, you can supply the newly generated reference.json to the --genome of renee run . For more information, please see the help page for the run and build sub commands. 2. Tools and versions \u00b6 Raw data > Adapter Trimming > Alignment > Quantification (genes and isoforms, gene-fusions) Tool Version Docker Notes FastQC 2 0.11.9 nciccbr/ccbr_fastqc_0.11.9 Quality-control step to assess sequencing quality, run before and after adapter trimming Cutadapt 3 1.18 nciccbr/ccbr_cutadapt_1.18 Data processing step to remove adapter sequences and perform quality trimming Kraken 4 2.1.1 nciccbr/ccbr_kraken_v2.1.1 Quality-control step to assess microbial taxonomic composition KronaTools 5 2.7.1 nciccbr/ccbr_kraken_v2.1.1 Quality-control step to visualize kraken output FastQ Screen 6 0.13.0 nciccbr/ccbr_fastq_screen_0.13.0 Quality-control step to assess contamination; additional dependencies: bowtie2/2.3.4 , perl/5.24.3 STAR 7 2.7.6a nciccbr/ccbr_arriba_2.0.0 Data processing step to align reads against reference genome (using its two-pass mode) bbtools 8 38.87 nciccbr/ccbr_bbtools_38.87 Quality-control step to calculate insert_size of assembled reads pairs with bbmerge QualiMap 9 2.2.1 nciccbr/ccbr_qualimap Quality-control step to assess various alignment metrics Picard 10 2.18.20 nciccbr/ccbr_picard Quality-control step to run MarkDuplicates , CollectRnaSeqMetrics and AddOrReplaceReadGroups Preseq 11 2.0.3 nciccbr/ccbr_preseq Quality-control step to estimate library complexity SAMtools 12 1.7 nciccbr/ccbr_arriba_2.0.0 Quality-control step to run flagstat to calculate alignment statistics bam2strandedbw custom nciccbr/ccbr_bam2strandedbw Summarization step to convert STAR aligned PE bam file into forward and reverse strand bigwigs suitable for a genomic track viewer like IGV RSeQC 13 4.0.0 nciccbr/ccbr_rseqc_4.0.0 Quality-control step to infer stranded-ness and read distributions over specific genomic features RSEM 14 1.3.3 nciccbr/ccbr_rsem_1.3.3 Data processing step to quantify gene and isoform counts Arriba 15 2.0.0 nciccbr/ccbr_arriba_2.0.0 Data processing step to quantify gene-fusions RNA Report custom nciccbr/ccbr_rna Summarization step to identify outliers and assess technical sources of variation MultiQC 16 1.12 skchronicles/multiqc Reporting step to aggregate sample statistics and quality-control information across all sample 3. Acknowledgements \u00b6 3.1 Biowulf \u00b6 If you utilized NIH's Biowulf cluster to run RENEE, please do not forget to provide an acknowlegement ! The continued growth and support of NIH's Biowulf cluster is dependent upon its demonstrable value to the NIH Intramural Research Program. If you publish research that involved significant use of Biowulf, please cite the cluster. Suggested citation text: This work utilized the computational resources of the NIH HPC Biowulf cluster. (http://hpc.nih.gov) 4. References \u00b6 1. Harrow, J., et al., GENCODE: the reference human genome annotation for The ENCODE Project. Genome Res, 2012. 22(9): p. 1760-74. 2. Andrews, S. (2010). FastQC: a quality control tool for high throughput sequence data. 3. Martin, M. (2011). \"Cutadapt removes adapter sequences from high-throughput sequencing reads.\" EMBnet 17(1): 10-12. 4. Wood, D. E. and S. L. Salzberg (2014). \"Kraken: ultrafast metagenomic sequence classification using exact alignments.\" Genome Biol 15(3): R46. 5. Ondov, B. D., et al. (2011). \"Interactive metagenomic visualization in a Web browser.\" BMC Bioinformatics 12(1): 385. 6. Wingett, S. and S. Andrews (2018). \"FastQ Screen: A tool for multi-genome mapping and quality control.\" F1000Research 7(2): 1338. 7. Dobin, A., et al., STAR: ultrafast universal RNA-seq aligner. Bioinformatics, 2013. 29(1): p. 15-21. 8. Bushnell, B., Rood, J., & Singer, E. (2017). BBMerge - Accurate paired shotgun read merging via overlap. PloS one, 12(10), e0185056. 9. Okonechnikov, K., et al. (2015). \"Qualimap 2: advanced multi-sample quality control for high-throughput sequencing data.\" Bioinformatics 32(2): 292-294. 10. The Picard toolkit. https://broadinstitute.github.io/picard/. 11. Daley, T. and A.D. Smith, Predicting the molecular complexity of sequencing libraries. Nat Methods, 2013. 10(4): p. 325-7. 12. Li, H., et al. (2009). \"The Sequence Alignment/Map format and SAMtools.\" Bioinformatics 25(16): 2078-2079. 13. Wang, L., et al. (2012). \"RSeQC: quality control of RNA-seq experiments.\" Bioinformatics 28(16): 2184-2185. 14. Li, B. and C.N. Dewey, RSEM: accurate transcript quantification from RNA-Seq data with or without a reference genome. BMC Bioinformatics, 2011. 12: p. 323. 15. Uhrig, S., et al. (2021). \"Accurate and efficient detection of gene fusions from RNA sequencing data\". Genome Res. 31(3): 448-460. 16. Ewels, P., et al. (2016). \"MultiQC: summarize analysis results for multiple tools and samples in a single report.\" Bioinformatics 32(19): 3047-3048.","title":"Resources"},{"location":"RNA-seq/Resources/#1-reference-genomes","text":"On Biowulf , RENEE comes bundled with the following pre-built GENCODE 1 reference genomes: As of RENEE v2.6.0, all hg19 and hg38 indices were built using the NCI Genomic Data Commons reference fasta , which contains the primary genome from Encode plus virus and decoy sequences. The hg38 fasta files were downloaded from the GDC with virus and decoy sequences already added, while these sequences were manually added to the hg19 fasta from Encode. See details here: https://github.com/CCBR/build-renee-refs Genome Species Annotation Version Notes hg19_19 Homo sapiens (human) Gencode Release 19 GRCh37 , Annotation Release date: 07/2013 hg19_36 Homo sapiens (human) Gencode Release 36-lift-37 GRCh37 hg38_30 Homo sapiens (human) Gencode Release 30 GRCh38 , Annotation Release date: 11/2018 hg38_34 Homo sapiens (human) Gencode Release 34 GRCh38 , Annotation Release date: 04/2020 hg38_36 Homo sapiens (human) Gencode Release 36 GRCh38 , Annotation Release date: 05/2020 hg38_38 Homo sapiens (human) Gencode Release 38 GRCh38 , Annotation Release date: 05/2021 hg38_41 Homo sapiens (human) Gencode Release 41 GRCh38 , Annotation Release date: 07/2022 hg38_45 Homo sapiens (human) Gencode Release 45 GRCh38 , Annotation Release date: 03/2023 mm10_M21 Mus musculus (mouse) Gencode Release M21 GRCm38 , Annotation Release date: 04/2019 mm10_M23 Mus musculus (mouse) Gencode Release M23 GRCm38 , Annotation Release date: 09/2019 mm10_M25 Mus musculus (mouse) Gencode Release M25 GRCm38 , Annotation Release date: 04/2020 mCalJac1_2021 Callithrix jacchus (white-tufted-ear marmoset) Genome assembly mCalJa1.2.pat.X Annotation release date: 04/2021 You can run renee run --help to view the most up-to-date list of genome annotations available in your installation of RENEE. Note: Newer annotations versions may be added upon request and may be already available. Please contact Vishal Koparde for details. However, building new reference genomes is easy! If you do not have access to Biowulf or you are looking for a reference genome and/or annotation that is currently not available , it can be built with RENEE's build sub-command. Given a genomic FASTA file (ref.fa) and a GTF file (genes.gtf), renee build will create all of the required reference files to run the RENEE pipeline. Once the build pipeline completes, you can supply the newly generated reference.json to the --genome of renee run . For more information, please see the help page for the run and build sub commands.","title":"1. Reference genomes"},{"location":"RNA-seq/Resources/#2-tools-and-versions","text":"Raw data > Adapter Trimming > Alignment > Quantification (genes and isoforms, gene-fusions) Tool Version Docker Notes FastQC 2 0.11.9 nciccbr/ccbr_fastqc_0.11.9 Quality-control step to assess sequencing quality, run before and after adapter trimming Cutadapt 3 1.18 nciccbr/ccbr_cutadapt_1.18 Data processing step to remove adapter sequences and perform quality trimming Kraken 4 2.1.1 nciccbr/ccbr_kraken_v2.1.1 Quality-control step to assess microbial taxonomic composition KronaTools 5 2.7.1 nciccbr/ccbr_kraken_v2.1.1 Quality-control step to visualize kraken output FastQ Screen 6 0.13.0 nciccbr/ccbr_fastq_screen_0.13.0 Quality-control step to assess contamination; additional dependencies: bowtie2/2.3.4 , perl/5.24.3 STAR 7 2.7.6a nciccbr/ccbr_arriba_2.0.0 Data processing step to align reads against reference genome (using its two-pass mode) bbtools 8 38.87 nciccbr/ccbr_bbtools_38.87 Quality-control step to calculate insert_size of assembled reads pairs with bbmerge QualiMap 9 2.2.1 nciccbr/ccbr_qualimap Quality-control step to assess various alignment metrics Picard 10 2.18.20 nciccbr/ccbr_picard Quality-control step to run MarkDuplicates , CollectRnaSeqMetrics and AddOrReplaceReadGroups Preseq 11 2.0.3 nciccbr/ccbr_preseq Quality-control step to estimate library complexity SAMtools 12 1.7 nciccbr/ccbr_arriba_2.0.0 Quality-control step to run flagstat to calculate alignment statistics bam2strandedbw custom nciccbr/ccbr_bam2strandedbw Summarization step to convert STAR aligned PE bam file into forward and reverse strand bigwigs suitable for a genomic track viewer like IGV RSeQC 13 4.0.0 nciccbr/ccbr_rseqc_4.0.0 Quality-control step to infer stranded-ness and read distributions over specific genomic features RSEM 14 1.3.3 nciccbr/ccbr_rsem_1.3.3 Data processing step to quantify gene and isoform counts Arriba 15 2.0.0 nciccbr/ccbr_arriba_2.0.0 Data processing step to quantify gene-fusions RNA Report custom nciccbr/ccbr_rna Summarization step to identify outliers and assess technical sources of variation MultiQC 16 1.12 skchronicles/multiqc Reporting step to aggregate sample statistics and quality-control information across all sample","title":"2. Tools and versions"},{"location":"RNA-seq/Resources/#3-acknowledgements","text":"","title":"3. Acknowledgements"},{"location":"RNA-seq/Resources/#31-biowulf","text":"If you utilized NIH's Biowulf cluster to run RENEE, please do not forget to provide an acknowlegement ! The continued growth and support of NIH's Biowulf cluster is dependent upon its demonstrable value to the NIH Intramural Research Program. If you publish research that involved significant use of Biowulf, please cite the cluster. Suggested citation text: This work utilized the computational resources of the NIH HPC Biowulf cluster. (http://hpc.nih.gov)","title":"3.1 Biowulf"},{"location":"RNA-seq/Resources/#4-references","text":"1. Harrow, J., et al., GENCODE: the reference human genome annotation for The ENCODE Project. Genome Res, 2012. 22(9): p. 1760-74. 2. Andrews, S. (2010). FastQC: a quality control tool for high throughput sequence data. 3. Martin, M. (2011). \"Cutadapt removes adapter sequences from high-throughput sequencing reads.\" EMBnet 17(1): 10-12. 4. Wood, D. E. and S. L. Salzberg (2014). \"Kraken: ultrafast metagenomic sequence classification using exact alignments.\" Genome Biol 15(3): R46. 5. Ondov, B. D., et al. (2011). \"Interactive metagenomic visualization in a Web browser.\" BMC Bioinformatics 12(1): 385. 6. Wingett, S. and S. Andrews (2018). \"FastQ Screen: A tool for multi-genome mapping and quality control.\" F1000Research 7(2): 1338. 7. Dobin, A., et al., STAR: ultrafast universal RNA-seq aligner. Bioinformatics, 2013. 29(1): p. 15-21. 8. Bushnell, B., Rood, J., & Singer, E. (2017). BBMerge - Accurate paired shotgun read merging via overlap. PloS one, 12(10), e0185056. 9. Okonechnikov, K., et al. (2015). \"Qualimap 2: advanced multi-sample quality control for high-throughput sequencing data.\" Bioinformatics 32(2): 292-294. 10. The Picard toolkit. https://broadinstitute.github.io/picard/. 11. Daley, T. and A.D. Smith, Predicting the molecular complexity of sequencing libraries. Nat Methods, 2013. 10(4): p. 325-7. 12. Li, H., et al. (2009). \"The Sequence Alignment/Map format and SAMtools.\" Bioinformatics 25(16): 2078-2079. 13. Wang, L., et al. (2012). \"RSeQC: quality control of RNA-seq experiments.\" Bioinformatics 28(16): 2184-2185. 14. Li, B. and C.N. Dewey, RSEM: accurate transcript quantification from RNA-Seq data with or without a reference genome. BMC Bioinformatics, 2011. 12: p. 323. 15. Uhrig, S., et al. (2021). \"Accurate and efficient detection of gene fusions from RNA sequencing data\". Genome Res. 31(3): 448-460. 16. Ewels, P., et al. (2016). \"MultiQC: summarize analysis results for multiple tools and samples in a single report.\" Bioinformatics 32(19): 3047-3048.","title":"4. References"},{"location":"RNA-seq/TLDR-RNA-seq/","text":"1. Introduction \u00b6 When processing RNA-sequencing data, there are often many steps that we must repeat. These are usually steps like removing adapter sequences, aligning reads against a reference genome, checking the quality of the data, and quantifying counts. RENEE is composed of several sub commands or convenience functions to automate these repetitive steps. With RENEE, you can run your samples through our highly-reproducible pipeline, build resources for new reference genomes, and more! Here is a list of available renee sub commands : run : run the rna-seq pipeline build : build reference files cache : cache remote resources locally unlock : unlock a working directory This page contains information for building reference files and running the RENEE pipeline. For more information about each of the available sub commands, please see the usage section . 2. Setup RENEE \u00b6 Estimated Reading Time: 3 Mintutes RENEE has two dependencies: singularity and snakemake . These dependencies can be installed by a sysadmin; however, snakemake is readily available through conda. Before running the pipeline or any of the commands below, please ensure singularity and snakemake are in your $PATH . Please see follow the instructions below for getting started with the RENEE pipeline. 2.1 Login to cluster \u00b6 # Setup Step 0.) ssh into cluster's head node # example below for Biowulf cluster ssh -Y $USER @biowulf.nih.gov 2.2 Grab an interactive node \u00b6 # Setup Step 1.) Please do not run RENEE on the head node! # Grab an interactive node first srun -N 1 -n 1 --time = 12 :00:00 -p interactive --mem = 8gb --cpus-per-task = 4 --pty bash 2.3 Load dependencies \u00b6 # Setup Step 2.) Add singularity and snakemake executables to $PATH module purge module load ccbrpipeliner 3. Building Reference files \u00b6 In this example, we will start off by building reference files downloaded from GENCODE . We recommend downloading the PRI Genome FASTA file and annotation from GENCODE . These PRI reference files contain the primary chromosomes and scaffolds. We do not recommend downloading the CHR reference files! Checkout this list for currently available resources on Biowulf. If your required genome + annotation combination is NOT available, only then proceed to building your own reference files. Also, if you think that your genome + annotation combination may be beneficial for other Biowulf users of RENEE as well, then please request it to be added to RENEE's default resources by opening an issue on Github . 3.1 Download References from GENCODE \u00b6 # Build Step 0.) Please do not run RENEE on the head node! # Grab an interactive node first # Assumes that you have already ssh-ed into cluster srun -N 1 -n 1 --time = 12 :00:00 -p interactive --mem = 8gb --cpus-per-task = 4 --pty bash # Build Step 1.) Download the PRI Genome FASTA file for GRCh38.p13 wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_36/GRCh38.primary_assembly.genome.fa.gz gzip -d GRCh38.primary_assembly.genome.fa.gz # Build Step 2.) Download the PRI release 36 annotation wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_36/gencode.v36.primary_assembly.annotation.gtf.gz gzip -d gencode.v36.primary_assembly.annotation.gtf.gz 3.2 Run Build pipeline \u00b6 # Build Step 3.) Load dependencies module purge module load ccbrpipeliner # Build Step 4.) Dry-run the build pipeline renee build --ref-fa GRCh38.primary_assembly.genome.fa \\ --ref-name hg38 \\ --ref-gtf gencode.v36.primary_assembly.annotation.gtf \\ --gtf-ver 36 --output /data/ $USER /hg38_36 --dry-run # Build Step 5.) Submit the build pipeline to cluster renee build --ref-fa GRCh38.primary_assembly.genome.fa \\ --ref-name hg38 \\ --ref-gtf gencode.v36.primary_assembly.annotation.gtf \\ --gtf-ver 36 --output /data/ $USER /hg38_36 An email notification will be sent out when the pipeline starts and ends. Once the build pipeline completes, you can run RENEE with the provided test dataset. Please see the instructions below for more information. 4. Running RENEE \u00b6 Run RENEE with the reference files we built above using hg38 (GRCh38.p13) Genome FASTA file and GENCODE release 36 annotation (GTF). For more information about how the reference files we generated, please see the instructions above. You can use those instructions as a guide for building any new reference genomes in the future. 4.1 Dry-run pipeline \u00b6 Dry-run the pipeline prior to submitting the pipeline's master job. Please note that if you wish to run RENEE with a new dataset, you will only need to update the values provided to the --input and --output arguments (and maybe --genome ). The --input argument supports globbing. If this is the first time running RENEE with for given dataset, the --output directory should not exist on your local filesystem. It will be created automatically during runtime. # Run Step 0.) Please do not run RENEE on the head node! # Grab an interactive node first # Assumes that you have already ssh-ed into cluster srun -N 1 -n 1 --time = 12 :00:00 -p interactive --mem = 8gb --cpus-per-task = 4 --pty bash # Run Step 1.) Load dependencies module purge module load ccbrpipeliner # Run Step 2.) Dry-run the pipeline with test dataset # And reference genome generated in the steps above # Test data consists of sub sampled FastQ files renee run \\ --input ${ RENEE_HOME } /.tests/*.R?.fastq.gz \\ --output /data/ ${ USER } /runner_hg38_36/ \\ --genome /data/ ${ USER } /hg38_36/hg38_36.json \\ --mode slurm \\ --star-2-pass-basic \\ --dry-run 4.2 Run pipeline \u00b6 Kick off the pipeline by submitting the master job to the cluster. It is essentially the same command above without the --dry-run flag. # Run Step 3.) Submit the master job # Runs the RENEE pipeline with the # reference genome generated in the steps above # and with the test dataset renee run \\ --input ${ RENEE_HOME } /.tests/*.R?.fastq.gz \\ --output /data/ ${ USER } /runner_hg38_36/ \\ --genome /data/ ${ USER } /hg38_36/hg38_36.json \\ --mode slurm \\ --star-2-pass-basic \\ --dry-run An email notification will be sent out when the pipeline starts and ends.","title":"Getting started"},{"location":"RNA-seq/TLDR-RNA-seq/#1-introduction","text":"When processing RNA-sequencing data, there are often many steps that we must repeat. These are usually steps like removing adapter sequences, aligning reads against a reference genome, checking the quality of the data, and quantifying counts. RENEE is composed of several sub commands or convenience functions to automate these repetitive steps. With RENEE, you can run your samples through our highly-reproducible pipeline, build resources for new reference genomes, and more! Here is a list of available renee sub commands : run : run the rna-seq pipeline build : build reference files cache : cache remote resources locally unlock : unlock a working directory This page contains information for building reference files and running the RENEE pipeline. For more information about each of the available sub commands, please see the usage section .","title":"1. Introduction"},{"location":"RNA-seq/TLDR-RNA-seq/#2-setup-renee","text":"Estimated Reading Time: 3 Mintutes RENEE has two dependencies: singularity and snakemake . These dependencies can be installed by a sysadmin; however, snakemake is readily available through conda. Before running the pipeline or any of the commands below, please ensure singularity and snakemake are in your $PATH . Please see follow the instructions below for getting started with the RENEE pipeline.","title":"2. Setup RENEE"},{"location":"RNA-seq/TLDR-RNA-seq/#21-login-to-cluster","text":"# Setup Step 0.) ssh into cluster's head node # example below for Biowulf cluster ssh -Y $USER @biowulf.nih.gov","title":"2.1 Login to cluster"},{"location":"RNA-seq/TLDR-RNA-seq/#22-grab-an-interactive-node","text":"# Setup Step 1.) Please do not run RENEE on the head node! # Grab an interactive node first srun -N 1 -n 1 --time = 12 :00:00 -p interactive --mem = 8gb --cpus-per-task = 4 --pty bash","title":"2.2 Grab an interactive node"},{"location":"RNA-seq/TLDR-RNA-seq/#23-load-dependencies","text":"# Setup Step 2.) Add singularity and snakemake executables to $PATH module purge module load ccbrpipeliner","title":"2.3 Load dependencies"},{"location":"RNA-seq/TLDR-RNA-seq/#3-building-reference-files","text":"In this example, we will start off by building reference files downloaded from GENCODE . We recommend downloading the PRI Genome FASTA file and annotation from GENCODE . These PRI reference files contain the primary chromosomes and scaffolds. We do not recommend downloading the CHR reference files! Checkout this list for currently available resources on Biowulf. If your required genome + annotation combination is NOT available, only then proceed to building your own reference files. Also, if you think that your genome + annotation combination may be beneficial for other Biowulf users of RENEE as well, then please request it to be added to RENEE's default resources by opening an issue on Github .","title":"3. Building Reference files"},{"location":"RNA-seq/TLDR-RNA-seq/#31-download-references-from-gencode","text":"# Build Step 0.) Please do not run RENEE on the head node! # Grab an interactive node first # Assumes that you have already ssh-ed into cluster srun -N 1 -n 1 --time = 12 :00:00 -p interactive --mem = 8gb --cpus-per-task = 4 --pty bash # Build Step 1.) Download the PRI Genome FASTA file for GRCh38.p13 wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_36/GRCh38.primary_assembly.genome.fa.gz gzip -d GRCh38.primary_assembly.genome.fa.gz # Build Step 2.) Download the PRI release 36 annotation wget ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_36/gencode.v36.primary_assembly.annotation.gtf.gz gzip -d gencode.v36.primary_assembly.annotation.gtf.gz","title":"3.1 Download References from GENCODE"},{"location":"RNA-seq/TLDR-RNA-seq/#32-run-build-pipeline","text":"# Build Step 3.) Load dependencies module purge module load ccbrpipeliner # Build Step 4.) Dry-run the build pipeline renee build --ref-fa GRCh38.primary_assembly.genome.fa \\ --ref-name hg38 \\ --ref-gtf gencode.v36.primary_assembly.annotation.gtf \\ --gtf-ver 36 --output /data/ $USER /hg38_36 --dry-run # Build Step 5.) Submit the build pipeline to cluster renee build --ref-fa GRCh38.primary_assembly.genome.fa \\ --ref-name hg38 \\ --ref-gtf gencode.v36.primary_assembly.annotation.gtf \\ --gtf-ver 36 --output /data/ $USER /hg38_36 An email notification will be sent out when the pipeline starts and ends. Once the build pipeline completes, you can run RENEE with the provided test dataset. Please see the instructions below for more information.","title":"3.2 Run Build pipeline"},{"location":"RNA-seq/TLDR-RNA-seq/#4-running-renee","text":"Run RENEE with the reference files we built above using hg38 (GRCh38.p13) Genome FASTA file and GENCODE release 36 annotation (GTF). For more information about how the reference files we generated, please see the instructions above. You can use those instructions as a guide for building any new reference genomes in the future.","title":"4. Running RENEE"},{"location":"RNA-seq/TLDR-RNA-seq/#41-dry-run-pipeline","text":"Dry-run the pipeline prior to submitting the pipeline's master job. Please note that if you wish to run RENEE with a new dataset, you will only need to update the values provided to the --input and --output arguments (and maybe --genome ). The --input argument supports globbing. If this is the first time running RENEE with for given dataset, the --output directory should not exist on your local filesystem. It will be created automatically during runtime. # Run Step 0.) Please do not run RENEE on the head node! # Grab an interactive node first # Assumes that you have already ssh-ed into cluster srun -N 1 -n 1 --time = 12 :00:00 -p interactive --mem = 8gb --cpus-per-task = 4 --pty bash # Run Step 1.) Load dependencies module purge module load ccbrpipeliner # Run Step 2.) Dry-run the pipeline with test dataset # And reference genome generated in the steps above # Test data consists of sub sampled FastQ files renee run \\ --input ${ RENEE_HOME } /.tests/*.R?.fastq.gz \\ --output /data/ ${ USER } /runner_hg38_36/ \\ --genome /data/ ${ USER } /hg38_36/hg38_36.json \\ --mode slurm \\ --star-2-pass-basic \\ --dry-run","title":"4.1 Dry-run pipeline"},{"location":"RNA-seq/TLDR-RNA-seq/#42-run-pipeline","text":"Kick off the pipeline by submitting the master job to the cluster. It is essentially the same command above without the --dry-run flag. # Run Step 3.) Submit the master job # Runs the RENEE pipeline with the # reference genome generated in the steps above # and with the test dataset renee run \\ --input ${ RENEE_HOME } /.tests/*.R?.fastq.gz \\ --output /data/ ${ USER } /runner_hg38_36/ \\ --genome /data/ ${ USER } /hg38_36/hg38_36.json \\ --mode slurm \\ --star-2-pass-basic \\ --dry-run An email notification will be sent out when the pipeline starts and ends.","title":"4.2 Run pipeline"},{"location":"RNA-seq/Theory/","text":"1. Introduction \u00b6 RNA-sequencing ( RNA-seq ) has a wide variety of applications; this transcriptome profiling method can be used to quantify gene and isoform expression, find changes in alternative splicing, detect gene-fusion events, call variants and much more. It is also worth noting that RNA-seq can be coupled with other biochemical assays to analyze many other aspects of RNA biology, such as RNA\u2013protein binding (CLIP-seq, RIP-seq), RNA structure (SHAPE-seq), or RNA\u2013RNA interactions (CLASH-seq). These applications are, however, beyond the scope of this documentation as we focus on typical RNA-seq project (i.e. quantifying expression and gene fusions). Our focus is to outline current standards and resources for the bioinformatics analysis of RNA-seq data. We do not aim to provide an exhaustive compilation of resources or software tools. Rather, we aim to provide a guideline and conceptual overview for RNA-seq data analysis based on our best-practices RNA-seq pipeline. Here we review all of the typical major steps in RNA-seq data analysis, starting from experimental design, quality control, read alignment, quantification of gene and transcript levels, and visualization. 2. Experimental Design \u00b6 Just like any other scientific experiment, a good RNA-seq experiment is hypothesis-driven. If you cannot describe the problem you are trying to address, throwing NGS at the problem is not a cure-all solution. Fishing for results is a waste of your time and is bad science. As so, designing a well-thought-out experiment around a testable question will maximize the likelihood of generating high-impact results. The data that is generated will determine whether you have the potential to answer your biological question of interest. As a prerequisite, you need to think about how you will construct your libraries; the correct sequencing depth to address your question of interest; the number of replicates, and strategies to reduce/mitigate batch effects. 2.1 Library construction \u00b6 rRNA can comprise up to 80% of the RNA in a cell. An important consideration is the RNA extraction protocol that will be used to remove the highly abundant ribosomal RNA (rRNA). For eukaryotic cells, there are two major considerations: choosing whether to enrich for mRNA or whether to deplete rRNA. 2.1.1 mRNA \u00b6 Poly-(A) selection is a common method used to enrich for mRNA. This method generates the highest percentage of reads which will ultimately map to protein-coding genes-- making it a common choice for most applications. That being said, poly(A)-selection requires your RNA to be of high quality with minimal degradation. Degraded samples that are followed with ploy(A)-selection may result in a 3\u2019 bias, which in effect, may introduce downstream biases into your results. 2.1.2 total RNA \u00b6 The second method captures total RNA through the depletion of rRNA. This method allows you to examine both mRNA and other non-coding RNA species such as lncRNAs. Again, depending on the question you are trying to answer this may be the right method for you. Although, it should be noted that both methods, mRNA and total RNA, require RINs (>8). But if you samples do contain slightly degraded RNA, you might be able to use the total RNA method over poly(A)-selection. 2.2 Sequencing Depth \u00b6 Sequencing depth or library size is another important design factor. As sequencing depth is increased, more transcripts will be detected (up until a saturation point), and their relative abundance will be quantified more accurately. At the end of the day, the targeted sequencing depth depends on the aims of the experiment. Are you trying to quantify differences in gene expression, are you trying to quantify differential isoform usage or alternative splicing events? The numbers quoted below are more or less tailored to quantify differences in gene expression. If you are trying to quantify changes in alternative splicing or isoform regulation, you are going to much higher coverage (~ 100M paired-end reads). 2.2.1 mRNA \u00b6 For mRNA libraries or libraries generated from a prep kit using poly-(A) selection, we recommend a minimum sequencing depth of 10-20M paired-end reads (or 20-40M reads). RNA must be of high quality or a 3' bias may be observed. 2.2.2 total RNA \u00b6 For total RNA libraries, we recommend a sequencing depth of 25-60M paired-end reads (or 50-120M reads). RNA must be of high quality. Note: In the sections above and below, when I say to paired-end reads I am referring to read pairs generated from paired-end sequencing of a given cDNA fragment. You will sometimes see reads reported as pairs of reads or total reads. 2.3 Replicates \u00b6 Another important design factor is the number of replicates. That being said, biological replicates are always preferred over technical replicates. 2.3.1 Recommendation \u00b6 We recommend 4 biological replicates per experimental condition or group. Having more replicates is good for several reasons because in the real world problems arise. If you have a bad sample that cannot be used due to severe QC issues, you are still left with 3 biological replicates. This allows you to drop a bad sample without comprising statistical power downstream. 2.3.2 Bare Minimum \u00b6 If cost is a factor, at a minimum, 3 biological replicates will ensure good statistical power for downstream analysis. 2.4 Reducing Batch Effects \u00b6 Batch effects represent unwanted sources of technical variation. Batch effects introduce non-biological variation into your data, which if not accounted for can influence the results. Through the process of library preparation to sequencing, there are a number of steps (such as RNA extraction to adapter ligation to lane loading, etc.) that might introduce biases into the resulting data. As a general rule of thumb, the best way to reduce the introduction of batch effects is through uniform processing-- meaning you need to ensure that differences in sample handling are minimal. This means that samples should be processed by the same lab technician and everything should be done in a uniform manner. That being said, do not extract your RNA at different times, do not use different lots of reagents! If a large number of samples are being processed and everything cannot be done at the same time, process representative samples from each biological group at the same time. This will ensure that batches and your variable of interest do not become confounded. Also, keep note of which samples belong to each batch. This information will be needed for batch correction. To reduce the possibility of introducing batch effects from sequencing, all samples should be multiplexed together on the same lane(s). Sample Group Batch Batch* Treatment_rep_1 KO 1 1 Treatment_rep_2 KO 2 1 Treatment_rep_3 KO 1 1 Treatment_rep_4 KO 2 1 Control_rep_1 WT 1 2 Control_rep_2 WT 2 2 Control_rep_3 WT 1 2 Control_rep_4 WT 2 2 Batch = properly balanced batches, easily corrected :relaxed: Batch* = groups and batch totally confounded, cannot be corrected :worried: That being said, some problems cannot be bioinformatically corrected. If your variable of interest is totally confounded with your batches, applying batch correction to fix the problem is not going to work, and will lead to undesired results (i.e. Batch* column). If batches must be introduced due to other constraining factors, please keep note which samples belong to each batch, and please put some thought into how to properly balance samples across your batches. 3. Quality Control \u00b6 Quality-control ( QC ) is extremely important! As the old adage goes: Garbage in, Garbage out! If there is one thing that to take away from this document, let it be that. Performing QC checks will help ensure that your results are reliable and reproducible. It is worth noting that there is a large variety of open-source tools that can be used to assess the quality of your data so there is no reason to re-invent the wheel. Please keep this in mind but also be aware that there are many wheels per se , and you will need to know which to use and when. In this next section, we will cover different quality-control checks that can be applied at different stages of your RNA-seq analysis. These recommendations are based on a few tools our best-practices RNA-seq pipeline employs. 3.1 Pre-aligment \u00b6 Before drawing biological conclusions, it is important to perform quality control checks to ensure that there are no signs of sequencing error, biases in your data, or other sources of contamination. Modern high-throughput sequencers generate millions of reads per run, and in the real world, problems can arise. The general idea is to assess the quality of your reads before and after adapter removal and to check for different sources of contamination before proceeding to alignment. Here are a few of the tools that we use and recommend. 3.1.1 Sequencing Quality \u00b6 To assess the sequencing quality of your data, we recommend running FastQC before and after adapter trimming. FastQC generates a set of basic statistics to identify problems that can arise during sequencing or library preparation. FastQC will summarize per base and per read QC metrics such as quality scores and GC content (ideally, this plot should have a normal distribution with no forms of bimodality). It will also summarize the distribution of sequence lengths and will report the presence of adapter sequences, which is one reason we run it after removing adapters. 3.1.2 Contamination Screening \u00b6 During the process of sample collection to library preparation, there is a risk for introducing wanted sources of DNA. FastQ Screen compares your sequencing data to a set of different reference genomes to determine if there is contamination. It allows a user to see if the composition of your library matches what you expect. If your data has high levels of human, mouse, fungi, or bacterial contamination, FastQ Screen will tell you. FastQ Screen will tell you what percentage of your library aligns against different reference genomes. If there are high levels of microbial contamination, Kraken will provide an estimation of the taxonomic composition. Kraken can be used in conjunction with Krona to produce interactive reports. Note: Due to high levels of homology between organisms, there may be a small portion of your reads that align to an unexpected reference genome. Again, this should be a minimal percentage of your reads. 3.2 Post-alignment \u00b6 Again, there are many tools available to assess the quality of your data post-alignment, and as stated before, there is no need to re-invent the wheel. Please see the table below for a generalized set of guidelines for different pre/post QC metrics. 3.2.1 Library Complexity \u00b6 Preseq can be used to estimate the complexity of a library for each of your samples. If the duplication rate is very high, the overall library complexity will be low. Low library complexity could signal an issue with library preparation or sample preparation (FFPE samples) where very little input RNA was over-amplified or the sample may be degraded. 3.2.2 Library Composition \u00b6 Picard has a particularly useful sub-command called CollectRNAseqMetrics which reports the number and percentage of reads that align to various regions: such as coding, intronic, UTR, intergenic and ribosomal regions. This is particularly useful as you would expect a library constructed with ploy(A)-selection to have a high percentage of reads that map to coding regions. Picard CollectRNAseqMetrics will also report the uniformity of coverage across all genes, which is useful for determining whether a sample has a 3' bias (observed in libraries containing degraded RNA). 3.2.3 RNA Quality \u00b6 This is another particularity useful package that is tailored for RNA-seq data. The package is made up of over 20 sub-module that can be used to do things like calculate the average insert size between paired-end reads (which is useful for GEO upload), annotate the percentage of reads spanning known or novel splice junctions, convert a BAM file into a normalized BigWig file, and infer RNA quality. 3.3 Guidelines \u00b6 Here is a set of generalized guidelines for different QC metrics. Some of these metrics will vary genome-to-genome depending on the quality of the assembly and annotation but that has been taken into consideration for our set of supported reference genomes. QC Metric Guidelines mRNA total RNA RNA Type(s) Coding Coding + non-coding RIN >= 8 [low RIN ~ 3' bias] >= 8 Single-end vs Paired-end Paired-end Paired-end Sequencing Depth 10-20M PE reads 25-60M PE reads FastQC Q30 > 70% Q30 > 70% Percent Aligned to Reference > 70% > 65% Million Reads Aligned Reference > 7M PE reads > 16.5M PE reads Percent Aligned to rRNA < 5% < 15% Picard RNAseqMetrics Coding > 50% Coding > 35% Picard RNAseqMetrics Intronic + Intergenic < 25% Intronic + Intergenic < 40% RSeQC TIN medTIN > 65 medTIN > 60 The median TIN value reported by RSeQC works reasonably well for quickly identifying problematic samples. 4. Data Processing \u00b6 Starting from raw data (FastQ files), how do we get a raw counts matrix, or how do we get a list of differential expressed genes? Before feeding your data into an R package for differential expression analysis, it needs to be processed to add biological context to it. In this section, we will talk about the data processing pipeline in more detail-- more specifically focusing on primary and secondary analysis. 4.1 Primary Analysis \u00b6 Raw data > Adapter Trimming > Alignment > Quantification 4.1.1 Adapter Trimming \u00b6 One of the first steps in this process is to remove any unwanted adapters sequences from your reads in before alignment. Adapters are composed of synthetic sequences and should be removed prior to alignment. Adapter removal is especially important in certain protocols, such as miRNA-seq. When smaller fragments are sequenced it is almost certain there will be some form of adapter contamination. 4.1.2 Alignment \u00b6 In the alignment step, we add biological context to the raw data. In this step, we align reads to the reference genome to find where the sequenced fragments originate. Accurate alignment of the cDNA fragments (which are derived from RNA) is difficult. Alternative splicing introduces the problem of aligning to non-contiguous regions, and using traditional genomic alignment algorithms can produce inaccurate or low-quality alignments due to the combination of alternative splicing and genomic variation (substitutions, insertions, and deletions). This has lead to the development of splice-aware aligners like STAR, which are designed to overcome these issues. STAR can also be run in a two-pass mode for enhanced detection of reads mapping to novel splice junctions. 4.1.3 Quantification \u00b6 In the quantification step, the number of reads that mapped to a particular genomic feature (such as a gene or isoform) is counted. It is important to keep in mind that raw counts are biased by a number of factors such as library size, feature-length, and other compositional biases. As so, it is important to normalize your data to remove these biases before summarizing differences between groups of samples.","title":"Theory"},{"location":"RNA-seq/Theory/#1-introduction","text":"RNA-sequencing ( RNA-seq ) has a wide variety of applications; this transcriptome profiling method can be used to quantify gene and isoform expression, find changes in alternative splicing, detect gene-fusion events, call variants and much more. It is also worth noting that RNA-seq can be coupled with other biochemical assays to analyze many other aspects of RNA biology, such as RNA\u2013protein binding (CLIP-seq, RIP-seq), RNA structure (SHAPE-seq), or RNA\u2013RNA interactions (CLASH-seq). These applications are, however, beyond the scope of this documentation as we focus on typical RNA-seq project (i.e. quantifying expression and gene fusions). Our focus is to outline current standards and resources for the bioinformatics analysis of RNA-seq data. We do not aim to provide an exhaustive compilation of resources or software tools. Rather, we aim to provide a guideline and conceptual overview for RNA-seq data analysis based on our best-practices RNA-seq pipeline. Here we review all of the typical major steps in RNA-seq data analysis, starting from experimental design, quality control, read alignment, quantification of gene and transcript levels, and visualization.","title":"1. Introduction"},{"location":"RNA-seq/Theory/#2-experimental-design","text":"Just like any other scientific experiment, a good RNA-seq experiment is hypothesis-driven. If you cannot describe the problem you are trying to address, throwing NGS at the problem is not a cure-all solution. Fishing for results is a waste of your time and is bad science. As so, designing a well-thought-out experiment around a testable question will maximize the likelihood of generating high-impact results. The data that is generated will determine whether you have the potential to answer your biological question of interest. As a prerequisite, you need to think about how you will construct your libraries; the correct sequencing depth to address your question of interest; the number of replicates, and strategies to reduce/mitigate batch effects.","title":"2. Experimental Design"},{"location":"RNA-seq/Theory/#21-library-construction","text":"rRNA can comprise up to 80% of the RNA in a cell. An important consideration is the RNA extraction protocol that will be used to remove the highly abundant ribosomal RNA (rRNA). For eukaryotic cells, there are two major considerations: choosing whether to enrich for mRNA or whether to deplete rRNA.","title":"2.1 Library construction"},{"location":"RNA-seq/Theory/#211-mrna","text":"Poly-(A) selection is a common method used to enrich for mRNA. This method generates the highest percentage of reads which will ultimately map to protein-coding genes-- making it a common choice for most applications. That being said, poly(A)-selection requires your RNA to be of high quality with minimal degradation. Degraded samples that are followed with ploy(A)-selection may result in a 3\u2019 bias, which in effect, may introduce downstream biases into your results.","title":"2.1.1 mRNA"},{"location":"RNA-seq/Theory/#212-total-rna","text":"The second method captures total RNA through the depletion of rRNA. This method allows you to examine both mRNA and other non-coding RNA species such as lncRNAs. Again, depending on the question you are trying to answer this may be the right method for you. Although, it should be noted that both methods, mRNA and total RNA, require RINs (>8). But if you samples do contain slightly degraded RNA, you might be able to use the total RNA method over poly(A)-selection.","title":"2.1.2 total RNA"},{"location":"RNA-seq/Theory/#22-sequencing-depth","text":"Sequencing depth or library size is another important design factor. As sequencing depth is increased, more transcripts will be detected (up until a saturation point), and their relative abundance will be quantified more accurately. At the end of the day, the targeted sequencing depth depends on the aims of the experiment. Are you trying to quantify differences in gene expression, are you trying to quantify differential isoform usage or alternative splicing events? The numbers quoted below are more or less tailored to quantify differences in gene expression. If you are trying to quantify changes in alternative splicing or isoform regulation, you are going to much higher coverage (~ 100M paired-end reads).","title":"2.2 Sequencing Depth"},{"location":"RNA-seq/Theory/#221-mrna","text":"For mRNA libraries or libraries generated from a prep kit using poly-(A) selection, we recommend a minimum sequencing depth of 10-20M paired-end reads (or 20-40M reads). RNA must be of high quality or a 3' bias may be observed.","title":"2.2.1 mRNA"},{"location":"RNA-seq/Theory/#222-total-rna","text":"For total RNA libraries, we recommend a sequencing depth of 25-60M paired-end reads (or 50-120M reads). RNA must be of high quality. Note: In the sections above and below, when I say to paired-end reads I am referring to read pairs generated from paired-end sequencing of a given cDNA fragment. You will sometimes see reads reported as pairs of reads or total reads.","title":"2.2.2 total RNA"},{"location":"RNA-seq/Theory/#23-replicates","text":"Another important design factor is the number of replicates. That being said, biological replicates are always preferred over technical replicates.","title":"2.3 Replicates"},{"location":"RNA-seq/Theory/#231-recommendation","text":"We recommend 4 biological replicates per experimental condition or group. Having more replicates is good for several reasons because in the real world problems arise. If you have a bad sample that cannot be used due to severe QC issues, you are still left with 3 biological replicates. This allows you to drop a bad sample without comprising statistical power downstream.","title":"2.3.1 Recommendation"},{"location":"RNA-seq/Theory/#232-bare-minimum","text":"If cost is a factor, at a minimum, 3 biological replicates will ensure good statistical power for downstream analysis.","title":"2.3.2 Bare Minimum"},{"location":"RNA-seq/Theory/#24-reducing-batch-effects","text":"Batch effects represent unwanted sources of technical variation. Batch effects introduce non-biological variation into your data, which if not accounted for can influence the results. Through the process of library preparation to sequencing, there are a number of steps (such as RNA extraction to adapter ligation to lane loading, etc.) that might introduce biases into the resulting data. As a general rule of thumb, the best way to reduce the introduction of batch effects is through uniform processing-- meaning you need to ensure that differences in sample handling are minimal. This means that samples should be processed by the same lab technician and everything should be done in a uniform manner. That being said, do not extract your RNA at different times, do not use different lots of reagents! If a large number of samples are being processed and everything cannot be done at the same time, process representative samples from each biological group at the same time. This will ensure that batches and your variable of interest do not become confounded. Also, keep note of which samples belong to each batch. This information will be needed for batch correction. To reduce the possibility of introducing batch effects from sequencing, all samples should be multiplexed together on the same lane(s). Sample Group Batch Batch* Treatment_rep_1 KO 1 1 Treatment_rep_2 KO 2 1 Treatment_rep_3 KO 1 1 Treatment_rep_4 KO 2 1 Control_rep_1 WT 1 2 Control_rep_2 WT 2 2 Control_rep_3 WT 1 2 Control_rep_4 WT 2 2 Batch = properly balanced batches, easily corrected :relaxed: Batch* = groups and batch totally confounded, cannot be corrected :worried: That being said, some problems cannot be bioinformatically corrected. If your variable of interest is totally confounded with your batches, applying batch correction to fix the problem is not going to work, and will lead to undesired results (i.e. Batch* column). If batches must be introduced due to other constraining factors, please keep note which samples belong to each batch, and please put some thought into how to properly balance samples across your batches.","title":"2.4 Reducing Batch Effects"},{"location":"RNA-seq/Theory/#3-quality-control","text":"Quality-control ( QC ) is extremely important! As the old adage goes: Garbage in, Garbage out! If there is one thing that to take away from this document, let it be that. Performing QC checks will help ensure that your results are reliable and reproducible. It is worth noting that there is a large variety of open-source tools that can be used to assess the quality of your data so there is no reason to re-invent the wheel. Please keep this in mind but also be aware that there are many wheels per se , and you will need to know which to use and when. In this next section, we will cover different quality-control checks that can be applied at different stages of your RNA-seq analysis. These recommendations are based on a few tools our best-practices RNA-seq pipeline employs.","title":"3. Quality Control"},{"location":"RNA-seq/Theory/#31-pre-aligment","text":"Before drawing biological conclusions, it is important to perform quality control checks to ensure that there are no signs of sequencing error, biases in your data, or other sources of contamination. Modern high-throughput sequencers generate millions of reads per run, and in the real world, problems can arise. The general idea is to assess the quality of your reads before and after adapter removal and to check for different sources of contamination before proceeding to alignment. Here are a few of the tools that we use and recommend.","title":"3.1 Pre-aligment"},{"location":"RNA-seq/Theory/#311-sequencing-quality","text":"To assess the sequencing quality of your data, we recommend running FastQC before and after adapter trimming. FastQC generates a set of basic statistics to identify problems that can arise during sequencing or library preparation. FastQC will summarize per base and per read QC metrics such as quality scores and GC content (ideally, this plot should have a normal distribution with no forms of bimodality). It will also summarize the distribution of sequence lengths and will report the presence of adapter sequences, which is one reason we run it after removing adapters.","title":"3.1.1 Sequencing Quality"},{"location":"RNA-seq/Theory/#312-contamination-screening","text":"During the process of sample collection to library preparation, there is a risk for introducing wanted sources of DNA. FastQ Screen compares your sequencing data to a set of different reference genomes to determine if there is contamination. It allows a user to see if the composition of your library matches what you expect. If your data has high levels of human, mouse, fungi, or bacterial contamination, FastQ Screen will tell you. FastQ Screen will tell you what percentage of your library aligns against different reference genomes. If there are high levels of microbial contamination, Kraken will provide an estimation of the taxonomic composition. Kraken can be used in conjunction with Krona to produce interactive reports. Note: Due to high levels of homology between organisms, there may be a small portion of your reads that align to an unexpected reference genome. Again, this should be a minimal percentage of your reads.","title":"3.1.2 Contamination Screening"},{"location":"RNA-seq/Theory/#32-post-alignment","text":"Again, there are many tools available to assess the quality of your data post-alignment, and as stated before, there is no need to re-invent the wheel. Please see the table below for a generalized set of guidelines for different pre/post QC metrics.","title":"3.2 Post-alignment"},{"location":"RNA-seq/Theory/#321-library-complexity","text":"Preseq can be used to estimate the complexity of a library for each of your samples. If the duplication rate is very high, the overall library complexity will be low. Low library complexity could signal an issue with library preparation or sample preparation (FFPE samples) where very little input RNA was over-amplified or the sample may be degraded.","title":"3.2.1 Library Complexity"},{"location":"RNA-seq/Theory/#322-library-composition","text":"Picard has a particularly useful sub-command called CollectRNAseqMetrics which reports the number and percentage of reads that align to various regions: such as coding, intronic, UTR, intergenic and ribosomal regions. This is particularly useful as you would expect a library constructed with ploy(A)-selection to have a high percentage of reads that map to coding regions. Picard CollectRNAseqMetrics will also report the uniformity of coverage across all genes, which is useful for determining whether a sample has a 3' bias (observed in libraries containing degraded RNA).","title":"3.2.2 Library Composition"},{"location":"RNA-seq/Theory/#323-rna-quality","text":"This is another particularity useful package that is tailored for RNA-seq data. The package is made up of over 20 sub-module that can be used to do things like calculate the average insert size between paired-end reads (which is useful for GEO upload), annotate the percentage of reads spanning known or novel splice junctions, convert a BAM file into a normalized BigWig file, and infer RNA quality.","title":"3.2.3 RNA Quality"},{"location":"RNA-seq/Theory/#33-guidelines","text":"Here is a set of generalized guidelines for different QC metrics. Some of these metrics will vary genome-to-genome depending on the quality of the assembly and annotation but that has been taken into consideration for our set of supported reference genomes. QC Metric Guidelines mRNA total RNA RNA Type(s) Coding Coding + non-coding RIN >= 8 [low RIN ~ 3' bias] >= 8 Single-end vs Paired-end Paired-end Paired-end Sequencing Depth 10-20M PE reads 25-60M PE reads FastQC Q30 > 70% Q30 > 70% Percent Aligned to Reference > 70% > 65% Million Reads Aligned Reference > 7M PE reads > 16.5M PE reads Percent Aligned to rRNA < 5% < 15% Picard RNAseqMetrics Coding > 50% Coding > 35% Picard RNAseqMetrics Intronic + Intergenic < 25% Intronic + Intergenic < 40% RSeQC TIN medTIN > 65 medTIN > 60 The median TIN value reported by RSeQC works reasonably well for quickly identifying problematic samples.","title":"3.3 Guidelines"},{"location":"RNA-seq/Theory/#4-data-processing","text":"Starting from raw data (FastQ files), how do we get a raw counts matrix, or how do we get a list of differential expressed genes? Before feeding your data into an R package for differential expression analysis, it needs to be processed to add biological context to it. In this section, we will talk about the data processing pipeline in more detail-- more specifically focusing on primary and secondary analysis.","title":"4. Data Processing"},{"location":"RNA-seq/Theory/#41-primary-analysis","text":"Raw data > Adapter Trimming > Alignment > Quantification","title":"4.1 Primary Analysis"},{"location":"RNA-seq/Theory/#411-adapter-trimming","text":"One of the first steps in this process is to remove any unwanted adapters sequences from your reads in before alignment. Adapters are composed of synthetic sequences and should be removed prior to alignment. Adapter removal is especially important in certain protocols, such as miRNA-seq. When smaller fragments are sequenced it is almost certain there will be some form of adapter contamination.","title":"4.1.1 Adapter Trimming"},{"location":"RNA-seq/Theory/#412-alignment","text":"In the alignment step, we add biological context to the raw data. In this step, we align reads to the reference genome to find where the sequenced fragments originate. Accurate alignment of the cDNA fragments (which are derived from RNA) is difficult. Alternative splicing introduces the problem of aligning to non-contiguous regions, and using traditional genomic alignment algorithms can produce inaccurate or low-quality alignments due to the combination of alternative splicing and genomic variation (substitutions, insertions, and deletions). This has lead to the development of splice-aware aligners like STAR, which are designed to overcome these issues. STAR can also be run in a two-pass mode for enhanced detection of reads mapping to novel splice junctions.","title":"4.1.2 Alignment"},{"location":"RNA-seq/Theory/#413-quantification","text":"In the quantification step, the number of reads that mapped to a particular genomic feature (such as a gene or isoform) is counted. It is important to keep in mind that raw counts are biased by a number of factors such as library size, feature-length, and other compositional biases. As so, it is important to normalize your data to remove these biases before summarizing differences between groups of samples.","title":"4.1.3 Quantification"},{"location":"RNA-seq/build/","text":"renee build \u00b6 1. About \u00b6 The renee executable is composed of several inter-related sub commands. Please see renee -h for all available options. This part of the documentation describes options and concepts for renee build sub command in more detail. With minimal configuration, the build sub command enables you to build new reference files for the renee run pipeline. Setting up the RENEE build pipeline is fast and easy! In its most basic form, renee build only has five required inputs . 2. Synopsis \u00b6 $ renee build [--help] \\ [--shared-resources SHARED_RESOURCES] [--small-genome] \\ [--dry-run] [--singularity-cache SINGULARITY_CACHE] \\ [--sif-cache SIF_CACHE] [--tmp-dir TMP_DIR] \\ --ref-fa REF_FA \\ --ref-name REF_NAME \\ --ref-gtf REF_GTF \\ --gtf-ver GTF_VER \\ --output OUTPUT The synopsis for each command shows its parameters and their usage. Optional parameters are shown in square brackets. A user must provide the genomic sequence of the reference's assembly in FASTA format via --ref-fa argument, an alias for the reference genome via --ref-name argument, a gene annotation for the reference assembly via --ref-gtf argument, an alias or version for the gene annotation via the --gtf-ver argument, and an output directory to store the built reference files via --output argument. If you are running the pipeline outside of Biowulf, you will need to additionally provide the the following options: --shared-resources , --tmp-dir . More information about each of these options can be found below. For human and mouse data, we highly recommend downloading the latest available PRI genome assembly and corresponding gene annotation from GENCODE . These reference files contain chromosomes and scaffolds sequences. The build pipeline will generate a JSON file containing key, value pairs to required reference files for the renee run pipeline. This file will be located in the path provided to --output . The name of this JSON file is dependent on the values provided to --ref-name and --gtf-ver and has the following naming convention: {OUTPUT}/{REF_NAME}_{GTF_VER}.json . Once the build pipeline completes, this reference JSON file can be passed to the --genome option of renee run . This is how new references are built for the RENEE pipeline. Use you can always use the -h option for information on a specific command. 2.1 Required Arguments \u00b6 Each of the following arguments are required. Failure to provide a required argument will result in a non-zero exit-code. --ref-fa REF_FA Genomic FASTA file of the reference genome. type: file This file represents the genome sequence of the reference assembly in FASTA format. If you are downloading this from GENCODE, you should select the PRI genomic FASTA file. This file will contain the primary genomic assembly (contains chromosomes and scaffolds). This input file should not be compressed. Sequence identifiers in this file must match with sequence identifiers in the GTF file provided to --ref-gtf . Example: > --ref-fa GRCh38.primary_assembly.genome.fa --ref-name REF_NAME Name of the reference genome. type: string Name or alias for the reference genome. This can be the common name for the reference genome. Here is a list of common examples for different model organisms: mm10, hg38, rn6, danRer11, dm6, canFam3, sacCer3, ce11. If the provided values contains one of the following sub-strings (hg19, hs37d, grch37, hg38, hs38d, grch38, mm10, grcm38), then Arriba will run with its corresponding blacklist. Example: --ref-name hg38 --ref-gtf REF_GTF Gene annotation or GTF file for the reference genome. type: file This file represents the reference genome's gene annotation in GTF format. If you are downloading this from GENCODE, you should select the 'PRI' GTF file. This file contains gene annotations for the primary assembly (contains chromosomes and scaffolds). This input file should not be compressed. Sequence identifiers (column 1) in this file must match with sequence identifiers in the FASTA file provided to --ref-fa . Example: --ref-gtf gencode.v36.primary_assembly.annotation.gtf --gtf-ver GTF_VER Version of the gene annotation or GTF file provided. type: string or int This is the version of the supplied gene annotation or GTF file. If you are using a GTF file from GENCODE, use the release number or version (i.e. M25 for mouse or 37 for human). Visit gencodegenes.org for more details. Example: --gtf-ver 36 --output OUTPUT Path to an output directory. type: path This location is where the build pipeline will create all of its output files. If the user-provided working directory has not been initialized, it will automatically be created. Note: by default, any files in config , resources, or workflow in the output directory may be overwritten by renee build . Example: --output /data/$USER/refs/hg38_36/ 2.2 Build Options \u00b6 Each of the following arguments are optional and do not need to be provided. If you are running the pipeline outside of Biowulf, the --shared-resources option only needs to be provided at least once. This will ensure reference files that are shared across different genomes are downloaded locally. --shared-resources SHARED_RESOURCES Local path to shared resources. type: path The pipeline uses a set of shared reference files that can be re-used across reference genomes. These currently include reference files for kraken and FQScreen. These reference files can be downloaded with the build sub command's --shared-resources option. With that being said, these files only need to be downloaded once. We recommend storing this files in a shared location on the filesystem that other people can access. If you are running the pipeline on Biowulf, you do NOT need to download these reference files! They already exist on the filesystem in a location that anyone can access; however, if you are running the pipeline on another cluster or target system, you will need to download the shared resources with the build sub command, and you will need to provide this option every time you run the pipeline. Please provide the same path that was provided to the build sub command's --shared-resources option. Again, if you are running the pipeline on Biowulf, you do NOT need to provide this option. For more information about how to download shared resources, please reference the build sub command's --shared-resources option. Example: --shared-resources /data/shared/renee --small-genome Builds a small genome index. type: boolean For small genomes, it is recommended running STAR with a scaled down --genomeSAindexNbases value. This option runs the build pipeline in a mode where it dynamically finds the optimal value for this option using the following formula: min(14, log2(GenomeSize)/2 - 1) . Generally speaking, this option is not really applicable for most mammalian reference genomes, i.e. human and mouse; however, researcher working with very small reference genomes, like S. cerevisiae ~ 12Mb, should provide this option. When in doubt feel free to provide this option, as the optimal value will be found based on your input. Example: --small-genome 2.3 Orchestration Options \u00b6 --dry-run Dry run the build pipeline. type: boolean Displays what steps in the build pipeline remain or will be run. Does not execute anything! Example: --dry-run --singularity-cache SINGULARITY_CACHE Overrides the $SINGULARITY_CACHEDIR environment variable. type: path default: --output OUTPUT/.singularity Singularity will cache image layers pulled from remote registries. This ultimately speeds up the process of pull an image from DockerHub if an image layer already exists in the singularity cache directory. By default, the cache is set to the value provided to the --output argument. Please note that this cache cannot be shared across users. Singularity strictly enforces you own the cache directory and will return a non-zero exit code if you do not own the cache directory! See the --sif-cache option to create a shareable resource. Example: --singularity-cache /data/$USER/.singularity --sif-cache SIF_CACHE Path where a local cache of SIFs are stored. type: path Uses a local cache of SIFs on the filesystem. This SIF cache can be shared across users if permissions are set correctly. If a SIF does not exist in the SIF cache, the image will be pulled from Dockerhub and a warning message will be displayed. The renee cache subcommand can be used to create a local SIF cache. Please see renee cache for more information. This command is extremely useful for avoiding DockerHub pull rate limits. It also remove any potential errors that could occur due to network issues or DockerHub being temporarily unavailable. We recommend running RENEE with this option when ever possible. Example: --singularity-cache /data/$USER/SIFs --tmp-dir TMP_DIR Path on the file system for writing temporary files. type: path default: /lscratch/$SLURM_JOBID Path on the file system for writing temporary output files. By default, the temporary directory is set to '/lscratch/$SLURM_JOBID' on NIH's Biowulf cluster and 'OUTPUT' on the FRCE cluster. However, if you are running the pipeline on another cluster, this option will need to be specified. Ideally, this path should point to a dedicated location on the filesystem for writing tmp files. On many systems, this location is set to somewhere in /scratch. If you need to inject avariable into this string that should NOT be expanded,please quote this options value in single quotes. Example: --tmp-dir /cluster_scratch/$USER/ 2.4 Misc Options \u00b6 Each of the following arguments are optional and do not need to be provided. -h, --help Display Help. type: boolean Shows command's synopsis, help message, and an example command Example: --help 3. Hybrid Genomes \u00b6 If you have two GTF files, e.g. hybrid genomes (host + virus), then you need to create one genomic FASTA file and one GTF file for the hybrid genome prior to running the renee build command. We recommend creating an artificial chromosome for the non-host sequence. The sequence identifier in the FASTA file must match the sequence identifier in the GTF file (column 1). Generally speaking, since the host annotation is usually downloaded from Ensembl or GENCODE, it will be correctly formatted; however, that may not be the case for the non-host sequence! Please ensure the non-host annotation contains the following features and/or constraints: for a given gene feature each gene entry has at least one transcript feature and each transcript entry has at least one exon feature gene_id , gene_name and gene_biotype are required for a given transcipt feature along with gene_id , gene_name and gene_biotype ... transcript_id is also required for a given exon feature gene_id , gene_name , gene_biotype , transcript_id are required If not, the GTF file may need to be manually curated until these conditions are satisfied. Here is an example feature from a hand-curated Biotyn_probe GTF file: Biot1 BiotynProbe gene 1 21 0 .000000 + . gene_id \"Biot1\" ; gene_name \"Biot1\" ; gene_biotype \"biotynlated_probe_control\" ; Biot1 BiotynProbe transcript 1 21 0 .000000 + . gene_id \"Biot1\" ; gene_name \"Biot1\" ; gene_biotype \"biotynlated_probe_control\" ; transcript_id \"Biot1\" ; transcript_name \"Biot1\" ; transcript_type \"biotynlated_probe_control\" ; Biot1 BiotynProbe exon 1 21 0 .000000 + . gene_id \"Biot1\" ; gene_biotype \"biotynlated_probe_control\" ; transcript_id \"Biot1\" ; transcript_type \"biotynlated_probe_control\" ; In this tab-delimited example above, line 1: the gene feature has 3 required attributes in column 9: gene_id and gene_name and gene_biotype line 2: the transcript entry for the above gene repeats the same attributes with following required fields: transcript_id and transcript_name Please note: transcript_type is optional line 3: the exon entry for the above transcript has 3 required attributes: gene_id and transcript_id and gene_biotype Please note: transcript_type is optional For a given gene, the combination of the gene_id AND gene_name should form a unique string. There should be no instances where two different genes share the same gene_id AND gene_name . 4. Convert NCBI GFF3 to GTF format \u00b6 It is worth noting that RENEE comes bundled with a script to convert GFF3 files downloaded from NCBI to GTF file format. This convenience script is useful as the renee build sub command takes a GTF file as one of its inputs. Please note that this script has only been tested with GFF3 files downloaded from NCBI, and it is not recommended to use with GFF3 files originating from other sources like Ensembl or GENCODE . If you are selecting an annotation from Ensembl or GENCODE, please download the GTF file option. The only dependency of the script is the python package argparse, which comes bundled with the following python\u2154 distributions: python>=2.7.18 or python>=3.2 . If argparse is not installed, it can be downloaded with pip by running the following command: pip install --upgrade pip pip install argparse For more information about the script and its usage, please run: ./resources/gff3togtf.py -h 5. Example \u00b6 5.1 Biowulf \u00b6 On Biowulf getting started with the pipeline is fast and easy! In this example, we build a mouse reference genome. # Step 0.) Grab an interactive node (do not run on head node) srun -N 1 -n 1 --time = 2 :00:00 -p interactive --mem = 8gb --cpus-per-task = 4 --pty bash module purge module load ccbrpipeliner # Step 1.) Dry run the Build pipeline renee build --ref-fa GRCm39.primary_assembly.genome.fa \\ --ref-name mm39 \\ --ref-gtf gencode.vM26.annotation.gtf \\ --gtf-ver M26 \\ --output /data/ $USER /refs/mm39_M26 \\ --sif-cache /data/CCBR_Pipeliner/SIFs/ \\ --dry-run # Step 2.) Build new RENEE reference files renee build --ref-fa GRCm39.primary_assembly.genome.fa \\ --ref-name mm39 \\ --ref-gtf gencode.vM26.annotation.gtf \\ --gtf-ver M26 \\ --output /data/ $USER /refs/mm39_M26 \\ --sif-cache /data/CCBR_Pipeliner/SIFs/ 5.2 Generic SLURM Cluster \u00b6 Running the pipeline outside of Biowulf is easy; however, there are a few extra options you must provide. Please note when running the build sub command for the first time, you will also need to provide the --shared-resources option. This option will download our kraken2 database and bowtie2 indices for FastQ Screen. The path provided to this option should be provided to the --shared-resources option of the run sub command. Next, you will also need to provide a path to write temporary output files via the --tmp-dir option. We also recommend providing a path to a SIF cache. You can cache software containers locally with the cache sub command. # Step 0.) Grab an interactive node (do not run on head node) srun -N 1 -n 1 --time = 2 :00:00 -p interactive --mem = 8gb --cpus-per-task = 4 --pty bash # Add snakemake and singularity to $PATH, # This step may vary across clusters, you # can reach out to a sys admin if snakemake # and singularity are not installed. module purge # Replace the following: # module load ccbrpipeliner # with module load statements that load # python >= 3.7, # snakemake, and # singularity # before running renee # Also, ensure that the `renee` executable is in PATH # Step 1.) Dry run the Build pipeline renee build --ref-fa GRCm39.primary_assembly.genome.fa \\ --ref-name mm39 \\ --ref-gtf gencode.vM26.annotation.gtf \\ --gtf-ver M26 \\ --output /data/ $USER /refs/mm39_M26 \\ --shared-resources /data/shared/renee \\ --tmp-dir /cluster_scratch/ $USER / \\ --sif-cache /data/ $USER /cache \\ --dry-run # Step 2.) Build new RENEE reference files renee build --ref-fa GRCm39.primary_assembly.genome.fa \\ --ref-name mm39 \\ --ref-gtf gencode.vM26.annotation.gtf \\ --gtf-ver M26 \\ --output /data/ $USER /refs/mm39_M26 \\ --shared-resources /data/shared/renee \\ --tmp-dir /cluster_scratch/ $USER / \\ --sif-cache /data/ $USER /cache","title":"build"},{"location":"RNA-seq/build/#renee-build","text":"","title":"renee build"},{"location":"RNA-seq/build/#1-about","text":"The renee executable is composed of several inter-related sub commands. Please see renee -h for all available options. This part of the documentation describes options and concepts for renee build sub command in more detail. With minimal configuration, the build sub command enables you to build new reference files for the renee run pipeline. Setting up the RENEE build pipeline is fast and easy! In its most basic form, renee build only has five required inputs .","title":"1. About"},{"location":"RNA-seq/build/#2-synopsis","text":"$ renee build [--help] \\ [--shared-resources SHARED_RESOURCES] [--small-genome] \\ [--dry-run] [--singularity-cache SINGULARITY_CACHE] \\ [--sif-cache SIF_CACHE] [--tmp-dir TMP_DIR] \\ --ref-fa REF_FA \\ --ref-name REF_NAME \\ --ref-gtf REF_GTF \\ --gtf-ver GTF_VER \\ --output OUTPUT The synopsis for each command shows its parameters and their usage. Optional parameters are shown in square brackets. A user must provide the genomic sequence of the reference's assembly in FASTA format via --ref-fa argument, an alias for the reference genome via --ref-name argument, a gene annotation for the reference assembly via --ref-gtf argument, an alias or version for the gene annotation via the --gtf-ver argument, and an output directory to store the built reference files via --output argument. If you are running the pipeline outside of Biowulf, you will need to additionally provide the the following options: --shared-resources , --tmp-dir . More information about each of these options can be found below. For human and mouse data, we highly recommend downloading the latest available PRI genome assembly and corresponding gene annotation from GENCODE . These reference files contain chromosomes and scaffolds sequences. The build pipeline will generate a JSON file containing key, value pairs to required reference files for the renee run pipeline. This file will be located in the path provided to --output . The name of this JSON file is dependent on the values provided to --ref-name and --gtf-ver and has the following naming convention: {OUTPUT}/{REF_NAME}_{GTF_VER}.json . Once the build pipeline completes, this reference JSON file can be passed to the --genome option of renee run . This is how new references are built for the RENEE pipeline. Use you can always use the -h option for information on a specific command.","title":"2. Synopsis"},{"location":"RNA-seq/build/#21-required-arguments","text":"Each of the following arguments are required. Failure to provide a required argument will result in a non-zero exit-code. --ref-fa REF_FA Genomic FASTA file of the reference genome. type: file This file represents the genome sequence of the reference assembly in FASTA format. If you are downloading this from GENCODE, you should select the PRI genomic FASTA file. This file will contain the primary genomic assembly (contains chromosomes and scaffolds). This input file should not be compressed. Sequence identifiers in this file must match with sequence identifiers in the GTF file provided to --ref-gtf . Example: > --ref-fa GRCh38.primary_assembly.genome.fa --ref-name REF_NAME Name of the reference genome. type: string Name or alias for the reference genome. This can be the common name for the reference genome. Here is a list of common examples for different model organisms: mm10, hg38, rn6, danRer11, dm6, canFam3, sacCer3, ce11. If the provided values contains one of the following sub-strings (hg19, hs37d, grch37, hg38, hs38d, grch38, mm10, grcm38), then Arriba will run with its corresponding blacklist. Example: --ref-name hg38 --ref-gtf REF_GTF Gene annotation or GTF file for the reference genome. type: file This file represents the reference genome's gene annotation in GTF format. If you are downloading this from GENCODE, you should select the 'PRI' GTF file. This file contains gene annotations for the primary assembly (contains chromosomes and scaffolds). This input file should not be compressed. Sequence identifiers (column 1) in this file must match with sequence identifiers in the FASTA file provided to --ref-fa . Example: --ref-gtf gencode.v36.primary_assembly.annotation.gtf --gtf-ver GTF_VER Version of the gene annotation or GTF file provided. type: string or int This is the version of the supplied gene annotation or GTF file. If you are using a GTF file from GENCODE, use the release number or version (i.e. M25 for mouse or 37 for human). Visit gencodegenes.org for more details. Example: --gtf-ver 36 --output OUTPUT Path to an output directory. type: path This location is where the build pipeline will create all of its output files. If the user-provided working directory has not been initialized, it will automatically be created. Note: by default, any files in config , resources, or workflow in the output directory may be overwritten by renee build . Example: --output /data/$USER/refs/hg38_36/","title":"2.1 Required Arguments"},{"location":"RNA-seq/build/#22-build-options","text":"Each of the following arguments are optional and do not need to be provided. If you are running the pipeline outside of Biowulf, the --shared-resources option only needs to be provided at least once. This will ensure reference files that are shared across different genomes are downloaded locally. --shared-resources SHARED_RESOURCES Local path to shared resources. type: path The pipeline uses a set of shared reference files that can be re-used across reference genomes. These currently include reference files for kraken and FQScreen. These reference files can be downloaded with the build sub command's --shared-resources option. With that being said, these files only need to be downloaded once. We recommend storing this files in a shared location on the filesystem that other people can access. If you are running the pipeline on Biowulf, you do NOT need to download these reference files! They already exist on the filesystem in a location that anyone can access; however, if you are running the pipeline on another cluster or target system, you will need to download the shared resources with the build sub command, and you will need to provide this option every time you run the pipeline. Please provide the same path that was provided to the build sub command's --shared-resources option. Again, if you are running the pipeline on Biowulf, you do NOT need to provide this option. For more information about how to download shared resources, please reference the build sub command's --shared-resources option. Example: --shared-resources /data/shared/renee --small-genome Builds a small genome index. type: boolean For small genomes, it is recommended running STAR with a scaled down --genomeSAindexNbases value. This option runs the build pipeline in a mode where it dynamically finds the optimal value for this option using the following formula: min(14, log2(GenomeSize)/2 - 1) . Generally speaking, this option is not really applicable for most mammalian reference genomes, i.e. human and mouse; however, researcher working with very small reference genomes, like S. cerevisiae ~ 12Mb, should provide this option. When in doubt feel free to provide this option, as the optimal value will be found based on your input. Example: --small-genome","title":"2.2 Build Options"},{"location":"RNA-seq/build/#23-orchestration-options","text":"--dry-run Dry run the build pipeline. type: boolean Displays what steps in the build pipeline remain or will be run. Does not execute anything! Example: --dry-run --singularity-cache SINGULARITY_CACHE Overrides the $SINGULARITY_CACHEDIR environment variable. type: path default: --output OUTPUT/.singularity Singularity will cache image layers pulled from remote registries. This ultimately speeds up the process of pull an image from DockerHub if an image layer already exists in the singularity cache directory. By default, the cache is set to the value provided to the --output argument. Please note that this cache cannot be shared across users. Singularity strictly enforces you own the cache directory and will return a non-zero exit code if you do not own the cache directory! See the --sif-cache option to create a shareable resource. Example: --singularity-cache /data/$USER/.singularity --sif-cache SIF_CACHE Path where a local cache of SIFs are stored. type: path Uses a local cache of SIFs on the filesystem. This SIF cache can be shared across users if permissions are set correctly. If a SIF does not exist in the SIF cache, the image will be pulled from Dockerhub and a warning message will be displayed. The renee cache subcommand can be used to create a local SIF cache. Please see renee cache for more information. This command is extremely useful for avoiding DockerHub pull rate limits. It also remove any potential errors that could occur due to network issues or DockerHub being temporarily unavailable. We recommend running RENEE with this option when ever possible. Example: --singularity-cache /data/$USER/SIFs --tmp-dir TMP_DIR Path on the file system for writing temporary files. type: path default: /lscratch/$SLURM_JOBID Path on the file system for writing temporary output files. By default, the temporary directory is set to '/lscratch/$SLURM_JOBID' on NIH's Biowulf cluster and 'OUTPUT' on the FRCE cluster. However, if you are running the pipeline on another cluster, this option will need to be specified. Ideally, this path should point to a dedicated location on the filesystem for writing tmp files. On many systems, this location is set to somewhere in /scratch. If you need to inject avariable into this string that should NOT be expanded,please quote this options value in single quotes. Example: --tmp-dir /cluster_scratch/$USER/","title":"2.3 Orchestration Options"},{"location":"RNA-seq/build/#24-misc-options","text":"Each of the following arguments are optional and do not need to be provided. -h, --help Display Help. type: boolean Shows command's synopsis, help message, and an example command Example: --help","title":"2.4 Misc Options"},{"location":"RNA-seq/build/#3-hybrid-genomes","text":"If you have two GTF files, e.g. hybrid genomes (host + virus), then you need to create one genomic FASTA file and one GTF file for the hybrid genome prior to running the renee build command. We recommend creating an artificial chromosome for the non-host sequence. The sequence identifier in the FASTA file must match the sequence identifier in the GTF file (column 1). Generally speaking, since the host annotation is usually downloaded from Ensembl or GENCODE, it will be correctly formatted; however, that may not be the case for the non-host sequence! Please ensure the non-host annotation contains the following features and/or constraints: for a given gene feature each gene entry has at least one transcript feature and each transcript entry has at least one exon feature gene_id , gene_name and gene_biotype are required for a given transcipt feature along with gene_id , gene_name and gene_biotype ... transcript_id is also required for a given exon feature gene_id , gene_name , gene_biotype , transcript_id are required If not, the GTF file may need to be manually curated until these conditions are satisfied. Here is an example feature from a hand-curated Biotyn_probe GTF file: Biot1 BiotynProbe gene 1 21 0 .000000 + . gene_id \"Biot1\" ; gene_name \"Biot1\" ; gene_biotype \"biotynlated_probe_control\" ; Biot1 BiotynProbe transcript 1 21 0 .000000 + . gene_id \"Biot1\" ; gene_name \"Biot1\" ; gene_biotype \"biotynlated_probe_control\" ; transcript_id \"Biot1\" ; transcript_name \"Biot1\" ; transcript_type \"biotynlated_probe_control\" ; Biot1 BiotynProbe exon 1 21 0 .000000 + . gene_id \"Biot1\" ; gene_biotype \"biotynlated_probe_control\" ; transcript_id \"Biot1\" ; transcript_type \"biotynlated_probe_control\" ; In this tab-delimited example above, line 1: the gene feature has 3 required attributes in column 9: gene_id and gene_name and gene_biotype line 2: the transcript entry for the above gene repeats the same attributes with following required fields: transcript_id and transcript_name Please note: transcript_type is optional line 3: the exon entry for the above transcript has 3 required attributes: gene_id and transcript_id and gene_biotype Please note: transcript_type is optional For a given gene, the combination of the gene_id AND gene_name should form a unique string. There should be no instances where two different genes share the same gene_id AND gene_name .","title":"3. Hybrid Genomes"},{"location":"RNA-seq/build/#4-convert-ncbi-gff3-to-gtf-format","text":"It is worth noting that RENEE comes bundled with a script to convert GFF3 files downloaded from NCBI to GTF file format. This convenience script is useful as the renee build sub command takes a GTF file as one of its inputs. Please note that this script has only been tested with GFF3 files downloaded from NCBI, and it is not recommended to use with GFF3 files originating from other sources like Ensembl or GENCODE . If you are selecting an annotation from Ensembl or GENCODE, please download the GTF file option. The only dependency of the script is the python package argparse, which comes bundled with the following python\u2154 distributions: python>=2.7.18 or python>=3.2 . If argparse is not installed, it can be downloaded with pip by running the following command: pip install --upgrade pip pip install argparse For more information about the script and its usage, please run: ./resources/gff3togtf.py -h","title":"4. Convert NCBI GFF3 to GTF format"},{"location":"RNA-seq/build/#5-example","text":"","title":"5. Example"},{"location":"RNA-seq/build/#51-biowulf","text":"On Biowulf getting started with the pipeline is fast and easy! In this example, we build a mouse reference genome. # Step 0.) Grab an interactive node (do not run on head node) srun -N 1 -n 1 --time = 2 :00:00 -p interactive --mem = 8gb --cpus-per-task = 4 --pty bash module purge module load ccbrpipeliner # Step 1.) Dry run the Build pipeline renee build --ref-fa GRCm39.primary_assembly.genome.fa \\ --ref-name mm39 \\ --ref-gtf gencode.vM26.annotation.gtf \\ --gtf-ver M26 \\ --output /data/ $USER /refs/mm39_M26 \\ --sif-cache /data/CCBR_Pipeliner/SIFs/ \\ --dry-run # Step 2.) Build new RENEE reference files renee build --ref-fa GRCm39.primary_assembly.genome.fa \\ --ref-name mm39 \\ --ref-gtf gencode.vM26.annotation.gtf \\ --gtf-ver M26 \\ --output /data/ $USER /refs/mm39_M26 \\ --sif-cache /data/CCBR_Pipeliner/SIFs/","title":"5.1 Biowulf"},{"location":"RNA-seq/build/#52-generic-slurm-cluster","text":"Running the pipeline outside of Biowulf is easy; however, there are a few extra options you must provide. Please note when running the build sub command for the first time, you will also need to provide the --shared-resources option. This option will download our kraken2 database and bowtie2 indices for FastQ Screen. The path provided to this option should be provided to the --shared-resources option of the run sub command. Next, you will also need to provide a path to write temporary output files via the --tmp-dir option. We also recommend providing a path to a SIF cache. You can cache software containers locally with the cache sub command. # Step 0.) Grab an interactive node (do not run on head node) srun -N 1 -n 1 --time = 2 :00:00 -p interactive --mem = 8gb --cpus-per-task = 4 --pty bash # Add snakemake and singularity to $PATH, # This step may vary across clusters, you # can reach out to a sys admin if snakemake # and singularity are not installed. module purge # Replace the following: # module load ccbrpipeliner # with module load statements that load # python >= 3.7, # snakemake, and # singularity # before running renee # Also, ensure that the `renee` executable is in PATH # Step 1.) Dry run the Build pipeline renee build --ref-fa GRCm39.primary_assembly.genome.fa \\ --ref-name mm39 \\ --ref-gtf gencode.vM26.annotation.gtf \\ --gtf-ver M26 \\ --output /data/ $USER /refs/mm39_M26 \\ --shared-resources /data/shared/renee \\ --tmp-dir /cluster_scratch/ $USER / \\ --sif-cache /data/ $USER /cache \\ --dry-run # Step 2.) Build new RENEE reference files renee build --ref-fa GRCm39.primary_assembly.genome.fa \\ --ref-name mm39 \\ --ref-gtf gencode.vM26.annotation.gtf \\ --gtf-ver M26 \\ --output /data/ $USER /refs/mm39_M26 \\ --shared-resources /data/shared/renee \\ --tmp-dir /cluster_scratch/ $USER / \\ --sif-cache /data/ $USER /cache","title":"5.2 Generic SLURM Cluster"},{"location":"RNA-seq/cache/","text":"renee cache \u00b6 1. About \u00b6 The renee executable is composed of several inter-related sub commands. Please see renee -h for all available options. This part of the documentation describes options and concepts for renee cache sub command in more detail. With minimal configuration, the cache sub command enables you to cache remote resources for the RENEE pipeline. Caching remote resources allows the pipeline to run in an offline mode. renee cache when run successfully submits a SLURM job to the job schedule and quits. squeue can then be used to track the progress of the caching. The cache sub command creates local cache on the filesysytem for resources hosted on DockerHub or AWS S3. These resources are normally pulled onto the filesystem when the pipeline runs; however, due to network issues or DockerHub pull rate limits, it may make sense to pull the resources once so a shared cache can be created and re-used. It is worth noting that a singularity cache cannot normally be shared across users. Singularity strictly enforces that its cache is owned by the user. To get around this issue, the cache subcommand can be used to create local SIFs on the filesystem from images on DockerHub. Caching remote resources for the RENEE pipeline is fast and easy! In its most basic form, renee cache only has one required input . 2. Synopsis \u00b6 $ renee cache [-h] --sif-cache SIF_CACHE \\ [--dry-run] The synopsis for each command shows its parameters and their usage. Optional parameters are shown in square brackets. A user must provide a directory to cache remote Docker images via the --sif-cache argument. Once the cache has pipeline completed, the local sif cache can be passed to the --sif-cache option of the renee build and renee run subcomand. This enables the build and run pipeline to run in an offline mode. Use you can always use the -h option for information on a specific command. 2.1 Required Arguments \u00b6 --sif-cache SIF_CACHE Path where a local cache of SIFs will be stored. type: path Any images defined in config/containers/images.json will be pulled into the local filesystem. The path provided to this option can be passed to the --sif-cache option of the renee build and renee run subcomand. This allows for running the build and run pipelines in an offline mode where no requests are made to external sources. This is useful for avoiding network issues or DockerHub pull rate limits. Please see renee build and run for more information. Example: --sif-cache /data/$USER/cache 2.2 Options \u00b6 Each of the following arguments are optional and do not need to be provided. -h, --help Display Help. type: boolean Shows command's synopsis, help message, and an example command Example: --help --dry-run Dry run the pipeline. type: boolean Displays what steps in the pipeline remain or will be run. Does not execute anything! Example: --dry-run 3. Example \u00b6 # Step 0.) Grab an interactive node (do not run on head node) srun -N 1 -n 1 --time = 12 :00:00 -p interactive --mem = 8gb --cpus-per-task = 4 --pty bash module purge module load ccbrpipeliner # Step 1.) Dry run cache to see what will be pulled renee cache --sif-cache /data/ $USER /cache \\ --dry-run # Step 2.) Cache remote resources locally renee cache --sif-cache /data/ $USER /cache","title":"cache"},{"location":"RNA-seq/cache/#renee-cache","text":"","title":"renee cache"},{"location":"RNA-seq/cache/#1-about","text":"The renee executable is composed of several inter-related sub commands. Please see renee -h for all available options. This part of the documentation describes options and concepts for renee cache sub command in more detail. With minimal configuration, the cache sub command enables you to cache remote resources for the RENEE pipeline. Caching remote resources allows the pipeline to run in an offline mode. renee cache when run successfully submits a SLURM job to the job schedule and quits. squeue can then be used to track the progress of the caching. The cache sub command creates local cache on the filesysytem for resources hosted on DockerHub or AWS S3. These resources are normally pulled onto the filesystem when the pipeline runs; however, due to network issues or DockerHub pull rate limits, it may make sense to pull the resources once so a shared cache can be created and re-used. It is worth noting that a singularity cache cannot normally be shared across users. Singularity strictly enforces that its cache is owned by the user. To get around this issue, the cache subcommand can be used to create local SIFs on the filesystem from images on DockerHub. Caching remote resources for the RENEE pipeline is fast and easy! In its most basic form, renee cache only has one required input .","title":"1. About"},{"location":"RNA-seq/cache/#2-synopsis","text":"$ renee cache [-h] --sif-cache SIF_CACHE \\ [--dry-run] The synopsis for each command shows its parameters and their usage. Optional parameters are shown in square brackets. A user must provide a directory to cache remote Docker images via the --sif-cache argument. Once the cache has pipeline completed, the local sif cache can be passed to the --sif-cache option of the renee build and renee run subcomand. This enables the build and run pipeline to run in an offline mode. Use you can always use the -h option for information on a specific command.","title":"2. Synopsis"},{"location":"RNA-seq/cache/#21-required-arguments","text":"--sif-cache SIF_CACHE Path where a local cache of SIFs will be stored. type: path Any images defined in config/containers/images.json will be pulled into the local filesystem. The path provided to this option can be passed to the --sif-cache option of the renee build and renee run subcomand. This allows for running the build and run pipelines in an offline mode where no requests are made to external sources. This is useful for avoiding network issues or DockerHub pull rate limits. Please see renee build and run for more information. Example: --sif-cache /data/$USER/cache","title":"2.1 Required Arguments"},{"location":"RNA-seq/cache/#22-options","text":"Each of the following arguments are optional and do not need to be provided. -h, --help Display Help. type: boolean Shows command's synopsis, help message, and an example command Example: --help --dry-run Dry run the pipeline. type: boolean Displays what steps in the pipeline remain or will be run. Does not execute anything! Example: --dry-run","title":"2.2 Options"},{"location":"RNA-seq/cache/#3-example","text":"# Step 0.) Grab an interactive node (do not run on head node) srun -N 1 -n 1 --time = 12 :00:00 -p interactive --mem = 8gb --cpus-per-task = 4 --pty bash module purge module load ccbrpipeliner # Step 1.) Dry run cache to see what will be pulled renee cache --sif-cache /data/ $USER /cache \\ --dry-run # Step 2.) Cache remote resources locally renee cache --sif-cache /data/ $USER /cache","title":"3. Example"},{"location":"RNA-seq/gui/","text":"Getting started \u00b6 1. Synopsis \u00b6 RENEE pipeline can be executed from either using the command line interface (CLI) or graphical user interface (GUI). GUI offers a more interactive way for the user to provide input and adjust parameter settings. This part of the documentation describes how to run renee using the GUI (with screenshots). See Command Line tab to read more about the renee executable and running RENEE pipeline using the CLI. 2. Setting up RENEE \u00b6 2.1 Login to cluster \u00b6 # Setup Step 1.) ssh into cluster's head node # example below for Biowulf cluster ssh -Y $USER @biowulf.nih.gov 2.2 Grab an interactive node \u00b6 NOTE : Make sure to add --tunnel flag to the sinteractive command for correct display settings. See details here: https://hpc.nih.gov/docs/tunneling/ # Setup Step 2.) Please do not run RENEE on the head node! # Grab an interactive node first sinteractive --tunnel --time = 12 :00:00 --mem = 8gb --cpus-per-task = 4 2.3 Load ccbrpipeliner module \u00b6 NOTE: ccbrpipeliner is a custom module created on biowulf which contains various NGS data analysis pipelines developed, tested, and benchmarked by experts at CCBR. # Setup Step 3.) Add ccbrpipeliner module module purge # to reset the module environment module load ccbrpipeliner If the module was loaded correctly, the greetings message should be displayed. [ + ] Loading ccbrpipeliner 6 ... ########################################################################### CCBR Pipeliner release 6 ########################################################################### \"ccbrpipeliner\" is a suite of end-to-end pipelines and tools Visit https://github.com/ccbr for more details. Pipelines are available on BIOWULF and FRCE. Tools are available on BIOWULF, HELIX and FRCE. The following pipelines/tools will be loaded in this module: PIPELINES: RENEE v2.5 https://ccbr.github.io/RENEE/ XAVIER v3.0 https://ccbr.github.io/XAVIER/ CARLISLE v2.4 https://ccbr.github.io/CARLISLE/ CHAMPAGNE v0.3 https://ccbr.github.io/CHAMPAGNE/ CRUISE v0.1 https://ccbr.github.io/CRUISE/ ASPEN v1.0 https://ccbr.github.io/ASPEN/ TOOLS: spacesavers2 v0.12 https://ccbr.github.io/spacesavers2/ permfix v0.6 https://github.com/ccbr/permfix/ ########################################################################### Thank you for using CCBR Pipeliner Comments/Questions/Requests: CCBR_Pipeliner@mail.nih.gov ########################################################################### To check the current version of RENEE, enter: renee --version 3. Running RENEE \u00b6 3.1 Launching RENEE GUI \u00b6 To run the RENEE pipeline from the GUI, simply enter: renee gui and it will launch the RENEE window. Note: Please wait until window created! message appears on the terminal. 3.2 Folder paths and reference genomes \u00b6 To enter the location of the input folder containing FASTQ files and the location where the results should be created, either simply type the absolute paths or use the Browse tab to choose the input and output directories Next, from the drop down menu select the reference genome (hg38/mm10) 3.3 Submit RENEE job \u00b6 After all the information is filled out, press Submit . If the pipeline detects no errors and the run was submitted, a new window appears that has the output of a \"dry-run\" which summarizes each step of the pipeline. Click OK A dialogue box will popup to confirm submitting the job to slurm. Click Yes An email notification will be sent out when the pipeline starts and ends. 4. Special instructions regarding X11 Window System \u00b6 RENEE GUI natively uses the X11 Window System to run RENEE pipeline and display the graphics on a personal desktop or laptop. The X11 Window System can be used to run a program on Biowulf and display the graphics on a desktop or laptop. However, X11 can be unreliable and fail with many graphics applications used on Biowulf. The HPC staff recommends NoMachine (NX) for users who need to run graphics applications. Please see details here on how to install and connect to Biowulf on your local computer using NoMachine. Once connected to Biowulf using NX, right click to open a terminal connection and start an interactive session (with --tunnel flag). Similar to the instructions above, load the ccbrpipeliner module and enter renee gui to launch the RENEE gui.","title":"Graphical Interface"},{"location":"RNA-seq/gui/#getting-started","text":"","title":"Getting started"},{"location":"RNA-seq/gui/#1-synopsis","text":"RENEE pipeline can be executed from either using the command line interface (CLI) or graphical user interface (GUI). GUI offers a more interactive way for the user to provide input and adjust parameter settings. This part of the documentation describes how to run renee using the GUI (with screenshots). See Command Line tab to read more about the renee executable and running RENEE pipeline using the CLI.","title":"1. Synopsis"},{"location":"RNA-seq/gui/#2-setting-up-renee","text":"","title":"2. Setting up RENEE"},{"location":"RNA-seq/gui/#21-login-to-cluster","text":"# Setup Step 1.) ssh into cluster's head node # example below for Biowulf cluster ssh -Y $USER @biowulf.nih.gov","title":"2.1 Login to cluster"},{"location":"RNA-seq/gui/#22-grab-an-interactive-node","text":"NOTE : Make sure to add --tunnel flag to the sinteractive command for correct display settings. See details here: https://hpc.nih.gov/docs/tunneling/ # Setup Step 2.) Please do not run RENEE on the head node! # Grab an interactive node first sinteractive --tunnel --time = 12 :00:00 --mem = 8gb --cpus-per-task = 4","title":"2.2 Grab an interactive node"},{"location":"RNA-seq/gui/#23-load-ccbrpipeliner-module","text":"NOTE: ccbrpipeliner is a custom module created on biowulf which contains various NGS data analysis pipelines developed, tested, and benchmarked by experts at CCBR. # Setup Step 3.) Add ccbrpipeliner module module purge # to reset the module environment module load ccbrpipeliner If the module was loaded correctly, the greetings message should be displayed. [ + ] Loading ccbrpipeliner 6 ... ########################################################################### CCBR Pipeliner release 6 ########################################################################### \"ccbrpipeliner\" is a suite of end-to-end pipelines and tools Visit https://github.com/ccbr for more details. Pipelines are available on BIOWULF and FRCE. Tools are available on BIOWULF, HELIX and FRCE. The following pipelines/tools will be loaded in this module: PIPELINES: RENEE v2.5 https://ccbr.github.io/RENEE/ XAVIER v3.0 https://ccbr.github.io/XAVIER/ CARLISLE v2.4 https://ccbr.github.io/CARLISLE/ CHAMPAGNE v0.3 https://ccbr.github.io/CHAMPAGNE/ CRUISE v0.1 https://ccbr.github.io/CRUISE/ ASPEN v1.0 https://ccbr.github.io/ASPEN/ TOOLS: spacesavers2 v0.12 https://ccbr.github.io/spacesavers2/ permfix v0.6 https://github.com/ccbr/permfix/ ########################################################################### Thank you for using CCBR Pipeliner Comments/Questions/Requests: CCBR_Pipeliner@mail.nih.gov ########################################################################### To check the current version of RENEE, enter: renee --version","title":"2.3 Load ccbrpipeliner module"},{"location":"RNA-seq/gui/#3-running-renee","text":"","title":"3. Running RENEE"},{"location":"RNA-seq/gui/#31-launching-renee-gui","text":"To run the RENEE pipeline from the GUI, simply enter: renee gui and it will launch the RENEE window. Note: Please wait until window created! message appears on the terminal.","title":"3.1 Launching RENEE GUI"},{"location":"RNA-seq/gui/#32-folder-paths-and-reference-genomes","text":"To enter the location of the input folder containing FASTQ files and the location where the results should be created, either simply type the absolute paths or use the Browse tab to choose the input and output directories Next, from the drop down menu select the reference genome (hg38/mm10)","title":"3.2 Folder paths and reference genomes"},{"location":"RNA-seq/gui/#33-submit-renee-job","text":"After all the information is filled out, press Submit . If the pipeline detects no errors and the run was submitted, a new window appears that has the output of a \"dry-run\" which summarizes each step of the pipeline. Click OK A dialogue box will popup to confirm submitting the job to slurm. Click Yes An email notification will be sent out when the pipeline starts and ends.","title":"3.3 Submit RENEE job"},{"location":"RNA-seq/gui/#4-special-instructions-regarding-x11-window-system","text":"RENEE GUI natively uses the X11 Window System to run RENEE pipeline and display the graphics on a personal desktop or laptop. The X11 Window System can be used to run a program on Biowulf and display the graphics on a desktop or laptop. However, X11 can be unreliable and fail with many graphics applications used on Biowulf. The HPC staff recommends NoMachine (NX) for users who need to run graphics applications. Please see details here on how to install and connect to Biowulf on your local computer using NoMachine. Once connected to Biowulf using NX, right click to open a terminal connection and start an interactive session (with --tunnel flag). Similar to the instructions above, load the ccbrpipeliner module and enter renee gui to launch the RENEE gui.","title":"4. Special instructions regarding X11 Window System"},{"location":"RNA-seq/output/","text":"After a successful renee run execution for multisample paired-end data, the following files and folders are created in the output folder. renee_output/ \u251c\u2500\u2500 bams \u251c\u2500\u2500 config \u251c\u2500\u2500 config.json # Contains the configuration and parameters used for this specific RENEE run \u251c\u2500\u2500 DEG_ALL \u251c\u2500\u2500 dryrun. { datetime } .log # Output from the dry-run of the pipeline \u251c\u2500\u2500 FQscreen \u251c\u2500\u2500 FQscreen2 \u251c\u2500\u2500 fusions \u251c\u2500\u2500 kraken \u251c\u2500\u2500 logfiles \u251c\u2500\u2500 nciccbr \u251c\u2500\u2500 preseq \u251c\u2500\u2500 QC \u251c\u2500\u2500 QualiMap \u251c\u2500\u2500 rawQC \u251c\u2500\u2500 Reports \u251c\u2500\u2500 resources \u251c\u2500\u2500 RSeQC \u251c\u2500\u2500 sample1.R1.fastq.gz -> /path/to/input/fastq/files/sample1.R1.fastq.gz \u251c\u2500\u2500 sample1.R2.fastq.gz -> /path/to/input/fastq/files/sample1.R2.fastq.gz ... .. . \u251c\u2500\u2500 sampleN.R1.fastq.gz -> /path/to/input/fastq/files/sampleN.R1.fastq.gz \u251c\u2500\u2500 sampleN.R2.fastq.gz -> /path/to/input/fastq/files/sampleN.R2.fastq.gz \u251c\u2500\u2500 STAR_files \u251c\u2500\u2500 trim \u2514\u2500\u2500 workflow Folder details and file descriptions \u00b6 1. bams \u00b6 Contains the STAR aligned reads for each sample analyzed in the run. /bams/ \u251c\u2500\u2500 sample1.fwd.bw # forward strand bigwig files suitable for a genomic track viewer like IGV \u251c\u2500\u2500 sample1.rev.bw # reverse strand bigwig files \u251c\u2500\u2500 sample1.p2.Aligned.toTranscriptome.out.bam # BAM alignments to transcriptome using STAR in two-pass mode \u251c\u2500\u2500 sample1.star_rg_added.sorted.dmark.bam # Read groups added and duplicates marked genomic BAM file (using STAR in two-pass mode) \u251c\u2500\u2500 sample1.star_rg_added.sorted.dmark.bam.bai ... .. . 2. config \u00b6 Contains config files for the pipeline. 3. DEG_ALL \u00b6 Contains the output from RSEM estimating gene and isoform expression levels for each sample and also combined data matrix with all samples. /DEG_ALL/ \u251c\u2500\u2500 combined_TIN.tsv # RSeQC logfiles containing transcript integrity number information for all samples \u251c\u2500\u2500 RSEM.genes.expected_count.all_samples.txt # Expected gene counts matrix for all samples (useful for downstream differential expression analysis) \u251c\u2500\u2500 RSEM.genes.expected_counts.all_samples.reformatted.tsv # Expected gene counts matrix for all samples with reformatted gene symbols (format: ENSEMBLID | GeneName) \u251c\u2500\u2500 RSEM.genes.FPKM.all_samples.txt # FPKM Normalized expected gene counts matrix for all samples \u251c\u2500\u2500 RSEM.genes.TPM.all_samples.txt # TPM Normalized expected gene counts matrix for all samples \u251c\u2500\u2500 RSEM.isoforms.expected_count.all_samples.txt # File containing isoform level expression estimates for all samples. \u251c\u2500\u2500 RSEM.isoforms.FPKM.all_samples.txt # FPKM Normalized expected isoform counts matrix for all samples \u251c\u2500\u2500 RSEM.isoforms.TPM.all_samples.txt # TPM Normalized expected isoform counts matrix for all samples \u251c\u2500\u2500 sample1.RSEM.genes.results # Expected gene counts for sample 1 \u251c\u2500\u2500 sample1.RSEM.isoforms.results # Expected isoform counts for sample 1 \u251c\u2500\u2500 sample1.RSEM.stat # RSEM stats for sample 1 \u2502 \u251c\u2500\u2500 sample1.RSEM.cnt \u2502 \u251c\u2500\u2500 sample1.RSEM.model \u2502 \u2514\u2500\u2500 sample1.RSEM.theta \u251c\u2500\u2500 sample1.RSEM.time # Run time log for sample 1 ... .. . \u251c\u2500\u2500 sampleN.RSEM.genes.results \u251c\u2500\u2500 sampleN.RSEM.isoforms.results \u251c\u2500\u2500 sampleN.RSEM.stat \u2502 \u251c\u2500\u2500 sampleN.RSEM.cnt \u2502 \u251c\u2500\u2500 sampleN.RSEM.model \u2502 \u2514\u2500\u2500 sampleN.RSEM.theta \u2514\u2500\u2500 sampleN.RSEM.time 4. FQScreen and FQScreen2 \u00b6 These folders contain results from quality-control step to screen for different sources of contamination. FastQ Screen compares your sequencing data to a set of different reference genomes to determine if there is contamination. It allows a user to see if the composition of your library matches what you expect. These results are plotted in the multiQC report. 5. fusions \u00b6 Contains gene fusions output for each sample. fusions/ \u251c\u2500\u2500 sample1_fusions.arriba.pdf \u251c\u2500\u2500 sample1_fusions.discarded.tsv # Contains all events that Arriba classified as an artifact or that are also observed in healthy tissue. \u251c\u2500\u2500 sample1_fusions.tsv # Contains fusions for sample 1 which pass all of Arriba's filters. The predictions are listed from highest to lowest confidence. \u251c\u2500\u2500 sample1.p2.arriba.Aligned.sortedByCoord.out.bam # Sorted BAM file for Arriba's Visualization \u251c\u2500\u2500 sample1.p2.arriba.Aligned.sortedByCoord.out.bam.bai \u251c\u2500\u2500 sample1.p2.Log.final.out # STAR final log file \u251c\u2500\u2500 sample1.p2.Log.out # STAR runtime log file \u251c\u2500\u2500 sample1.p2.Log.progress.out # log files \u251c\u2500\u2500 sample1.p2.Log.std.out # STAR runtime output log \u251c\u2500\u2500 sample1.p2.SJ.out.tab # Summarizes the high confidence splice junctions for sample 1 \u251c\u2500\u2500 sample1.p2._STARgenome # Extra files generated during STAR aligner \u2502 \u251c\u2500\u2500 exonGeTrInfo.tab \u2502 \u251c\u2500\u2500 . \u2502 \u251c\u2500\u2500 . \u2502 \u2514\u2500\u2500 transcriptInfo.tab \u251c\u2500\u2500 sample1.p2._STARpass1 # Extra files generated during STAR first pass \u2502 \u251c\u2500\u2500 . \u2502 \u2514\u2500\u2500 . ... .. . 6. kraken \u00b6 Contains per sample kraken output files which is a Quality-control step to assess for potential sources of microbial contamination. Kraken is used in conjunction with Krona to produce an interactive reports stored in .krona.html files. These results are present in the multiQC report. 7. logfiles \u00b6 Contains logfiles for the entire RENEE run, job error/output files for each individual job that was submitted to SLURM, and some other stats generated by different software. Important to diagnose errors if the pipeline fails. The per sample stats information is present in the mulitQC report. /logfiles/ \u251c\u2500\u2500 master.log # Logfile for the main (master) RENEE job \u251c\u2500\u2500 mjobid.log # SLURM JOBID for the master RENEE job \u251c\u2500\u2500 runtime_statistics.json # Runtime statistics for each rule in the RENEE run \u251c\u2500\u2500 sample1.flagstat.concord.txt # sample mapping stats \u251c\u2500\u2500 sample1.p2.Log.final.out # sample STAR alignment stats \u251c\u2500\u2500 sample1.RnaSeqMetrics.txt # sample stats collected by Picard CollectRnaSeqMetrics \u251c\u2500\u2500 sample1.star.duplic # Mark duplicate metrics ... .. . \u251c\u2500\u2500 slurmfiles \u2502 \u251c\u2500\u2500 { MASTER_JOBID } . { JOBID } . { rule } . { wildcards } .out \u2502 \u251c\u2500\u2500 { MASTER_JOBID } . { JOBID } . { rule } . { wildcards } .err \u2502 ... \u2502 .. \u2502 . \u251c\u2500\u2500 snakemake.log # The snakemake log file which documents the entire pipeline log \u251c\u2500\u2500 snakemake.log.jobby # Detailed summary report for each individual job. \u2514\u2500\u2500 snakemake.log.jobby.short # Short summary report for each individual job. 8. nciccbr \u00b6 Contain Arriba resources for gene fusion estimation. Manually curated and only exist for a few reference genomes (mm10, hg38, hg19). 9. preseq \u00b6 Contains library complexity curves for each sample. These results are part of the multiQC report. 10. QC and rawQC \u00b6 Contains per sample output from FastQC for raw and adapter trimmed fastq files with insert size estimates. These results are part of the multiQC report. 11. QualiMap \u00b6 Contains per sample output for Quality-control step to assess various post-alignment metrics and a secondary method to calculate insert size. These results are part of the multiQC report. 12. Reports \u00b6 Contains the multiQC report which visually summarizes the quality control metrics and other statistics for each sample ( multiqc_report.html ). All the data tables used to generate the multiQC report is available in the multiqc_data folder. The RNA_report.html file is an interactive report the aggregates sample quality-control metrics across all samples. This interactive report to allow users to identify problematic samples prior to downstream analysis. It uses flowcell and lane information from the FastQ file. 13. resources \u00b6 Contains resources necessary to run the RENEE pipeline. 14. RSeQC \u00b6 Contains various QC metrics for each sample collected by RSeQC. These results are part of the multiQC report. 15. STAR_files \u00b6 Contains log files, splice junction tab file ( SJ.out.tab ), and ReadsPerGene.out.tab file, and other various output files for each sample generated by STAR aligner. 16. trim \u00b6 Contains adapter trimmed FASTQ files for each sample used for all the downstream analysis. trim \u251c\u2500\u2500 sample1.R1.trim.fastq.gz \u251c\u2500\u2500 sample1.R2.trim.fastq.gz ... .. . \u251c\u2500\u2500 sampleN.R1.trim.fastq.gz \u2514\u2500\u2500 sampleN.R2.trim.fastq.gz 17. workflow \u00b6 Contains the RENEE pipeline workflow.","title":"Expected Output"},{"location":"RNA-seq/output/#folder-details-and-file-descriptions","text":"","title":"Folder details and file descriptions"},{"location":"RNA-seq/output/#1-bams","text":"Contains the STAR aligned reads for each sample analyzed in the run. /bams/ \u251c\u2500\u2500 sample1.fwd.bw # forward strand bigwig files suitable for a genomic track viewer like IGV \u251c\u2500\u2500 sample1.rev.bw # reverse strand bigwig files \u251c\u2500\u2500 sample1.p2.Aligned.toTranscriptome.out.bam # BAM alignments to transcriptome using STAR in two-pass mode \u251c\u2500\u2500 sample1.star_rg_added.sorted.dmark.bam # Read groups added and duplicates marked genomic BAM file (using STAR in two-pass mode) \u251c\u2500\u2500 sample1.star_rg_added.sorted.dmark.bam.bai ... .. .","title":"1. bams"},{"location":"RNA-seq/output/#2-config","text":"Contains config files for the pipeline.","title":"2. config"},{"location":"RNA-seq/output/#3-deg_all","text":"Contains the output from RSEM estimating gene and isoform expression levels for each sample and also combined data matrix with all samples. /DEG_ALL/ \u251c\u2500\u2500 combined_TIN.tsv # RSeQC logfiles containing transcript integrity number information for all samples \u251c\u2500\u2500 RSEM.genes.expected_count.all_samples.txt # Expected gene counts matrix for all samples (useful for downstream differential expression analysis) \u251c\u2500\u2500 RSEM.genes.expected_counts.all_samples.reformatted.tsv # Expected gene counts matrix for all samples with reformatted gene symbols (format: ENSEMBLID | GeneName) \u251c\u2500\u2500 RSEM.genes.FPKM.all_samples.txt # FPKM Normalized expected gene counts matrix for all samples \u251c\u2500\u2500 RSEM.genes.TPM.all_samples.txt # TPM Normalized expected gene counts matrix for all samples \u251c\u2500\u2500 RSEM.isoforms.expected_count.all_samples.txt # File containing isoform level expression estimates for all samples. \u251c\u2500\u2500 RSEM.isoforms.FPKM.all_samples.txt # FPKM Normalized expected isoform counts matrix for all samples \u251c\u2500\u2500 RSEM.isoforms.TPM.all_samples.txt # TPM Normalized expected isoform counts matrix for all samples \u251c\u2500\u2500 sample1.RSEM.genes.results # Expected gene counts for sample 1 \u251c\u2500\u2500 sample1.RSEM.isoforms.results # Expected isoform counts for sample 1 \u251c\u2500\u2500 sample1.RSEM.stat # RSEM stats for sample 1 \u2502 \u251c\u2500\u2500 sample1.RSEM.cnt \u2502 \u251c\u2500\u2500 sample1.RSEM.model \u2502 \u2514\u2500\u2500 sample1.RSEM.theta \u251c\u2500\u2500 sample1.RSEM.time # Run time log for sample 1 ... .. . \u251c\u2500\u2500 sampleN.RSEM.genes.results \u251c\u2500\u2500 sampleN.RSEM.isoforms.results \u251c\u2500\u2500 sampleN.RSEM.stat \u2502 \u251c\u2500\u2500 sampleN.RSEM.cnt \u2502 \u251c\u2500\u2500 sampleN.RSEM.model \u2502 \u2514\u2500\u2500 sampleN.RSEM.theta \u2514\u2500\u2500 sampleN.RSEM.time","title":"3. DEG_ALL"},{"location":"RNA-seq/output/#4-fqscreen-and-fqscreen2","text":"These folders contain results from quality-control step to screen for different sources of contamination. FastQ Screen compares your sequencing data to a set of different reference genomes to determine if there is contamination. It allows a user to see if the composition of your library matches what you expect. These results are plotted in the multiQC report.","title":"4. FQScreen and FQScreen2"},{"location":"RNA-seq/output/#5-fusions","text":"Contains gene fusions output for each sample. fusions/ \u251c\u2500\u2500 sample1_fusions.arriba.pdf \u251c\u2500\u2500 sample1_fusions.discarded.tsv # Contains all events that Arriba classified as an artifact or that are also observed in healthy tissue. \u251c\u2500\u2500 sample1_fusions.tsv # Contains fusions for sample 1 which pass all of Arriba's filters. The predictions are listed from highest to lowest confidence. \u251c\u2500\u2500 sample1.p2.arriba.Aligned.sortedByCoord.out.bam # Sorted BAM file for Arriba's Visualization \u251c\u2500\u2500 sample1.p2.arriba.Aligned.sortedByCoord.out.bam.bai \u251c\u2500\u2500 sample1.p2.Log.final.out # STAR final log file \u251c\u2500\u2500 sample1.p2.Log.out # STAR runtime log file \u251c\u2500\u2500 sample1.p2.Log.progress.out # log files \u251c\u2500\u2500 sample1.p2.Log.std.out # STAR runtime output log \u251c\u2500\u2500 sample1.p2.SJ.out.tab # Summarizes the high confidence splice junctions for sample 1 \u251c\u2500\u2500 sample1.p2._STARgenome # Extra files generated during STAR aligner \u2502 \u251c\u2500\u2500 exonGeTrInfo.tab \u2502 \u251c\u2500\u2500 . \u2502 \u251c\u2500\u2500 . \u2502 \u2514\u2500\u2500 transcriptInfo.tab \u251c\u2500\u2500 sample1.p2._STARpass1 # Extra files generated during STAR first pass \u2502 \u251c\u2500\u2500 . \u2502 \u2514\u2500\u2500 . ... .. .","title":"5. fusions"},{"location":"RNA-seq/output/#6-kraken","text":"Contains per sample kraken output files which is a Quality-control step to assess for potential sources of microbial contamination. Kraken is used in conjunction with Krona to produce an interactive reports stored in .krona.html files. These results are present in the multiQC report.","title":"6. kraken"},{"location":"RNA-seq/output/#7-logfiles","text":"Contains logfiles for the entire RENEE run, job error/output files for each individual job that was submitted to SLURM, and some other stats generated by different software. Important to diagnose errors if the pipeline fails. The per sample stats information is present in the mulitQC report. /logfiles/ \u251c\u2500\u2500 master.log # Logfile for the main (master) RENEE job \u251c\u2500\u2500 mjobid.log # SLURM JOBID for the master RENEE job \u251c\u2500\u2500 runtime_statistics.json # Runtime statistics for each rule in the RENEE run \u251c\u2500\u2500 sample1.flagstat.concord.txt # sample mapping stats \u251c\u2500\u2500 sample1.p2.Log.final.out # sample STAR alignment stats \u251c\u2500\u2500 sample1.RnaSeqMetrics.txt # sample stats collected by Picard CollectRnaSeqMetrics \u251c\u2500\u2500 sample1.star.duplic # Mark duplicate metrics ... .. . \u251c\u2500\u2500 slurmfiles \u2502 \u251c\u2500\u2500 { MASTER_JOBID } . { JOBID } . { rule } . { wildcards } .out \u2502 \u251c\u2500\u2500 { MASTER_JOBID } . { JOBID } . { rule } . { wildcards } .err \u2502 ... \u2502 .. \u2502 . \u251c\u2500\u2500 snakemake.log # The snakemake log file which documents the entire pipeline log \u251c\u2500\u2500 snakemake.log.jobby # Detailed summary report for each individual job. \u2514\u2500\u2500 snakemake.log.jobby.short # Short summary report for each individual job.","title":"7. logfiles"},{"location":"RNA-seq/output/#8-nciccbr","text":"Contain Arriba resources for gene fusion estimation. Manually curated and only exist for a few reference genomes (mm10, hg38, hg19).","title":"8. nciccbr"},{"location":"RNA-seq/output/#9-preseq","text":"Contains library complexity curves for each sample. These results are part of the multiQC report.","title":"9. preseq"},{"location":"RNA-seq/output/#10-qc-and-rawqc","text":"Contains per sample output from FastQC for raw and adapter trimmed fastq files with insert size estimates. These results are part of the multiQC report.","title":"10. QC and rawQC"},{"location":"RNA-seq/output/#11-qualimap","text":"Contains per sample output for Quality-control step to assess various post-alignment metrics and a secondary method to calculate insert size. These results are part of the multiQC report.","title":"11. QualiMap"},{"location":"RNA-seq/output/#12-reports","text":"Contains the multiQC report which visually summarizes the quality control metrics and other statistics for each sample ( multiqc_report.html ). All the data tables used to generate the multiQC report is available in the multiqc_data folder. The RNA_report.html file is an interactive report the aggregates sample quality-control metrics across all samples. This interactive report to allow users to identify problematic samples prior to downstream analysis. It uses flowcell and lane information from the FastQ file.","title":"12. Reports"},{"location":"RNA-seq/output/#13-resources","text":"Contains resources necessary to run the RENEE pipeline.","title":"13. resources"},{"location":"RNA-seq/output/#14-rseqc","text":"Contains various QC metrics for each sample collected by RSeQC. These results are part of the multiQC report.","title":"14. RSeQC"},{"location":"RNA-seq/output/#15-star_files","text":"Contains log files, splice junction tab file ( SJ.out.tab ), and ReadsPerGene.out.tab file, and other various output files for each sample generated by STAR aligner.","title":"15. STAR_files"},{"location":"RNA-seq/output/#16-trim","text":"Contains adapter trimmed FASTQ files for each sample used for all the downstream analysis. trim \u251c\u2500\u2500 sample1.R1.trim.fastq.gz \u251c\u2500\u2500 sample1.R2.trim.fastq.gz ... .. . \u251c\u2500\u2500 sampleN.R1.trim.fastq.gz \u2514\u2500\u2500 sampleN.R2.trim.fastq.gz","title":"16. trim"},{"location":"RNA-seq/output/#17-workflow","text":"Contains the RENEE pipeline workflow.","title":"17. workflow"},{"location":"RNA-seq/run/","text":"renee run \u00b6 1. About \u00b6 The renee executable is composed of several inter-related sub commands. Please see renee -h for all available options. This part of the documentation describes options and concepts for renee run sub command in more detail. With minimal configuration, the run sub command enables you to start running the data processing and quality-control pipeline. Setting up the RENEE pipeline is fast and easy! In its most basic form, renee run only has three required inputs . 2. Synopsis \u00b6 $ renee run [--help] \\ [--small-rna] [--star-2-pass-basic] \\ [--dry-run] [--mode {slurm, local}] \\ [--shared-resources SHARED_RESOURCES] \\ [--singularity-cache SINGULARITY_CACHE] \\ [--sif-cache SIF_CACHE] \\ [--tmp-dir TMP_DIR] \\ [--threads THREADS] \\ --input INPUT [INPUT ...] \\ --output OUTPUT \\ --genome {hg38_36, mm10_M21, ...} The synopsis for each command shows its parameters and their usage. Optional parameters are shown in square brackets. A user must provide a list of FastQ files (globbing is supported) to analyze via --input argument, an output directory to store results via --output argument and select reference genome for alignment and annotation via the --genome argument which is hg38_36 by default. If you are running the pipeline outside of Biowulf, you will need to additionally provide the the following options: --shared-resources , --tmp-dir . More information about each of these options can be found below. Use you can always use the -h option for information on a specific sub command. 2.1 Required Arguments \u00b6 Each of the following arguments are required. Failure to provide a required argument will result in a non-zero exit-code. --input INPUT [INPUT ...] Input FastQ file(s) to process. type: file One or more FastQ files can be provided. From the command-line, each FastQ file should separated by a space. Globbing is supported! This makes selecting FastQ files easier. Input FastQ files should be gzipp-ed. The pipeline supports single-end and pair-end RNA-seq data; however, the pipeline will not process a mixture of single-end and paired-end samples together. If you have a mixture of single-end and pair-end samples to process, please process them as two separate instances of the RENEE pipeline (with two separate output directories). Example: --input .tests/*.R?.fastq.gz --output OUTPUT Path to an output directory. type: path This location is where the pipeline will create all of its output files, also known as the pipeline's working directory. If the provided output directory does not exist, it will be initialized automatically. Example: --output /data/$USER/RNA_hg38 --genome {hg38_36,mm10_M21,custom.json,...} Reference genome. type: string or file This option defines the reference genome for your set of samples. The default is hg38_36 . On Biowulf, RENEE does comes bundled with pre built reference files for human and mouse samples; however, it is worth noting that the pipeline does accept a custom reference genome built with the build sub command. Building a new reference genome is easy! You can create a custom reference genome with a single command. This is extremely useful when working with non-model organisms. New users can reference the documentation's getting started section to see how a reference genome is built. Pre built Option Pre build genomes are available with RENEE. Please see the resources page for more information about each pre built option. Custom Option A user can also supply a custom reference genome built with the build sub command. Please supply the custom reference JSON file that was generated by the build sub command. The name of this custom reference JSON file is dependent on the values provided to the following renee build args, --ref-name REF_NAME and --gtf-ver GTF_VER , where the name of the provided custom reference JSON file would be: {REF_NAME}_{GTF_VER}.json . Example: --genome hg38_36 OR --genome /data/${USER}/hg38_36/hg38_36.json 2.2 Analysis Options \u00b6 --small-rna Run STAR using ENCODE's recommendations for small RNA. type: boolean This option should only be used with small RNA libraries. These are rRNA-depleted libraries that have been size selected to contain fragments shorter than 200bp. Size selection enriches for small RNA species such as miRNAs, siRNAs, or piRNAs. Also, this option should not be combined with the star 2-pass basic option. If the two options are combined, STAR will run in pass basic mode. This means that STAR will not run with ENCODE's recommendations for small RNA alignment. As so, please take caution not to combine both options together. Please note: This option is only supported with single-end data. Example: --small-rna --star-2-pass-basic Run STAR in per sample 2-pass mapping mode. type: boolean It is recommended to use this option when processing a set of unrelated samples or when processing samples in a clinical setting. It is not adivsed to use this option for a study with multiple related samples. By default, the pipeline ultilizes a multi sample 2-pass mapping approach where the set of splice junctions detected across all samples are provided to the second pass of STAR. This option overrides the default behavior so each sample will be processed in a per sample two-pass basic mode. This option should not be combined with the small RNA option. If the two options are combined, STAR will run in pass basic mode. Example: --star-2-pass-basic 2.3 Orchestration Options \u00b6 Each of the following arguments are optional and do not need to be provided. --dry-run Dry run the pipeline. type: boolean Displays what steps in the pipeline remain or will be run. Does not execute anything! Example: --dry-run --mode {slurm,local} Execution Method. > type: string default: slurm Execution Method. Defines the mode or method of execution. Valid mode options include: slurm or local. local Local executions will run serially on compute instance. This is useful for testing, debugging, or when a users does not have access to a high performance computing environment. If this option is not provided, it will default to a local execution mode. slurm The slurm execution method will submit jobs to a cluster using a slurm + singularity backend. This method will automatically submit the master job to the cluster. It is recommended running RENEE in this mode as execution will be significantly faster in a distributed environment. Example: --mode slurm --shared-resources SHARED_RESOURCES Local path to shared resources. type: path The pipeline uses a set of shared reference files that can be re-used across reference genomes. These currently include reference files for kraken and FQScreen. These reference files can be downloaded with the build sub command's --shared-resources option. With that being said, these files only need to be downloaded once. We recommend storing this files in a shared location on the filesystem that other people can access. If you are running the pipeline on Biowulf, you do NOT need to download these reference files! They already exist on the filesystem in a location that anyone can access; however, if you are running the pipeline on another cluster or target system, you will need to download the shared resources with the build sub command, and you will need to provide this option every time you run the pipeline. Please provide the same path that was provided to the build sub command's --shared-resources option. Again, if you are running the pipeline on Biowulf, you do NOT need to provide this option. For more information about how to download shared resources, please reference the build sub command's --shared-resources option. Example: --shared-resources /data/shared/renee --singularity-cache SINGULARITY_CACHE Overrides the $SINGULARITY_CACHEDIR environment variable. type: path default: --output OUTPUT/.singularity Singularity will cache image layers pulled from remote registries. This ultimately speeds up the process of pull an image from DockerHub if an image layer already exists in the singularity cache directory. By default, the cache is set to the value provided to the --output argument. Please note that this cache cannot be shared across users. Singularity strictly enforces you own the cache directory and will return a non-zero exit code if you do not own the cache directory! See the --sif-cache option to create a shareable resource. Example: --singularity-cache /data/$USER/.singularity --sif-cache SIF_CACHE Path where a local cache of SIFs are stored. type: path Uses a local cache of SIFs on the filesystem. This SIF cache can be shared across users if permissions are set correctly. If a SIF does not exist in the SIF cache, the image will be pulled from Dockerhub and a warning message will be displayed. The renee cache subcommand can be used to create a local SIF cache. Please see renee cache for more information. This command is extremely useful for avoiding DockerHub pull rate limits. It also remove any potential errors that could occur due to network issues or DockerHub being temporarily unavailable. We recommend running RENEE with this option when ever possible. Example: --singularity-cache /data/$USER/SIFs --tmp-dir TMP_DIR Path on the file system for writing temporary files. type: path default: /lscratch/$SLURM_JOBID Path on the file system for writing temporary output files. By default, the temporary directory is set to '/lscratch/$SLURM_JOBID' on NIH's Biowulf cluster and 'OUTPUT' on the FRCE cluster. However, if you are running the pipeline on another cluster, this option will need to be specified. Ideally, this path should point to a dedicated location on the filesystem for writing tmp files. On many systems, this location is set to somewhere in /scratch. If you need to inject avariable into this string that should NOT be expanded,please quote this options value in single quotes. Example: --tmp-dir '/cluster_scratch/$USER/' --threads THREADS Max number of threads for each process. type: int default: 2 Max number of threads for each process. This option is more applicable when running the pipeline with --mode local . It is recommended setting this value to the maximum number of CPUs available on the host machine. Example: --threads 12 2.4 Misc Options \u00b6 Each of the following arguments are optional and do not need to be provided. -h, --help Display Help. type: boolean Shows command's synopsis, help message, and an example command Example: --help 3. Example \u00b6 3.1 Biowulf \u00b6 On Biowulf getting started with the pipeline is fast and easy! The pipeline comes bundled with pre-built human and mouse reference genomes. In the example below, we will use the pre-built human reference genome. # Step 0.) Grab an interactive node (do not run on head node) srun -N 1 -n 1 --time = 12 :00:00 -p interactive --mem = 8gb --cpus-per-task = 4 --pty bash module purge module load ccbrpipeliner # Step 1.) Dry run pipeline with provided test data renee run --input .tests/*.R?.fastq.gz \\ --output /data/ $USER /RNA_hg38 \\ --genome hg38_36 \\ --mode slurm \\ --star-2-pass-basic \\ --sif-cache /data/OpenOmics/SIFs/ \\ --dry-run # Step 2.) Run RENEE pipeline # The slurm mode will submit jobs to the cluster. # It is recommended running renee in this mode. renee run --input .tests/*.R?.fastq.gz \\ --output /data/ $USER /RNA_hg38 \\ --genome hg38_36 \\ --mode slurm \\ --sif-cache /data/OpenOmics/SIFs/ \\ --star-2-pass-basic 3.2 Generic SLURM Cluster \u00b6 Running the pipeline outside of Biowulf is easy; however, there are a few extra steps you must first take. Before getting started, you will need to build reference files for the pipeline. Please note when running the build sub command for the first time, you will also need to provide the --shared-resources option. This option will download our kraken2 database and bowtie2 indices for FastQ Screen. The path provided to this option should be provided to the --shared-resources option of the run sub command. Next, you will also need to provide a path to write temporary output files via the --tmp-dir option. We also recommend providing a path to a SIF cache. You can cache software containers locally with the cache sub command. # Step 0.) Grab an interactive node (do not run on head node) srun -N 1 -n 1 --time = 2 :00:00 -p interactive --mem = 8gb --cpus-per-task = 4 --pty bash # Add snakemake and singularity to $PATH, # This step may vary across clusters, you # can reach out to a sys admin if snakemake # and singularity are not installed. module purge # Replace the following: # module load ccbrpipeliner # with module load statements that load # python >= 3.7, # snakemake, and # singularity # before running renee # Also, ensure that the `renee` execulable is in PATH # Step 1.) Dry run pipeline with provided test data renee run --input .tests/*.R?.fastq.gz \\ --output /data/ $USER /RNA_hg38 \\ --genome /data/ $USER /hg38_36/hg38_36.json \\ --mode slurm \\ --sif-cache /data/ $USER /cache \\ --star-2-pass-basic \\ --shared-resources /data/shared/renee \\ --tmp-dir /cluster_scratch/ $USER / \\ --dry-run # Step 2.) Run RENEE pipeline # The slurm mode will submit jobs to the cluster. # It is recommended running renee in this mode. renee run --input .tests/*.R?.fastq.gz \\ --output /data/ $USER /RNA_hg38 \\ --genome /data/ $USER /hg38_36/hg38_36.json \\ --mode slurm \\ --sif-cache /data/ $USER /cache \\ --star-2-pass-basic \\ --shared-resources /data/shared/renee \\ --tmp-dir /cluster_scratch/ $USER / \\ --dry-run","title":"run"},{"location":"RNA-seq/run/#renee-run","text":"","title":"renee run"},{"location":"RNA-seq/run/#1-about","text":"The renee executable is composed of several inter-related sub commands. Please see renee -h for all available options. This part of the documentation describes options and concepts for renee run sub command in more detail. With minimal configuration, the run sub command enables you to start running the data processing and quality-control pipeline. Setting up the RENEE pipeline is fast and easy! In its most basic form, renee run only has three required inputs .","title":"1. About"},{"location":"RNA-seq/run/#2-synopsis","text":"$ renee run [--help] \\ [--small-rna] [--star-2-pass-basic] \\ [--dry-run] [--mode {slurm, local}] \\ [--shared-resources SHARED_RESOURCES] \\ [--singularity-cache SINGULARITY_CACHE] \\ [--sif-cache SIF_CACHE] \\ [--tmp-dir TMP_DIR] \\ [--threads THREADS] \\ --input INPUT [INPUT ...] \\ --output OUTPUT \\ --genome {hg38_36, mm10_M21, ...} The synopsis for each command shows its parameters and their usage. Optional parameters are shown in square brackets. A user must provide a list of FastQ files (globbing is supported) to analyze via --input argument, an output directory to store results via --output argument and select reference genome for alignment and annotation via the --genome argument which is hg38_36 by default. If you are running the pipeline outside of Biowulf, you will need to additionally provide the the following options: --shared-resources , --tmp-dir . More information about each of these options can be found below. Use you can always use the -h option for information on a specific sub command.","title":"2. Synopsis"},{"location":"RNA-seq/run/#21-required-arguments","text":"Each of the following arguments are required. Failure to provide a required argument will result in a non-zero exit-code. --input INPUT [INPUT ...] Input FastQ file(s) to process. type: file One or more FastQ files can be provided. From the command-line, each FastQ file should separated by a space. Globbing is supported! This makes selecting FastQ files easier. Input FastQ files should be gzipp-ed. The pipeline supports single-end and pair-end RNA-seq data; however, the pipeline will not process a mixture of single-end and paired-end samples together. If you have a mixture of single-end and pair-end samples to process, please process them as two separate instances of the RENEE pipeline (with two separate output directories). Example: --input .tests/*.R?.fastq.gz --output OUTPUT Path to an output directory. type: path This location is where the pipeline will create all of its output files, also known as the pipeline's working directory. If the provided output directory does not exist, it will be initialized automatically. Example: --output /data/$USER/RNA_hg38 --genome {hg38_36,mm10_M21,custom.json,...} Reference genome. type: string or file This option defines the reference genome for your set of samples. The default is hg38_36 . On Biowulf, RENEE does comes bundled with pre built reference files for human and mouse samples; however, it is worth noting that the pipeline does accept a custom reference genome built with the build sub command. Building a new reference genome is easy! You can create a custom reference genome with a single command. This is extremely useful when working with non-model organisms. New users can reference the documentation's getting started section to see how a reference genome is built. Pre built Option Pre build genomes are available with RENEE. Please see the resources page for more information about each pre built option. Custom Option A user can also supply a custom reference genome built with the build sub command. Please supply the custom reference JSON file that was generated by the build sub command. The name of this custom reference JSON file is dependent on the values provided to the following renee build args, --ref-name REF_NAME and --gtf-ver GTF_VER , where the name of the provided custom reference JSON file would be: {REF_NAME}_{GTF_VER}.json . Example: --genome hg38_36 OR --genome /data/${USER}/hg38_36/hg38_36.json","title":"2.1 Required Arguments"},{"location":"RNA-seq/run/#22-analysis-options","text":"--small-rna Run STAR using ENCODE's recommendations for small RNA. type: boolean This option should only be used with small RNA libraries. These are rRNA-depleted libraries that have been size selected to contain fragments shorter than 200bp. Size selection enriches for small RNA species such as miRNAs, siRNAs, or piRNAs. Also, this option should not be combined with the star 2-pass basic option. If the two options are combined, STAR will run in pass basic mode. This means that STAR will not run with ENCODE's recommendations for small RNA alignment. As so, please take caution not to combine both options together. Please note: This option is only supported with single-end data. Example: --small-rna --star-2-pass-basic Run STAR in per sample 2-pass mapping mode. type: boolean It is recommended to use this option when processing a set of unrelated samples or when processing samples in a clinical setting. It is not adivsed to use this option for a study with multiple related samples. By default, the pipeline ultilizes a multi sample 2-pass mapping approach where the set of splice junctions detected across all samples are provided to the second pass of STAR. This option overrides the default behavior so each sample will be processed in a per sample two-pass basic mode. This option should not be combined with the small RNA option. If the two options are combined, STAR will run in pass basic mode. Example: --star-2-pass-basic","title":"2.2 Analysis Options"},{"location":"RNA-seq/run/#23-orchestration-options","text":"Each of the following arguments are optional and do not need to be provided. --dry-run Dry run the pipeline. type: boolean Displays what steps in the pipeline remain or will be run. Does not execute anything! Example: --dry-run --mode {slurm,local} Execution Method. > type: string default: slurm Execution Method. Defines the mode or method of execution. Valid mode options include: slurm or local. local Local executions will run serially on compute instance. This is useful for testing, debugging, or when a users does not have access to a high performance computing environment. If this option is not provided, it will default to a local execution mode. slurm The slurm execution method will submit jobs to a cluster using a slurm + singularity backend. This method will automatically submit the master job to the cluster. It is recommended running RENEE in this mode as execution will be significantly faster in a distributed environment. Example: --mode slurm --shared-resources SHARED_RESOURCES Local path to shared resources. type: path The pipeline uses a set of shared reference files that can be re-used across reference genomes. These currently include reference files for kraken and FQScreen. These reference files can be downloaded with the build sub command's --shared-resources option. With that being said, these files only need to be downloaded once. We recommend storing this files in a shared location on the filesystem that other people can access. If you are running the pipeline on Biowulf, you do NOT need to download these reference files! They already exist on the filesystem in a location that anyone can access; however, if you are running the pipeline on another cluster or target system, you will need to download the shared resources with the build sub command, and you will need to provide this option every time you run the pipeline. Please provide the same path that was provided to the build sub command's --shared-resources option. Again, if you are running the pipeline on Biowulf, you do NOT need to provide this option. For more information about how to download shared resources, please reference the build sub command's --shared-resources option. Example: --shared-resources /data/shared/renee --singularity-cache SINGULARITY_CACHE Overrides the $SINGULARITY_CACHEDIR environment variable. type: path default: --output OUTPUT/.singularity Singularity will cache image layers pulled from remote registries. This ultimately speeds up the process of pull an image from DockerHub if an image layer already exists in the singularity cache directory. By default, the cache is set to the value provided to the --output argument. Please note that this cache cannot be shared across users. Singularity strictly enforces you own the cache directory and will return a non-zero exit code if you do not own the cache directory! See the --sif-cache option to create a shareable resource. Example: --singularity-cache /data/$USER/.singularity --sif-cache SIF_CACHE Path where a local cache of SIFs are stored. type: path Uses a local cache of SIFs on the filesystem. This SIF cache can be shared across users if permissions are set correctly. If a SIF does not exist in the SIF cache, the image will be pulled from Dockerhub and a warning message will be displayed. The renee cache subcommand can be used to create a local SIF cache. Please see renee cache for more information. This command is extremely useful for avoiding DockerHub pull rate limits. It also remove any potential errors that could occur due to network issues or DockerHub being temporarily unavailable. We recommend running RENEE with this option when ever possible. Example: --singularity-cache /data/$USER/SIFs --tmp-dir TMP_DIR Path on the file system for writing temporary files. type: path default: /lscratch/$SLURM_JOBID Path on the file system for writing temporary output files. By default, the temporary directory is set to '/lscratch/$SLURM_JOBID' on NIH's Biowulf cluster and 'OUTPUT' on the FRCE cluster. However, if you are running the pipeline on another cluster, this option will need to be specified. Ideally, this path should point to a dedicated location on the filesystem for writing tmp files. On many systems, this location is set to somewhere in /scratch. If you need to inject avariable into this string that should NOT be expanded,please quote this options value in single quotes. Example: --tmp-dir '/cluster_scratch/$USER/' --threads THREADS Max number of threads for each process. type: int default: 2 Max number of threads for each process. This option is more applicable when running the pipeline with --mode local . It is recommended setting this value to the maximum number of CPUs available on the host machine. Example: --threads 12","title":"2.3 Orchestration Options"},{"location":"RNA-seq/run/#24-misc-options","text":"Each of the following arguments are optional and do not need to be provided. -h, --help Display Help. type: boolean Shows command's synopsis, help message, and an example command Example: --help","title":"2.4 Misc Options"},{"location":"RNA-seq/run/#3-example","text":"","title":"3. Example"},{"location":"RNA-seq/run/#31-biowulf","text":"On Biowulf getting started with the pipeline is fast and easy! The pipeline comes bundled with pre-built human and mouse reference genomes. In the example below, we will use the pre-built human reference genome. # Step 0.) Grab an interactive node (do not run on head node) srun -N 1 -n 1 --time = 12 :00:00 -p interactive --mem = 8gb --cpus-per-task = 4 --pty bash module purge module load ccbrpipeliner # Step 1.) Dry run pipeline with provided test data renee run --input .tests/*.R?.fastq.gz \\ --output /data/ $USER /RNA_hg38 \\ --genome hg38_36 \\ --mode slurm \\ --star-2-pass-basic \\ --sif-cache /data/OpenOmics/SIFs/ \\ --dry-run # Step 2.) Run RENEE pipeline # The slurm mode will submit jobs to the cluster. # It is recommended running renee in this mode. renee run --input .tests/*.R?.fastq.gz \\ --output /data/ $USER /RNA_hg38 \\ --genome hg38_36 \\ --mode slurm \\ --sif-cache /data/OpenOmics/SIFs/ \\ --star-2-pass-basic","title":"3.1 Biowulf"},{"location":"RNA-seq/run/#32-generic-slurm-cluster","text":"Running the pipeline outside of Biowulf is easy; however, there are a few extra steps you must first take. Before getting started, you will need to build reference files for the pipeline. Please note when running the build sub command for the first time, you will also need to provide the --shared-resources option. This option will download our kraken2 database and bowtie2 indices for FastQ Screen. The path provided to this option should be provided to the --shared-resources option of the run sub command. Next, you will also need to provide a path to write temporary output files via the --tmp-dir option. We also recommend providing a path to a SIF cache. You can cache software containers locally with the cache sub command. # Step 0.) Grab an interactive node (do not run on head node) srun -N 1 -n 1 --time = 2 :00:00 -p interactive --mem = 8gb --cpus-per-task = 4 --pty bash # Add snakemake and singularity to $PATH, # This step may vary across clusters, you # can reach out to a sys admin if snakemake # and singularity are not installed. module purge # Replace the following: # module load ccbrpipeliner # with module load statements that load # python >= 3.7, # snakemake, and # singularity # before running renee # Also, ensure that the `renee` execulable is in PATH # Step 1.) Dry run pipeline with provided test data renee run --input .tests/*.R?.fastq.gz \\ --output /data/ $USER /RNA_hg38 \\ --genome /data/ $USER /hg38_36/hg38_36.json \\ --mode slurm \\ --sif-cache /data/ $USER /cache \\ --star-2-pass-basic \\ --shared-resources /data/shared/renee \\ --tmp-dir /cluster_scratch/ $USER / \\ --dry-run # Step 2.) Run RENEE pipeline # The slurm mode will submit jobs to the cluster. # It is recommended running renee in this mode. renee run --input .tests/*.R?.fastq.gz \\ --output /data/ $USER /RNA_hg38 \\ --genome /data/ $USER /hg38_36/hg38_36.json \\ --mode slurm \\ --sif-cache /data/ $USER /cache \\ --star-2-pass-basic \\ --shared-resources /data/shared/renee \\ --tmp-dir /cluster_scratch/ $USER / \\ --dry-run","title":"3.2 Generic SLURM Cluster"},{"location":"RNA-seq/unlock/","text":"renee unlock \u00b6 1. About \u00b6 The renee executable is composed of several inter-related sub commands. Please see renee -h for all available options. This part of the documentation describes options and concepts for renee unlock sub command in more detail. With minimal configuration, the unlock sub command enables you to unlock a pipeline output directory. If the pipeline fails ungracefully, it maybe required to unlock the working directory before proceeding again. Snakemake will inform a user when it maybe necessary to unlock a working directory with an error message stating: Error: Directory cannot be locked . Please verify that the pipeline is not running before running this command. If the pipeline is currently running, the workflow manager will report the working directory is locked. The is the default behavior of snakemake, and it is normal. Do NOT run this command if the pipeline is still running! Please kill the master job and it's child jobs prior to running this command. Unlocking an RENEE pipeline output directory is fast and easy! In its most basic form, renee run only has one required inputs . 2. Synopsis \u00b6 $ renee unlock [-h] --output OUTPUT The synopsis for this command shows its parameters and their usage. Optional parameters are shown in square brackets. A user must provide an output directory to unlock via --output argument. After running the unlock sub command, you can resume the build or run pipeline from where it left off by re-running it. Use you can always use the -h option for information on a specific command. 2.1 Required Arguments \u00b6 --output OUTPUT Output directory to unlock. type: path Path to a previous run's output directory to unlock. This will remove a lock on the working directory. Please verify that the pipeline is not running before running this command. Example: --output /data/$USER/RNA_hg38 2.2 Options \u00b6 Each of the following arguments are optional and do not need to be provided. -h, --help Display Help. type: boolean Shows command's synopsis, help message, and an example command Example: --help 3. Example \u00b6 # Step 0.) Grab an interactive node (do not run on head node) srun -N 1 -n 1 --time = 12 :00:00 -p interactive --mem = 8gb --cpus-per-task = 4 --pty bash module purge module load ccbrpipeliner # Step 1.) Unlock a pipeline output directory renee unlock --output /data/ $USER /RNA_hg38","title":"unlock"},{"location":"RNA-seq/unlock/#renee-unlock","text":"","title":"renee unlock"},{"location":"RNA-seq/unlock/#1-about","text":"The renee executable is composed of several inter-related sub commands. Please see renee -h for all available options. This part of the documentation describes options and concepts for renee unlock sub command in more detail. With minimal configuration, the unlock sub command enables you to unlock a pipeline output directory. If the pipeline fails ungracefully, it maybe required to unlock the working directory before proceeding again. Snakemake will inform a user when it maybe necessary to unlock a working directory with an error message stating: Error: Directory cannot be locked . Please verify that the pipeline is not running before running this command. If the pipeline is currently running, the workflow manager will report the working directory is locked. The is the default behavior of snakemake, and it is normal. Do NOT run this command if the pipeline is still running! Please kill the master job and it's child jobs prior to running this command. Unlocking an RENEE pipeline output directory is fast and easy! In its most basic form, renee run only has one required inputs .","title":"1. About"},{"location":"RNA-seq/unlock/#2-synopsis","text":"$ renee unlock [-h] --output OUTPUT The synopsis for this command shows its parameters and their usage. Optional parameters are shown in square brackets. A user must provide an output directory to unlock via --output argument. After running the unlock sub command, you can resume the build or run pipeline from where it left off by re-running it. Use you can always use the -h option for information on a specific command.","title":"2. Synopsis"},{"location":"RNA-seq/unlock/#21-required-arguments","text":"--output OUTPUT Output directory to unlock. type: path Path to a previous run's output directory to unlock. This will remove a lock on the working directory. Please verify that the pipeline is not running before running this command. Example: --output /data/$USER/RNA_hg38","title":"2.1 Required Arguments"},{"location":"RNA-seq/unlock/#22-options","text":"Each of the following arguments are optional and do not need to be provided. -h, --help Display Help. type: boolean Shows command's synopsis, help message, and an example command Example: --help","title":"2.2 Options"},{"location":"RNA-seq/unlock/#3-example","text":"# Step 0.) Grab an interactive node (do not run on head node) srun -N 1 -n 1 --time = 12 :00:00 -p interactive --mem = 8gb --cpus-per-task = 4 --pty bash module purge module load ccbrpipeliner # Step 1.) Unlock a pipeline output directory renee unlock --output /data/ $USER /RNA_hg38","title":"3. Example"},{"location":"dev/coming-soon/","text":"Coming Soon \u00b6 This page is under construction, and our team is actively working on bringing you the most up-to-date documentation. Thank you for your patience!","title":"Coming Soon"},{"location":"dev/coming-soon/#coming-soon","text":"This page is under construction, and our team is actively working on bringing you the most up-to-date documentation. Thank you for your patience!","title":"Coming Soon"},{"location":"dev/lorem_ipsum/","text":"Lorem ipsum \u00b6 Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum. h2 Heading \u00b6 h3 Heading \u00b6 h4 Heading \u00b6 h5 Heading \u00b6 h6 Heading \u00b6 Horizontal Rules \u00b6 Emphasis \u00b6 This is bold text This is bold text This is italic text This is italic text Strikethrough Blockquotes \u00b6 Blockquotes can also be nested... ...by using additional greater-than signs right next to each other... ...or with spaces between arrows. Lists \u00b6 Unordered Create a list by starting a line with + , - , or * Sub-lists are made by indenting 2 spaces: Marker character change forces new list start: Ac tristique libero volutpat at Facilisis in pretium nisl aliquet Nulla volutpat aliquam velit Very easy! Ordered Lorem ipsum dolor sit amet Consectetur adipiscing elit Integer molestie lorem at massa You can use sequential numbers... ...or keep all the numbers as 1. Start numbering with offset: foo bar Code \u00b6 Inline code Indented code // Some comments line 1 of code line 2 of code line 3 of code Block code \"fences\" Sample text here... Syntax highlighting var foo = function ( bar ) { return bar ++ ; }; console . log ( foo ( 5 )); Tables \u00b6 Option Description data path to data files to supply the data that will be passed into templates. engine engine to be used for processing templates. Handlebars is the default. ext extension to be used for dest files. Right-aligned columns Option Description data path to data files to supply the data that will be passed into templates. engine engine to be used for processing templates. Handlebars is the default. ext extension to be used for dest files. Links \u00b6 link text link with title Images \u00b6 Like links, Images also have a footnote style syntax With a reference later in the document defining the URL location: Plugins \u00b6 The killer feature of markdown-it is very effective support of syntax plugins . Footnotes \u00b6 Footnote 1 link 1 . Footnote 2 link 2 . Inline footnote^[Text of inline footnote] definition. Duplicated footnote reference 2 . Definition lists \u00b6 Term 1 Definition 1 with lazy continuation. Term 2 with inline markup Definition 2 { some code, part of Definition 2 } Third paragraph of definition 2. Compact style: Term 1 ~ Definition 1 Term 2 ~ Definition 2a ~ Definition 2b Footnote can have markup and multiple paragraphs. \u21a9 Footnote text. \u21a9 \u21a9","title":"Lorem ipsum"},{"location":"dev/lorem_ipsum/#lorem-ipsum","text":"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.","title":"Lorem ipsum"},{"location":"dev/lorem_ipsum/#h2-heading","text":"","title":"h2 Heading"},{"location":"dev/lorem_ipsum/#h3-heading","text":"","title":"h3 Heading"},{"location":"dev/lorem_ipsum/#h4-heading","text":"","title":"h4 Heading"},{"location":"dev/lorem_ipsum/#h5-heading","text":"","title":"h5 Heading"},{"location":"dev/lorem_ipsum/#h6-heading","text":"","title":"h6 Heading"},{"location":"dev/lorem_ipsum/#horizontal-rules","text":"","title":"Horizontal Rules"},{"location":"dev/lorem_ipsum/#emphasis","text":"This is bold text This is bold text This is italic text This is italic text Strikethrough","title":"Emphasis"},{"location":"dev/lorem_ipsum/#blockquotes","text":"Blockquotes can also be nested... ...by using additional greater-than signs right next to each other... ...or with spaces between arrows.","title":"Blockquotes"},{"location":"dev/lorem_ipsum/#lists","text":"Unordered Create a list by starting a line with + , - , or * Sub-lists are made by indenting 2 spaces: Marker character change forces new list start: Ac tristique libero volutpat at Facilisis in pretium nisl aliquet Nulla volutpat aliquam velit Very easy! Ordered Lorem ipsum dolor sit amet Consectetur adipiscing elit Integer molestie lorem at massa You can use sequential numbers... ...or keep all the numbers as 1. Start numbering with offset: foo bar","title":"Lists"},{"location":"dev/lorem_ipsum/#code","text":"Inline code Indented code // Some comments line 1 of code line 2 of code line 3 of code Block code \"fences\" Sample text here... Syntax highlighting var foo = function ( bar ) { return bar ++ ; }; console . log ( foo ( 5 ));","title":"Code"},{"location":"dev/lorem_ipsum/#tables","text":"Option Description data path to data files to supply the data that will be passed into templates. engine engine to be used for processing templates. Handlebars is the default. ext extension to be used for dest files. Right-aligned columns Option Description data path to data files to supply the data that will be passed into templates. engine engine to be used for processing templates. Handlebars is the default. ext extension to be used for dest files.","title":"Tables"},{"location":"dev/lorem_ipsum/#links","text":"link text link with title","title":"Links"},{"location":"dev/lorem_ipsum/#images","text":"Like links, Images also have a footnote style syntax With a reference later in the document defining the URL location:","title":"Images"},{"location":"dev/lorem_ipsum/#plugins","text":"The killer feature of markdown-it is very effective support of syntax plugins .","title":"Plugins"},{"location":"dev/lorem_ipsum/#footnotes","text":"Footnote 1 link 1 . Footnote 2 link 2 . Inline footnote^[Text of inline footnote] definition. Duplicated footnote reference 2 .","title":"Footnotes"},{"location":"dev/lorem_ipsum/#definition-lists","text":"Term 1 Definition 1 with lazy continuation. Term 2 with inline markup Definition 2 { some code, part of Definition 2 } Third paragraph of definition 2. Compact style: Term 1 ~ Definition 1 Term 2 ~ Definition 2a ~ Definition 2b Footnote can have markup and multiple paragraphs. \u21a9 Footnote text. \u21a9 \u21a9","title":"Definition lists"}]}