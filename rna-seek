#!/usr/bin/env python
# -*- coding: UTF-8 -*-

"""RNA-seek: an highly reproducible and portable RNA-seq pipeline
About:
    This is the main entry for the RNA-seek pipeline.
USAGE:
	$ rna-seek <run|unlock> [OPTIONS]
Example:
    $ rna-seek run --input .tests/*.R?.fastq.gz --output /scratch/$USER/RNA_hg38 --genome hg38_30 --mode local
"""

from __future__ import print_function
import sys, os, json

__author__ = 'Skyler Kuhn'
__version__ = 'v0.0.1'
__email__ = 'kuhnsa@nih.gov'


def exists(testpath):
    """Checks if file exists on the local filesystem.
    @param parser <argparse.ArgumentParser() object>:
        argparse parser object
    @param testpath <str>:
        Name of file/directory to check
    @return does_exist <boolean>:
        True when file/directory exists, False when file/directory does not exist
    """
    does_exist = True
    if not os.path.exists(testpath):
        does_exist = False # File or directory does not exist on the filesystem

    return does_exist


def permissions(parser, filename, *args, **kwargs):
    """Checks permissions using os.access() to see the user is authorized to access
    a file/directory. Checks for existence, readability, writability and executability via:
    os.F_OK (tests existence), os.R_OK (tests read), os.W_OK (tests write), os.X_OK (tests exec).
    @param parser <argparse.ArgumentParser() object>:
        Argparse parser object
    @param filename <str>:
        Name of file to check
    @return filename <str>:
        If file exists and user can read from file
    """
    if not exists(filename):
        parser.error("File '{}' does not exists! Failed to provided vaild input.".format(filename))

    if not os.access(filename, *args, **kwargs):
        parser.error("File '{}' exists, but cannot read file due to permissions!".format(filename))

    return filename


def _cp_r_safe_(source, target, resources = []):
    """Private function: Given a list paths it will recursively copy each to the
    target location. If a target path already exists, it will NOT over-write the
    existing paths data.
    @param resources <list[str]>:
        List of paths to copy over to target location
    @params source <str>:
        Add a prefix PATH to each resource
    @param target <str>:
        Target path to copy templates and required resources
    """
    from shutil import copytree

    for resource in resources:
        destination = os.path.join(target, resource)
        if not exists(destination):
            # Required resources do not exist
            copytree(os.path.join(source, resource), destination)


def rename(filename):
    """Dynamically renames FastQ file to have one of the following extensions: *.R1.fastq.gz, *.R2.fastq.gz
    To automatically rename the fastq files, a few assumptions are made. If the extension of the
    FastQ file cannot be infered, an exception is raised telling the user to fix the filename
    of the fastq files.
    @param filename <str>:
        Original name of file to be renamed
    @return filename <str>:
        A renamed FastQ filename
    """
    import re

    # Covers common extensions from SF, SRA, EBI, TCGA, and external sequencing providers
    # key = regex to match string and value = how it will be renamed
    extensions = {
        # Matches: _R[12]_fastq.gz, _R[12].fastq.gz, _R[12]_fq.gz, etc.
        ".R1.f(ast)?q.gz$": ".R1.fastq.gz",
        ".R2.f(ast)?q.gz$": ".R2.fastq.gz",
        # Matches: _R[12]_001_fastq_gz, _R[12].001.fastq.gz, _R[12]_001.fq.gz, etc.
        # Capture lane information as named group
        ".R1.(?P<lane>...).f(ast)?q.gz$": ".R1.fastq.gz",
        ".R2.(?P<lane>...).f(ast)?q.gz$": ".R2.fastq.gz",
        # Matches: _[12].fastq.gz, _[12].fq.gz, _[12]_fastq_gz, etc.
        "_1.f(ast)?q.gz$": ".R1.fastq.gz",
        "_2.f(ast)?q.gz$": ".R2.fastq.gz"
    }

    if filename.endswith('.R1.fastq.gz') or filename.endswith('.R2.fastq.gz'):
        # Filename is already in the correct format
        return filename

    converted = False
    for regex, new_ext in extensions.items():
        matched = re.search(regex, filename)
        if matched:
            # regex matches with a pattern in extensions
            converted = True
            # Try to get substring for named group lane, retain this in new file extension
            # Come back to this later, I am not sure if this is necessary
            # That string maybe static (i.e. always the same)
            # https://support.illumina.com/help/BaseSpace_OLH_009008/Content/Source/Informatics/BS/NamingConvention_FASTQ-files-swBS.htm#
            try: new_ext = "_{}{}".format(matched.group('lane'), new_ext)
            except IndexError: pass # Does not contain the named group lane

            filename = re.sub(regex, new_ext, filename)
            break # only rename once

    if not converted:
        raise NameError("""\n\tFatal: Failed to rename provided input '{}'!
        Cannot determine the extension of the user provided input file.
        Please rename the file list above before trying again.
        Here is example of acceptable input file extensions:
          sampleName.R1.fastq.gz      sampleName.R2.fastq.gz
          sampleName_R1_001.fastq.gz  sampleName_R1_001.fastq.gz
          sampleName_1.fastq.gz       sampleName_2.fastq.gz
        Please also check that your input files are gzipped?
        If they are not, please gzip them before proceeding again.
        """.format(filename, sys.argv[0])
        )

    return filename


def _sym_safe_(input_data, target):
    """Creates re-named symlinks for each FastQ file provided
    as input. If a symlink already exists, it will not try to create a new symlink.
    If relative source PATH is provided, it will be converted to an absolute PATH.
    @param input_data <list[<str>]>:
        List of input files to symlink to target location
    @param target <str>:
        Target path to copy templates and required resources
    @return input_fastqs list[<str>]:
        List of renamed input FastQs
    """
    input_fastqs = [] # store renamed fastq file names
    for file in input_data:
        filename = os.path.basename(file)
        renamed = os.path.join(target, rename(filename))
        input_fastqs.append(renamed)

        if not exists(renamed):
            # Create a symlink if it does not already exist
            os.symlink(os.path.abspath(file), renamed)

    return input_fastqs


def initialize(sub_args, repo_path, output_path):
    """Initialize the output directory and copy over required pipeline resources.
    If user provides a output directory path that already exists on the filesystem
    as a file (small chance of happening but possible), a OSError is raised. If the
    output directory PATh already EXISTS, it will not try to create the directory.
    If a resource also already exists in the output directory (i.e. output/workflow),
    it will not try to copy over that directory. In the future, it maybe worth adding
    an optional cli arg called --force, that can modifiy this behavior. Returns a list
    of renamed FastQ files (i.e. renamed symlinks).
    @param sub_args <parser.parse_args() object>:
        Parsed arguments for run sub-command
    @param repo_path <str>:
        Path to RNA-seek source code and its templates
    @param output_path <str>:
        Pipeline output path, created if it does not exist
    @return inputs list[<str>]:
        List of pipeline's input FastQ files
    """
    if not exists(output_path):
        # Pipleine output directory does not exist on filesystem
        os.makedirs(output_path)

    elif exists(output_path) and os.path.isfile(output_path):
        # Provided Path for pipleine output directory exists as file
        raise OSError("""\n\tFatal: Failed to create provided pipeline output directory!
        User provided --ouput PATH already exists on the filesystem as a file.
        Please run {} again with a different --output PATH.
        """.format(sys.argv[0])
        )

    # Copy over templates are other required resources
    required_resources = ['workflow', 'resources', 'config']
    _cp_r_safe_(source = repo_path, target = output_path, resources = required_resources)

    # Create renamed symlinks to rawdata
    inputs = _sym_safe_(input_data = sub_args.input, target = output_path)

    return inputs


def join_jsons(templates):
    """Joins multiple JSON files to into one data structure
    Used to join multiple template JSON files to create a global config dictionary.
    @params templates <list[str]>:
        List of template JSON files to join together
    @return aggregated <dict>:
        Dictionary containing the contents of all the input JSON files
    """
    aggregated = {}

    for file in templates:
        with open(file, 'r') as fh:
            aggregated.update(json.load(fh))

    return aggregated


def add_user_information(config):
    """Adds username and user's home directory to config.
    @params config <dict>:
        Config dictionary containing metadata to run pipeline
    @return config <dict>:
         Updated config dictionary containing user information (username and home directory)
    """
    # Get PATH to user's home directory
    # Method is portable across unix-like OS and Windows
    home = os.path.expanduser("~")

    # Get username from home directory PATH
    username = os.path.split(home)[-1]

    # Update config with home directory and username
    config['project']['userhome'] = home
    config['project']['username'] = username

    return config


def get_nends(ifiles):
    """Determines whether the dataset is paired-end or single-end.
    If paired-end data, checks to see if both mates (R1 and R2) are present for each sample.
    If single-end, nends is set to 1. Else if paired-end, nends is set to 2.
    @params ifiles list[<str>]:
        List containing pipeline input files (renamed symlinks)
    @return nends_status <int>:
         Integer reflecting nends status: 1 = se, 2 = pe
    """
    import re

    # Determine if dataset contains paired-end data
    paired_end  = False
    nends_status = 1
    for file in ifiles:
        if file.endswith('.R2.fastq.gz'):
            paired_end = True
            nends_status = 2
            break # dataset is paired-end

    # Check to see if both mates (R1 and R2) are present paired-end data
    if paired_end:
        nends = {} # keep count of R1 and R2 for each sample
        for file in ifiles:
            # Split sample name on file extension
            sample = re.split('\.R[12]\.fastq\.gz', os.path.basename(file))[0]
            if sample not in nends:
                nends[sample] = 0

            nends[sample] += 1

        # Check if samples contain both read mates
        missing_mates = [sample for sample, count in nends.items() if count == 1]
        if missing_mates:
            # Missing an R1 or R2 for a provided input sample
            raise NameError("""\n\tFatal: Detected pair-end data but user failed to provide
               both mates (R1 and R2) for the following samples:\n\t\t{}\n
            Please check that the basename for each sample is consistent across mates.
            Here is an example of a consistent basename across mates:
              consistent_basename.R1.fastq.gz
              consistent_basename.R2.fastq.gz

            Please do not run the pipeline with a mixture of single-end and paired-end
            samples. This feature is currently not supported within {}, and it is
            not recommended either. If this is a priority for your project, please run
            paired-end samples and single-end samples separately (in two separate output directories).
            If you feel like this functionality should exist, feel free to open an issue on Github.
            """.format(missing_mates, sys.argv[0])
            )

    return nends_status


def get_rawdata_bind_paths(input_files):
    """
    Gets rawdata bind paths of user provided fastq files.
    @params input_files list[<str>]:
        List containing user-provided input fastq files
    @return bindpaths <set>:
        Set of rawdata bind paths
    """
    bindpaths = []
    for file in input_files:
        # Get directory of input file
        rawdata_src_path = os.path.dirname(os.path.abspath(file))
        if rawdata_src_path not in bindpaths:
            bindpaths.append(rawdata_src_path)

    return bindpaths


def add_sample_metadata(input_files, config, group=None):
    """Adds sample metadata such as sample basename, label, and group information.
    If sample sheet is provided, it will default to using information in that file.
    If no sample sheet is provided, it will only add sample basenames and labels.
    @params input_files list[<str>]:
        List containing pipeline input fastq files
    @params config <dict>:
        Config dictionary containing metadata to run pipeline
    @params group <str>:
        Sample sheet containing basename, group, and label for each sample
    @return config <dict>:
        Updated config with basenames, labels, and groups (if provided)
    """
    import re

    # TODO: Add functionality for basecase when user has samplesheet
    added = []
    for file in input_files:
        # Split sample name on file extension
        sample = re.split('\.R[12]\.fastq\.gz', os.path.basename(file))[0]
        if sample not in added:
            # Only add PE sample information once
            added.append(sample)
            config['project']['groups']['rsamps'].append(sample)
            config['project']['groups']['rlabels'].append(sample)

    return config


def add_rawdata_information(sub_args, config, ifiles):
    """Adds information about rawdata provided to pipeline.
    Determines whether the dataset is paired-end or single-end and finds the set of all
    rawdata directories (needed for -B option when running singularity). If a user provides
    paired-end data, checks to see if both mates (R1 and R2) are present for each sample.
    @param sub_args <parser.parse_args() object>:
        Parsed arguments for run sub-command
    @params ifiles list[<str>]:
        List containing pipeline input files (renamed symlinks)
    @params config <dict>:
        Config dictionary containing metadata to run pipeline
    @return config <dict>:
         Updated config dictionary containing user information (username and home directory)
    """
    # Determine whether dataset is paired-end or single-ends
    # Updates config['project']['nends']: 1 = single-end, 2 = paired-end
    nends = get_nends(ifiles)  # Checks PE data for both mates (R1 and R2)
    config['project']['nends'] = nends

    # Finds the set of rawdata directories to bind
    rawdata_paths = get_rawdata_bind_paths(input_files = sub_args.input)
    config['project']['datapath'] = ','.join(rawdata_paths)

    # Add each sample's basename, label and group info
    config = add_sample_metadata(input_files = ifiles, config = config)

    return config


def get_repo_git_commit_hash(repo_path):
    """Gets the git commit hash of the RNA-seek repo.
    @param repo_path <str>:
        Path to RNA-seek git repo
    @return githash <str>:
        Latest git commit hash
    """
    import subprocess

    githash = subprocess.check_output(['git', 'rev-parse', 'HEAD'], cwd = repo_path).strip()

    return githash


def setup(sub_args, ifiles, repo_path, output_path):
    """Setup the pipeline for execution and creates config file from templates
    @param sub_args <parser.parse_args() object>:
        Parsed arguments for run sub-command
    @param repo_path <str>:
        Path to RNA-seek source code and its templates
    @param output_path <str>:
        Pipeline output path, created if it does not exist
    @return config <dict>:
         Config dictionary containing metadata to run the pipeline
    """

    required = {
        # Template for project-level information
        "project": os.path.join('config','templates', 'project.json'),
        # Template for genomic reference files
        # User provided argument --genome is used to select the template
        "genome": os.path.join('config','genomes', sub_args.genome + '.json'),
        # Template for tool information
        "tools": os.path.join('config','templates', 'tools.json'),
    }

    # Global config file for pipeline, config.json
    config = join_jsons(required.values())
    config = add_user_information(config)
    config = add_rawdata_information(sub_args, config, ifiles)

    # Add other cli collected info
    config['project']['annotation'] = sub_args.genome
    config['project']['version'] = __version__
    config['project']['workpath'] = os.path.abspath(sub_args.output)

    # Get latest git commit hash
    git_hash = get_repo_git_commit_hash(repo_path)
    config['project']['git_commit_hash'] = git_hash

    # Save config to ouput directory
    print("\nGenerating config file:")
    print(json.dumps(config, indent = 4, sort_keys=True))
    with open(os.path.join(output_path, 'config.json'), 'w') as fh:
        json.dump(config, fh, indent = 4, sort_keys = True)

    return config


def dryrun(outdir):
    """Dryruns the pipeline to ensure there are no errors prior to runnning.
    @param outdir <str>:
        Pipeline output PATH
    @return dryrun_output <str>:
        Byte string representation of dryrun command
    """
    import subprocess

    try:
        dryrun_output = subprocess.check_output([
            'snakemake', '-npr',
            '--use-singularity',
            '--cores', '4',
            '--configfile=config.json'
        ], cwd = outdir,
        stderr=subprocess.STDOUT)

    except subprocess.CalledProcessError as e:
        sys.exit("{}\n{}".format(e, e.output))

    return dryrun_output


def orchestrate(mode, outdir, additional_bind_paths, threads=2):
    """Runs RNA-seek pipeline via selected executor: local or slurm.
    If 'local' is selected, the pipeline is executed locally on a compute node/instance.
    If 'slurm' is selected, jobs will be submited to the cluster using SLURM job scheduler.
    Support for additional job schedulers (i.e. PBS, SGE, LSF) may be added in the future.
    @param outdir <str>:
        Pipeline output PATH
    @param mode <str>:
        Execution method or mode:
            local runs serially a compute instance without sumbitting to the cluster.
            slurm will submit jobs to the cluster using the SLURM job scheduler.
    @return masterjob <subprocess.Popen() object>:
    """
    import subprocess

    # Default PATHs to bind to the container's filesystem
    bindpaths = "{},/data/CCBR_Pipeliner/db/PipeDB/,/lscratch,/fdb".format(outdir)
    # Create log file for master job
    logfh = open(os.path.join(outdir, 'snakemake.log'), 'w')
    # Set ENV variable 'SINGULARITY_CACHEDIR' to output directory
    my_env = {}; my_env.update(os.environ)
    my_env['SINGULARITY_CACHEDIR'] = os.path.join(outdir, ".singularity")


    if additional_bind_paths:
        # Add Bind PATHs for rawdata directories
        bindpaths = "{},{}".format(additional_bind_paths,bindpaths)

    # Run on compute node or instance without submitting jobs to a scheduler
    if mode == 'local':
        # Run RNA-seek: instantiate main/master process
        # Look into later: it maybe worth replacing Popen subprocess with a direct
        # snakemake API call: https://snakemake.readthedocs.io/en/stable/api_reference/snakemake.html
        masterjob = subprocess.Popen([
                'snakemake', '-pr',
                '--use-singularity',
                '--singularity-args', "'-B {}'".format(bindpaths),
                '--cores', threads,
                '--configfile=config.json'
            ], cwd = outdir, stderr=subprocess.STDOUT, stdout=logfh, env=my_env)

    # Submitting jobs to cluster via SLURM's job scheduler
    elif mode == 'slurm':
        # Run RNA-seek: instantiate main/master process
        # Look into later: it maybe worth replacing Popen subprocess with a direct
        # snakemake API call: https://snakemake.readthedocs.io/en/stable/api_reference/snakemake.html
        # snakemake --latency-wait 120  -s $R/Snakefile -d $R --printshellcmds
        #    --cluster-config $R/cluster.json --keep-going --restart-times 3
        #    --cluster "sbatch --gres {cluster.gres} --cpus-per-task {cluster.threads} -p {cluster.partition} -t {cluster.time} --mem {cluster.mem} --job-name={params.rname}"
        #    -j 500 --rerun-incomplete --stats $R/Reports/initialqc.stats -T
        #    2>&1|tee -a $R/Reports/snakemake.log
        print('Feature coming soon! Please select "local" execution mode for now.')
        sys.exit()



    return masterjob



def run(sub_args):
    """Initialize, setup, and run the RNA-seek pipeline.
    Calls initialize() to create output directory and copy over pipeline resources,
    setup() to create the pipeline config file, dryrun() to ensure their are no issues
    before running the pipeline, and finally run() to execute the Snakemake workflow.
    @param sub_args <parser.parse_args() object>:
        Parsed arguments for run sub-command
    """
    print(sub_args)

    # Get PATH to RNA-seek git repository for copying over pipeline resources
    git_repo = os.path.dirname(os.path.abspath(__file__))

    # Initialize working directory, copy over required pipeline resources
    input_files = initialize(sub_args, repo_path = git_repo, output_path = sub_args.output)

    # Step pipeline for execution, create config.json config file from templates
    config = setup(sub_args, ifiles = input_files, repo_path = git_repo, output_path = sub_args.output)

    # Dryrun pipeline
    dryrun_output = dryrun(outdir = sub_args.output)
    print("\nDry-running RNA-seek pipeline:\n{}".format(dryrun_output))

    # Run pipelie
    rawdata_bind_paths = config['project']['datapath']
    masterjob = orchestrate(mode = sub_args.mode, outdir = sub_args.output, additional_bind_paths = rawdata_bind_paths, threads = sub_args.threads)
    print("\nRunning RNA-seek pipeline in '{}' mode...".format(sub_args.mode))
    masterjob.wait()
    print('RNA-seek pipeline has completed with exit-code: {}'.format(masterjob.returncode))

def unlock(sub_args):
    """Unlocks a previous runs output directory. If snakemake fails ungracefully,
    it maybe required to unlock the working directory before proceeding again.
    This is rare but it does occasionally happen.
    @param sub_args <parser.parse_args() object>:
        Parsed arguments for unlock sub-command
    """
    # cd /path/to/output; snakemake --unlock --cores 2 --configfile=config.json
    print(sub_args)


def parsed_arguments():
    """Parses user-provided command-line arguments. Requires argparse package.
    """
    import argparse

    # Create a top-level parser
    parser = argparse.ArgumentParser(description = 'RNA-seek: \
                                                    a highly-reproducible RNA-seq pipeline')
    # Create sub-command parser
    subparsers = parser.add_subparsers()

    # Options for the "run" sub-command
    subparser_run = subparsers.add_parser('run',
                                            help = 'Run the RNA-seek pipeline with your FastQ files.',
                                            description = 'This section explains \
                                                            concepts and options for \
                                                            running the RNA-seek pipeline.\
                                                            Visit "https://github.com/skchronicles/RNA-seek" \
                                                            for more information.')
    # Input FastQ files
    subparser_run.add_argument('--input',
                                # Check if the file exists and if it is readable
                                type = lambda file: permissions(parser, file, os.R_OK),
                                required = True,
                                nargs = '+',
                                help = '<Required> Input FastQ file(s) to process. \
                                        One or multiple FastQ files can be provided. \
                                        The pipeline supports single-end and pair-end RNA-seq data.\
                                        Example: --input .tests/*.R?.fastq.gz')
    # Output Directory (analysis working directory)
    subparser_run.add_argument('--output',
                                type = str,
                                required = True,
                                help = '<Required> Path to an output directory. \
                                        This location is where the pipeline will create all of \
                                        its output files. If the user-provided working directory \
                                        has not been initialized, it will automatically be created. \
                                        Example: --output /scratch/$USER/RNA_hg38')
    # Reference Genome (to dynamically select reference files)
    subparser_run.add_argument('--genome',
                                type = str,
                                required = True,
                                choices = ['hg38_30', 'mm10_M21'],
                                help = '<Required> Reference Genome. \
                                        This is the reference genome or assembly to used for selecting \
                                        reference files. The pipeline currently supports human and mouse data. \
                                        Please select from one of the following options: hg38_30 or mm10_M21. \
                                        hg38 uses the following annotation: Gencode Release 30. \
                                        mm10 uses the following annotation: Gencode Release M21. \
                                        Please note: hg38_30 is a human reference genome and \
                                        mm10_M21 is a mouse reference genome. \
                                        Example: --genome hg38_30')
    # Execution Method (run locally on a compute node, submit to SLURM job scheduler, etc.)
    subparser_run.add_argument('--mode',
                                type = str,
                                required = False,
                                default = "local",
                                choices = ['local', 'slurm'],
                                help = '<Optional> Execution Method [Default: local]. \
                                        Defines the mode or method of execution. \
                                        Vaild mode options include: local or slurm. \
                                        local: uses local method of execution. \
                                        local executions will run serially on compute \
                                        instance. This is useful for testing, debugging, \
                                        or when a users does not have access to a high \
                                        performance computing environment. If this option is \
                                        not provided, it will default to a local execution mode. \
                                        slurm: uses slurm and singularity backend. \
                                        The slurm execution method will submit jobs to a cluster. \
                                        It is recommended running RNA-seek in this mode as execution will \
                                        be significantly faster in a distributed environment. \
                                        Example: --mode slurm')

    # Dry-run (do not execute the workflow)
    subparser_run.add_argument('--dry-run',
                                action = 'store_true',
                                required = False,
                                default = False,
                                help = '<Optional> Do not execute anything, \
                                and only display what steps in the pipeline remain or will \
                                be run [Default: False].')

    # Number of threads for the RNA-seek pipeline's main proceess
    subparser_run.add_argument('--threads',
                                type = int,
                                required = False,
                                default = 2,
                                help = '<Optional> Max number of threads for each process. \
                                It is recommended setting this vaule to the maximum number of CPUs \
                                available on the host machine [Default: 2].')


    # Options for the "run" sub-command
    subparser_unlock = subparsers.add_parser('unlock',
                                            help = 'Unlocks a previous runs output directory.',
                                            description = 'If the pipeline fails ungracefully, it maybe \
                                                            required to unlock the working directory before \
                                                            proceeding again. \
                                                            Please verify that the pipeline is not running before \
                                                            running this command. If the pipeline is still running, \
                                                            the workflow manager will report the working directory \
                                                            is locked. This is normal behavior. Do NOT run this command \
                                                            if the pipeline is still running.')
    # Output Directory (analysis working directory)
    subparser_unlock.add_argument('--output',
                                type = str,
                                required = True,
                                help = "<Required> Path to a previous run's output directory to unlock. \
                                        This will remove a lock on the working directory. \
                                        Please verify that the pipeline is not running before \
                                        running this command. \
                                        Example: --output /scratch/$USER/RNA_hg38")


    # Define run() as handler for sub-parser
    subparser_run.set_defaults(func = run)
    subparser_unlock.set_defaults(func = unlock)

    # Parse command-line args
    args = parser.parse_args()
    return args


def main():

    # Collect args for sub-command
    args = parsed_arguments()

    # Mediator method to call sub-command's set handler function
    args.func(args)


if __name__ == '__main__':
    main()
